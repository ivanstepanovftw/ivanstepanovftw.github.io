<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Lecturers: Alan Edelman and Steven&nbsp;G.&nbsp;Johnson Notes by Paige Bright, Alan Edelman, and Steven&nbsp;G.&nbsp;Johnson">

<title>Matrix Calculus (for Machine Learning and Beyond) – ivanstepanovftw</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-831a6e13d34e89ab8943621a9b1bffba.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  </head><body>true

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>





<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#overview-and-motivation" id="toc-overview-and-motivation" class="nav-link active" data-scroll-target="#overview-and-motivation">Overview and Motivation</a>
  <ul class="collapse">
  <li><a href="#applications" id="toc-applications" class="nav-link" data-scroll-target="#applications">Applications</a>
  <ul class="collapse">
  <li><a href="#applications-machine-learning" id="toc-applications-machine-learning" class="nav-link" data-scroll-target="#applications-machine-learning">Applications: Machine learning</a></li>
  <li><a href="#applications-physical-modeling" id="toc-applications-physical-modeling" class="nav-link" data-scroll-target="#applications-physical-modeling">Applications: Physical modeling</a></li>
  <li><a href="#applications-data-science-and-multivariable-statistics" id="toc-applications-data-science-and-multivariable-statistics" class="nav-link" data-scroll-target="#applications-data-science-and-multivariable-statistics">Applications: Data science and multivariable statistics</a></li>
  <li><a href="#applications-automatic-differentiation" id="toc-applications-automatic-differentiation" class="nav-link" data-scroll-target="#applications-automatic-differentiation">Applications: Automatic differentiation</a></li>
  </ul></li>
  <li><a href="#first-derivatives" id="toc-first-derivatives" class="nav-link" data-scroll-target="#first-derivatives">First Derivatives</a></li>
  <li><a href="#intro-matrix-and-vector-product-rule" id="toc-intro-matrix-and-vector-product-rule" class="nav-link" data-scroll-target="#intro-matrix-and-vector-product-rule">Intro: Matrix and Vector Product Rule</a></li>
  </ul></li>
  <li><a href="#sec1S" id="toc-sec1S" class="nav-link" data-scroll-target="#sec1S">Derivatives as Linear Operators</a>
  <ul class="collapse">
  <li><a href="#revisiting-single-variable-calculus" id="toc-revisiting-single-variable-calculus" class="nav-link" data-scroll-target="#revisiting-single-variable-calculus">Revisiting single-variable calculus</a></li>
  <li><a href="#linear-operators" id="toc-linear-operators" class="nav-link" data-scroll-target="#linear-operators">Linear operators</a>
  <ul class="collapse">
  <li><a href="#sec:directional" id="toc-sec:directional" class="nav-link" data-scroll-target="#sec\:directional">Directional derivatives</a></li>
  </ul></li>
  <li><a href="#sec:multivarPart1" id="toc-sec:multivarPart1" class="nav-link" data-scroll-target="#sec\:multivarPart1">Revisiting multivariable calculus, Part 1: Scalar-valued functions</a></li>
  <li><a href="#revisiting-multivariable-calculus-part-2-vector-valued-functions" id="toc-revisiting-multivariable-calculus-part-2-vector-valued-functions" class="nav-link" data-scroll-target="#revisiting-multivariable-calculus-part-2-vector-valued-functions">Revisiting multivariable calculus, Part 2: Vector-valued functions</a></li>
  <li><a href="#sec:chainrule" id="toc-sec:chainrule" class="nav-link" data-scroll-target="#sec\:chainrule">The Chain Rule</a>
  <ul class="collapse">
  <li><a href="#sec:cost-matrix-mult" id="toc-sec:cost-matrix-mult" class="nav-link" data-scroll-target="#sec\:cost-matrix-mult">Cost of Matrix Multiplication</a></li>
  </ul></li>
  <li><a href="#beyond-multi-variable-derivatives" id="toc-beyond-multi-variable-derivatives" class="nav-link" data-scroll-target="#beyond-multi-variable-derivatives">Beyond Multi-Variable Derivatives</a></li>
  </ul></li>
  <li><a href="#sec:kronecker" id="toc-sec:kronecker" class="nav-link" data-scroll-target="#sec\:kronecker">Jacobians of Matrix Functions</a>
  <ul class="collapse">
  <li><a href="#derivatives-of-matrix-functions-linear-operators" id="toc-derivatives-of-matrix-functions-linear-operators" class="nav-link" data-scroll-target="#derivatives-of-matrix-functions-linear-operators">Derivatives of matrix functions: Linear operators</a></li>
  <li><a href="#a-simple-example-the-two-by-two-matrix-square-function" id="toc-a-simple-example-the-two-by-two-matrix-square-function" class="nav-link" data-scroll-target="#a-simple-example-the-two-by-two-matrix-square-function">A simple example: The two-by-two matrix-square function</a>
  <ul class="collapse">
  <li><a href="#the-matrix-squaring-four-by-four-jacobian-matrix" id="toc-the-matrix-squaring-four-by-four-jacobian-matrix" class="nav-link" data-scroll-target="#the-matrix-squaring-four-by-four-jacobian-matrix">The matrix-squaring four-by-four Jacobian matrix</a></li>
  </ul></li>
  <li><a href="#kronecker-products" id="toc-kronecker-products" class="nav-link" data-scroll-target="#kronecker-products">Kronecker Products</a>
  <ul class="collapse">
  <li><a href="#key-kronecker-product-identity" id="toc-key-kronecker-product-identity" class="nav-link" data-scroll-target="#key-kronecker-product-identity">Key Kronecker-product identity</a></li>
  <li><a href="#the-jacobian-in-kronecker-product-notation" id="toc-the-jacobian-in-kronecker-product-notation" class="nav-link" data-scroll-target="#the-jacobian-in-kronecker-product-notation">The Jacobian in Kronecker-product notation</a></li>
  <li><a href="#the-computational-cost-of-kronecker-products" id="toc-the-computational-cost-of-kronecker-products" class="nav-link" data-scroll-target="#the-computational-cost-of-kronecker-products">The computational cost of Kronecker products</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec:finitedifference" id="toc-sec:finitedifference" class="nav-link" data-scroll-target="#sec\:finitedifference">Finite-Difference Approximations</a>
  <ul class="collapse">
  <li><a href="#sec:approximation" id="toc-sec:approximation" class="nav-link" data-scroll-target="#sec\:approximation">Why compute derivatives approximately instead of exactly?</a></li>
  <li><a href="#finite-difference-approximations-easy-version" id="toc-finite-difference-approximations-easy-version" class="nav-link" data-scroll-target="#finite-difference-approximations-easy-version">Finite-Difference Approximations: Easy Version</a></li>
  <li><a href="#example-matrix-squaring" id="toc-example-matrix-squaring" class="nav-link" data-scroll-target="#example-matrix-squaring">Example: Matrix squaring</a></li>
  <li><a href="#accuracy-of-finite-differences" id="toc-accuracy-of-finite-differences" class="nav-link" data-scroll-target="#accuracy-of-finite-differences">Accuracy of Finite Differences</a></li>
  <li><a href="#order-of-accuracy" id="toc-order-of-accuracy" class="nav-link" data-scroll-target="#order-of-accuracy">Order of accuracy</a></li>
  <li><a href="#roundoff-error" id="toc-roundoff-error" class="nav-link" data-scroll-target="#roundoff-error">Roundoff error</a></li>
  <li><a href="#other-finite-difference-methods" id="toc-other-finite-difference-methods" class="nav-link" data-scroll-target="#other-finite-difference-methods">Other finite-difference methods</a></li>
  </ul></li>
  <li><a href="#sec:generalvectorspaces" id="toc-sec:generalvectorspaces" class="nav-link" data-scroll-target="#sec\:generalvectorspaces">Derivatives in General Vector Spaces</a>
  <ul class="collapse">
  <li><a href="#a-simple-matrix-dot-product-and-norm" id="toc-a-simple-matrix-dot-product-and-norm" class="nav-link" data-scroll-target="#a-simple-matrix-dot-product-and-norm">A Simple Matrix Dot Product and Norm</a></li>
  <li><a href="#sec:banach" id="toc-sec:banach" class="nav-link" data-scroll-target="#sec\:banach">Derivatives, Norms, and Banach spaces</a></li>
  </ul></li>
  <li><a href="#nonlinear-root-finding-optimization-and-adjoint-differentiation" id="toc-nonlinear-root-finding-optimization-and-adjoint-differentiation" class="nav-link" data-scroll-target="#nonlinear-root-finding-optimization-and-adjoint-differentiation">Nonlinear Root-Finding, Optimization, and Adjoint Differentiation</a>
  <ul class="collapse">
  <li><a href="#sec:newton-roots" id="toc-sec:newton-roots" class="nav-link" data-scroll-target="#sec\:newton-roots">Newton’s Method</a>
  <ul class="collapse">
  <li><a href="#scalar-functions" id="toc-scalar-functions" class="nav-link" data-scroll-target="#scalar-functions">Scalar Functions</a></li>
  <li><a href="#multidimensional-functions" id="toc-multidimensional-functions" class="nav-link" data-scroll-target="#multidimensional-functions">Multidimensional Functions</a></li>
  </ul></li>
  <li><a href="#optimization" id="toc-optimization" class="nav-link" data-scroll-target="#optimization">Optimization</a>
  <ul class="collapse">
  <li><a href="#nonlinear-optimization" id="toc-nonlinear-optimization" class="nav-link" data-scroll-target="#nonlinear-optimization">Nonlinear Optimization</a></li>
  <li><a href="#engineeringphysical-optimization" id="toc-engineeringphysical-optimization" class="nav-link" data-scroll-target="#engineeringphysical-optimization">Engineering/Physical Optimization</a></li>
  </ul></li>
  <li><a href="#sec:adjoint-method" id="toc-sec:adjoint-method" class="nav-link" data-scroll-target="#sec\:adjoint-method">Reverse-mode “Adjoint” Differentiation</a>
  <ul class="collapse">
  <li><a href="#nonlinear-equations" id="toc-nonlinear-equations" class="nav-link" data-scroll-target="#nonlinear-equations">Nonlinear equations</a></li>
  <li><a href="#adjoint-methods-and-ad" id="toc-adjoint-methods-and-ad" class="nav-link" data-scroll-target="#adjoint-methods-and-ad">Adjoint methods and AD</a></li>
  <li><a href="#adjoint-method-example" id="toc-adjoint-method-example" class="nav-link" data-scroll-target="#adjoint-method-example">Adjoint-method example</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#derivative-of-matrix-determinant-and-inverse" id="toc-derivative-of-matrix-determinant-and-inverse" class="nav-link" data-scroll-target="#derivative-of-matrix-determinant-and-inverse">Derivative of Matrix Determinant and Inverse</a>
  <ul class="collapse">
  <li><a href="#two-derivations" id="toc-two-derivations" class="nav-link" data-scroll-target="#two-derivations">Two Derivations</a></li>
  <li><a href="#applications-1" id="toc-applications-1" class="nav-link" data-scroll-target="#applications-1">Applications</a>
  <ul class="collapse">
  <li><a href="#characteristic-polynomial" id="toc-characteristic-polynomial" class="nav-link" data-scroll-target="#characteristic-polynomial">Characteristic Polynomial</a></li>
  <li><a href="#the-logarithmic-derivative" id="toc-the-logarithmic-derivative" class="nav-link" data-scroll-target="#the-logarithmic-derivative">The Logarithmic Derivative</a></li>
  </ul></li>
  <li><a href="#sec:jacobian-inverse" id="toc-sec:jacobian-inverse" class="nav-link" data-scroll-target="#sec\:jacobian-inverse">Jacobian of the Inverse</a></li>
  </ul></li>
  <li><a href="#sec:AD" id="toc-sec:AD" class="nav-link" data-scroll-target="#sec\:AD">Forward and Reverse-Mode Automatic Differentiation</a>
  <ul class="collapse">
  <li><a href="#sec:dual-AD" id="toc-sec:dual-AD" class="nav-link" data-scroll-target="#sec\:dual-AD">Automatic Differentiation via Dual Numbers</a>
  <ul class="collapse">
  <li><a href="#sec:babylonian" id="toc-sec:babylonian" class="nav-link" data-scroll-target="#sec\:babylonian">Example: Babylonian square root</a></li>
  <li><a href="#easy-forward-mode-ad" id="toc-easy-forward-mode-ad" class="nav-link" data-scroll-target="#easy-forward-mode-ad">Easy forward-mode AD</a></li>
  <li><a href="#dual-numbers" id="toc-dual-numbers" class="nav-link" data-scroll-target="#dual-numbers">Dual numbers</a></li>
  </ul></li>
  <li><a href="#naive-symbolic-differentiation" id="toc-naive-symbolic-differentiation" class="nav-link" data-scroll-target="#naive-symbolic-differentiation">Naive symbolic differentiation</a></li>
  <li><a href="#automatic-differentiation-via-computational-graphs" id="toc-automatic-differentiation-via-computational-graphs" class="nav-link" data-scroll-target="#automatic-differentiation-via-computational-graphs">Automatic Differentiation via Computational Graphs</a>
  <ul class="collapse">
  <li><a href="#reverse-mode-automatic-differentiation-on-graphs" id="toc-reverse-mode-automatic-differentiation-on-graphs" class="nav-link" data-scroll-target="#reverse-mode-automatic-differentiation-on-graphs">Reverse Mode Automatic Differentiation on Graphs</a></li>
  </ul></li>
  <li><a href="#sec:forward-vs-reverse" id="toc-sec:forward-vs-reverse" class="nav-link" data-scroll-target="#sec\:forward-vs-reverse">Forward- vs.&nbsp;Reverse-mode Differentiation</a>
  <ul class="collapse">
  <li><a href="#sec:forward-over-reverse" id="toc-sec:forward-over-reverse" class="nav-link" data-scroll-target="#sec\:forward-over-reverse">Forward-over-reverse mode: Second derivatives</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#differentiating-ode-solutions" id="toc-differentiating-ode-solutions" class="nav-link" data-scroll-target="#differentiating-ode-solutions">Differentiating ODE solutions</a>
  <ul class="collapse">
  <li><a href="#ordinary-differential-equations-odes" id="toc-ordinary-differential-equations-odes" class="nav-link" data-scroll-target="#ordinary-differential-equations-odes">Ordinary differential equations (ODEs)</a></li>
  <li><a href="#sec:ODE-sensitivity" id="toc-sec:ODE-sensitivity" class="nav-link" data-scroll-target="#sec\:ODE-sensitivity">Sensitivity analysis of ODE solutions</a>
  <ul class="collapse">
  <li><a href="#forward-sensitivity-analysis-of-odes" id="toc-forward-sensitivity-analysis-of-odes" class="nav-link" data-scroll-target="#forward-sensitivity-analysis-of-odes">Forward sensitivity analysis of ODEs</a></li>
  <li><a href="#reverseadjoint-sensitivity-analysis-of-odes" id="toc-reverseadjoint-sensitivity-analysis-of-odes" class="nav-link" data-scroll-target="#reverseadjoint-sensitivity-analysis-of-odes">Reverse/adjoint sensitivity analysis of ODEs</a></li>
  </ul></li>
  <li><a href="#example" id="toc-example" class="nav-link" data-scroll-target="#example">Example</a>
  <ul class="collapse">
  <li><a href="#forward-mode" id="toc-forward-mode" class="nav-link" data-scroll-target="#forward-mode">Forward mode</a></li>
  <li><a href="#reverse-mode" id="toc-reverse-mode" class="nav-link" data-scroll-target="#reverse-mode">Reverse mode</a></li>
  </ul></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading">Further reading</a></li>
  </ul></li>
  <li><a href="#calculus-of-variations" id="toc-calculus-of-variations" class="nav-link" data-scroll-target="#calculus-of-variations">Calculus of Variations</a>
  <ul class="collapse">
  <li><a href="#functionals-mapping-functions-to-scalars" id="toc-functionals-mapping-functions-to-scalars" class="nav-link" data-scroll-target="#functionals-mapping-functions-to-scalars">Functionals: Mapping functions to scalars</a></li>
  <li><a href="#inner-products-of-functions" id="toc-inner-products-of-functions" class="nav-link" data-scroll-target="#inner-products-of-functions">Inner products of functions</a></li>
  <li><a href="#example-minimizing-arc-length" id="toc-example-minimizing-arc-length" class="nav-link" data-scroll-target="#example-minimizing-arc-length">Example: Minimizing arc length</a></li>
  <li><a href="#eulerlagrange-equations" id="toc-eulerlagrange-equations" class="nav-link" data-scroll-target="#eulerlagrange-equations">Euler–Lagrange equations</a></li>
  </ul></li>
  <li><a href="#derivatives-of-random-functions" id="toc-derivatives-of-random-functions" class="nav-link" data-scroll-target="#derivatives-of-random-functions">Derivatives of Random Functions</a>
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#stochastic-programs" id="toc-stochastic-programs" class="nav-link" data-scroll-target="#stochastic-programs">Stochastic programs</a></li>
  <li><a href="#stochastic-differentials-and-the-reparameterization-trick" id="toc-stochastic-differentials-and-the-reparameterization-trick" class="nav-link" data-scroll-target="#stochastic-differentials-and-the-reparameterization-trick">Stochastic differentials and the reparameterization trick</a></li>
  <li><a href="#handling-discrete-randomness" id="toc-handling-discrete-randomness" class="nav-link" data-scroll-target="#handling-discrete-randomness">Handling discrete randomness</a></li>
  </ul></li>
  <li><a href="#sec:hessians" id="toc-sec:hessians" class="nav-link" data-scroll-target="#sec\:hessians">Second Derivatives, Bilinear Maps, and Hessian Matrices</a>
  <ul class="collapse">
  <li><a href="#sec:Hessian-scalar" id="toc-sec:Hessian-scalar" class="nav-link" data-scroll-target="#sec\:Hessian-scalar">Hessian matrices of scalar-valued functions</a></li>
  <li><a href="#general-second-derivatives-bilinear-maps" id="toc-general-second-derivatives-bilinear-maps" class="nav-link" data-scroll-target="#general-second-derivatives-bilinear-maps">General second derivatives: Bilinear maps</a></li>
  <li><a href="#sec:Hessian-quadratic" id="toc-sec:Hessian-quadratic" class="nav-link" data-scroll-target="#sec\:Hessian-quadratic">Generalized quadratic approximation</a></li>
  <li><a href="#hessians-and-optimization" id="toc-hessians-and-optimization" class="nav-link" data-scroll-target="#hessians-and-optimization">Hessians and optimization</a>
  <ul class="collapse">
  <li><a href="#newton-like-methods" id="toc-newton-like-methods" class="nav-link" data-scroll-target="#newton-like-methods">Newton-like methods</a></li>
  <li><a href="#computing-hessians" id="toc-computing-hessians" class="nav-link" data-scroll-target="#computing-hessians">Computing Hessians</a></li>
  <li><a href="#minima-maxima-and-saddle-points" id="toc-minima-maxima-and-saddle-points" class="nav-link" data-scroll-target="#minima-maxima-and-saddle-points">Minima, maxima, and saddle points</a></li>
  </ul></li>
  <li><a href="#further-reading-1" id="toc-further-reading-1" class="nav-link" data-scroll-target="#further-reading-1">Further Reading</a></li>
  </ul></li>
  <li><a href="#derivatives-of-eigenproblems" id="toc-derivatives-of-eigenproblems" class="nav-link" data-scroll-target="#derivatives-of-eigenproblems">Derivatives of Eigenproblems</a>
  <ul class="collapse">
  <li><a href="#differentiating-on-the-unit-sphere" id="toc-differentiating-on-the-unit-sphere" class="nav-link" data-scroll-target="#differentiating-on-the-unit-sphere">Differentiating on the Unit Sphere</a>
  <ul class="collapse">
  <li><a href="#special-case-a-circle" id="toc-special-case-a-circle" class="nav-link" data-scroll-target="#special-case-a-circle">Special Case: A Circle</a></li>
  <li><a href="#on-the-sphere" id="toc-on-the-sphere" class="nav-link" data-scroll-target="#on-the-sphere">On the Sphere</a></li>
  </ul></li>
  <li><a href="#differentiating-on-orthogonal-matrices" id="toc-differentiating-on-orthogonal-matrices" class="nav-link" data-scroll-target="#differentiating-on-orthogonal-matrices">Differentiating on Orthogonal Matrices</a>
  <ul class="collapse">
  <li><a href="#differentiating-the-symmetric-eigendecomposition" id="toc-differentiating-the-symmetric-eigendecomposition" class="nav-link" data-scroll-target="#differentiating-the-symmetric-eigendecomposition">Differentiating the Symmetric Eigendecomposition</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#where-we-go-from-here" id="toc-where-we-go-from-here" class="nav-link" data-scroll-target="#where-we-go-from-here">Where We Go From Here</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><p>Matrix Calculus<br>
(for Machine Learning and Beyond)</p></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p> </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">Invalid Date</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>{% raw %} # Introduction {#introduction .unnumbered}</p>
<p>These notes are based on the class as it was run for the second time in January 2023, taught by Professors Alan Edelman and Steven&nbsp;G.&nbsp;Johnson at MIT. The previous version of this course, run in January 2022, can be found <a href="https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2022/">on OCW here</a>.</p>
<p>Both Professors Edelman and Johnson use he/him pronouns and are in the Department of Mathematics at MIT; Prof.&nbsp;Edelman is also a Professor in the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) running the Julia lab, while Prof.&nbsp;Johnson is also a Professor in the Department of Physics.</p>
<p>Here is a description of the course.:</p>
<blockquote class="blockquote">
<p>We all know that typical calculus course sequences begin with univariate and vector calculus, respectively. Modern applications such as machine learning and large-scale optimization require the next big step, “matrix calculus” and calculus on arbitrary vector spaces.</p>
<p>This class covers a coherent approach to matrix calculus showing techniques that allow you to think of a matrix holistically (not just as an array of scalars), generalize and compute derivatives of important matrix factorizations and many other complicated-looking operations, and understand how differentiation formulas must be re-imagined in large-scale computing. We will discuss “reverse” (“adjoint”, “backpropagation”) differentiation and how modern automatic differentiation is more computer science than calculus (it is neither symbolic formulas nor finite differences).</p>
</blockquote>
<p>The class involved numerous example numerical computations using the Julia language, which you can install on your own computer following <a href="https://github.com/mitmath/julia-mit#installing-julia-and-ijulia-on-your-own-computer">these instructions</a>. The material for this class is also located on GitHub at <a href="https://github.com/mitmath/matrixcalc" class="uri">https://github.com/mitmath/matrixcalc</a>.</p>
<section id="overview-and-motivation" class="level1">
<h1>Overview and Motivation</h1>
<p>Firstly, where does matrix calculus fit into the MIT course catalog? Well, there are 18.01 (Single-Variable Calculus) and 18.02 (Vector Calculus) that students are required to take at MIT. But it seems as though this sequence of material is being cut off arbitrarily: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mtext mathvariant="normal">Scalar </mtext><mspace width="0.333em"></mspace></mrow><mo>→</mo><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> Vector </mtext><mspace width="0.333em"></mspace></mrow><mo>→</mo><mtext mathvariant="normal">Matrices</mtext><mo>→</mo><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> Higher-Order Arrays?</mtext></mrow></mrow><annotation encoding="application/x-tex">\text{Scalar }\to \text{ Vector }\to \text{Matrices}  \to \text{ Higher-Order Arrays?}</annotation></semantics></math> After all, this is how the sequence is portrayed in many computer programming languages, including Julia! Why should calculus stop with vectors?</p>
<p>In the last decade, linear algebra has taken on larger and larger importance in numerous areas, such as machine learning, statistics, engineering, etc. In this sense, linear algebra has gradually taken over a much larger part of today’s tools for lots of areas of study—now everybody needs linear algebra. So it makes sense that we would <em>want</em> to do calculus on these higher-order arrays, and it won’t be a simple/obvious generalization (for instance, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mi>A</mi></mrow></mfrac><msup><mi>A</mi><mn>2</mn></msup><mo>≠</mo><mn>2</mn><mi>A</mi></mrow><annotation encoding="application/x-tex">\frac{d}{dA} A^2 \neq 2A</annotation></semantics></math> for non-scalar matrices <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>).</p>
<p>More generally, the subjects of <em>differentiation</em> and <em>sensitivity analysis</em> are much deeper than one might suspect from the simple rules learned in first- or second-semester calculus. Differentiating functions whose inputs and/or outputs are in more complicated vector spaces (e.g.&nbsp;matrices, functions, or more) is one part of this subject. Another topic is the <em>efficient</em> evaluation of derivatives of functions involving very complicated calculations, from neural networks to huge engineering simulations—this leads to the topic of “adjoint” or “reverse-mode” differentiation, also known as “backpropagation.” <em>Automatic differentiation (AD)</em> of computer programs by compilers is another surprising topic, in which the computer does something very different from the typical human process of first writing out an explicit symbolic formula and then passing the chain rule through it. These are only a few examples: the key point is that differentiation is more complicated than you may realize, and that these complexities are increasingly relevant for a wide variety of applications.</p>
<p>Let’s quickly talk about some of these applications.</p>
<section id="applications" class="level2">
<h2 class="anchored" data-anchor-id="applications">Applications</h2>
<section id="applications-machine-learning" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="applications-machine-learning">Applications: Machine learning</h3>
<p>Machine learning has numerous buzzwords associated with it, including but not limited to: parameter optimization, stochastic gradient descent, automatic differentiation, and backpropagation. In this whole collage you can see a fraction of how matrix calculus applies to machine learning. It is recommended that you look into some of these topics yourself if you are interested.</p>
</section>
<section id="applications-physical-modeling" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="applications-physical-modeling">Applications: Physical modeling</h3>
<p>Large physical simulations, such as engineering-design problems, are increasingly characterized by <em>huge</em> numbers of parameters, and the <em>derivatives</em> of simulation outputs with respect to these parameters is crucial in order to evaluate sensitivity to uncertainties as well as to apply large-scale optimization.</p>
<p>For example, the shape of an airplane wing might be characterized by thousands of parameters, and if you can compute the derivative of the drag force (from a large fluid-flow simulation) with respect to these parameters then you could optimize the wing shape to minimize the drag for a given lift or other constraints.</p>
<p>An extreme version of such parameterization is known as “topology optimization,” in which the material at “every point” in space is potentially a degree of freedom, and optimizing over these parameters can discover not only a optimal shape but an optimal <em>topology</em> (how materials are connected in space, e.g.&nbsp;how many holes are present). For example, topology optimization has been applied in mechanical engineering to design the cross sections of airplane wings, artificial hips, and more into a complicated lattice of metal struts (e.g.&nbsp;minimizing weight for a given strength).</p>
<p>Besides engineering design, complicated differentiation problems arise in <em>fitting</em> unknown parameters of a model to experimental data, and also in <em>evaluating uncertainties</em> in the outputs of models with imprecise parameters/inputs. This is closely related to regression problems in statistics, as discussed below, except that here the model might be a giant set of differential equations with some unknown parameters.</p>
</section>
<section id="applications-data-science-and-multivariable-statistics" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="applications-data-science-and-multivariable-statistics">Applications: Data science and multivariable statistics</h3>
<p>In multivariate statistics, models are often framed in terms of matrix inputs and outputs (or even more complicated objects such as tensors). For example, a “simple” linear multivariate matrix model might be <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>X</mi><mi>B</mi><mo>+</mo><mi>U</mi></mrow><annotation encoding="application/x-tex">Y(X) = XB + U</annotation></semantics></math>, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> is an unknown matrix of coefficients (to be determined by some form of fit/regression) and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>U</mi><annotation encoding="application/x-tex">U</annotation></semantics></math> is unknown matrix of random noise (that prevents the model from exactly fitting the data). Regression then involves minimizing some function of the error <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>Y</mi><mo>−</mo><mi>X</mi><mi>B</mi></mrow><annotation encoding="application/x-tex">U(B) = Y - XB</annotation></semantics></math> between the model <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi>B</mi></mrow><annotation encoding="application/x-tex">XB</annotation></semantics></math> and data <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>; for example, a matrix norm <span class="math inline">$\Vert U\Vert_F^2 = \tr U^T U$</span>, a determinant <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>det</mo><msup><mi>U</mi><mi>T</mi></msup><mi>U</mi></mrow><annotation encoding="application/x-tex">\det U^T U</annotation></semantics></math>, or more complicated functions. Estimating the best-fit coefficients <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>, analyzing uncertainties, and many other statistical analyses require differentiating such functions with respect to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> or other parameters. A recent review article on this topic is Liu et&nbsp;al.&nbsp;(2022): “Matrix differential calculus with applications in the multivariate linear model and its diagnostics” (<a href="https://doi.org/10.1016/j.sctalk.2023.100274" class="uri">https://doi.org/10.1016/j.sctalk.2023.100274</a>).</p>
</section>
<section id="applications-automatic-differentiation" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="applications-automatic-differentiation">Applications: Automatic differentiation</h3>
<p>Typical differential calculus classes are based on symbolic calculus, with students essentially learning to do what Mathematica or Wolfram Alpha can do. Even if you are using a computer to take derivatives symbolically, to use this effectively you need to understand what is going on beneath the hood. But while, similarly, some numerics may show up for a small portion of this class (such as to approximate a derivative using the difference quotient), <em>today’s</em> automatic differentiation is neither of those two things. It is more in the field of the computer science topic of compiler technology than mathematics. However, the underlying mathematics of automatic differentiation is interesting, and we will learn about this in this class!</p>
<p>Even <em>approximate</em> computer differentiation is more complicated than you might expect. For single-variable functions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math>, derivatives are defined as the limit of a difference <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mi>/</mi><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">[f(x+\delta x)-f(x)]/\delta x</annotation></semantics></math> as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi><mo>→</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\delta x\to 0</annotation></semantics></math>. A crude “finite-difference” approximation is simply to approximate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)</annotation></semantics></math> by this formula for a small <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math>, but this turns out to raise many interesting issues involving balancing truncation and roundoff errors, higher-order approximations, and numerical extrapolation.</p>
</section>
</section>
<section id="first-derivatives" class="level2">
<h2 class="anchored" data-anchor-id="first-derivatives">First Derivatives</h2>
<p>The derivative of a function of one variable is itself a function of one variable– it simply is (roughly) defined as the linearization of a function. I.e., it is of the form <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>≈</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>−</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">(f(x)-f(x_0)) \approx f'(x_0) (x-x_0)</annotation></semantics></math>. In this sense, “everything is easy” with scalar functions of scalars (by which we mean, functions that take in one number and spit out one number).</p>
<p>There are occasionally other notations used for this linearization:</p>
<ul>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>y</mi><mo>≈</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta y \approx f'(x) \delta x</annotation></semantics></math>,</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>y</mi><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dy = f'(x) dx</annotation></semantics></math>,</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo>−</mo><msub><mi>y</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>≈</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>−</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">(y-y_0) \approx f'(x_0)(x-x_0)</annotation></semantics></math>,</p></li>
<li><p>and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">df = f'(x)  dx</annotation></semantics></math>.</p></li>
</ul>
<p>This last one will be the preferred of the above for this class. One can think of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>y</mi></mrow><annotation encoding="application/x-tex">dy</annotation></semantics></math> as “really small numbers.” In mathematics, they are called <a href="https://en.wikipedia.org/wiki/Infinitesimal">infinitesimals</a>, defined rigorously via taking limits. Note that here we do not want to divide by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math>. While this is completely fine to do with scalars, once we get to vectors and matrices you can’t always divide!</p>
<p>The numerics of such derivatives are simple enough to play around with. For instance, consider the function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>x</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">f(x) = x^2</annotation></semantics></math> and the point <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo>,</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo>,</mo><mn>9</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">(x_0,f(x_0)) = (3,9)</annotation></semantics></math>. Then, we have the following numerical values near <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo>,</mo><mn>9</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(3,9)</annotation></semantics></math>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mi>.</mi><mn>𝟎𝟎𝟎𝟏</mn><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mn>9</mn><mi>.</mi><mn>𝟎𝟎𝟎𝟔</mn><mn>0001</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mi>.</mi><mn>𝟎𝟎𝟎𝟎𝟏</mn><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mn>9</mn><mi>.</mi><mn>𝟎𝟎𝟎𝟎𝟔</mn><mn>00001</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mi>.</mi><mn>𝟎𝟎𝟎𝟎𝟎𝟏</mn><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mn>9</mn><mi>.</mi><mn>𝟎𝟎𝟎𝟎𝟎𝟔</mn><mn>000001</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mi>.</mi><mn>𝟎𝟎𝟎𝟎𝟎𝟎𝟏</mn><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mn>9</mn><mi>.</mi><mn>𝟎𝟎𝟎𝟎𝟎𝟎𝟔</mn><mn>0000001</mn><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    f(3.\mathbf{0001}) &amp;= 9.\mathbf{0006}0001 \\
    f(3.\mathbf{00001}) &amp;= 9.\mathbf{00006}00001 \\
    f(3.\mathbf{000001}) &amp;= 9.\mathbf{000006}000001 \\
    f(3.\mathbf{0000001}) &amp;= 9.\mathbf{0000006}0000001.
\end{aligned}</annotation></semantics></math> Here, the bolded digits on the left are <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\Delta x</annotation></semantics></math> and the bolded digits on the right are <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Δ</mi><mi>y</mi></mrow><annotation encoding="application/x-tex">\Delta y</annotation></semantics></math>. Notice that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Δ</mi><mi>y</mi><mo>=</mo><mn>6</mn><mi>Δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\Delta y = 6\Delta x</annotation></semantics></math>. Hence, we have that <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo>+</mo><mi>Δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>9</mn><mo>+</mo><mi>Δ</mi><mi>y</mi><mo>=</mo><mn>9</mn><mo>+</mo><mn>6</mn><mi>Δ</mi><mi>x</mi><mo>⟹</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo>+</mo><mi>Δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>6</mn><mi>Δ</mi><mi>x</mi><mo>≈</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>Δ</mi><mi>x</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">f(3+\Delta x) = 9 + \Delta y = 9+6\Delta x \implies f(3+\Delta x) - f(3) = 6\Delta x \approx f'(3)\Delta x.</annotation></semantics></math> Therefore, we have that the linearization of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>x</mi><mn>2</mn></msup><annotation encoding="application/x-tex">x^2</annotation></semantics></math> at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">x=3</annotation></semantics></math> is the function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>≈</mo><mn>6</mn><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>−</mo><mn>3</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)-f(3) \approx 6(x-3)</annotation></semantics></math>.</p>
<hr>
<p>We now leave the world of scalar calculus and enter the world of vector/matrix calculus! Professor Edelman invites us to think about matrices <em>holistically</em>—not just as a table of numbers.</p>
<p>The notion of linearizing your function will conceptually carry over as we define the derivative of functions which take in/spit out more than one number. Of course, this means that the derivative will have a different “shape” than a single number. Here is a table on the <em>shape</em> of the first derivative. The inputs of the function are given on the left hand side of the table, and the outputs of the function are given across the top.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 23%">
<col style="width: 23%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><p>2-4</p>
<p>input <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math> and output <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>→</mo><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></p></th>
<th style="text-align: left;">scalar</th>
<th style="text-align: left;">vector</th>
<th style="text-align: left;">matrix</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">scalar</td>
<td style="text-align: left;">scalar</td>
<td style="text-align: left;">vector (for instance, velocity)</td>
<td style="text-align: left;">matrix</td>
</tr>
<tr class="even">
<td style="text-align: left;">vector</td>
<td style="text-align: left;">gradient = (column) vector</td>
<td style="text-align: left;">matrix (called the Jacobian matrix)</td>
<td style="text-align: left;">higher order array</td>
</tr>
<tr class="odd">
<td style="text-align: left;">matrix</td>
<td style="text-align: left;">matrix</td>
<td style="text-align: left;">higher order array</td>
<td style="text-align: left;">higher order array</td>
</tr>
</tbody>
</table>
<p>You will ultimately learn how to do any of these in great detail eventually in this class! The purpose of this table is to plant the notion of differentials as linearization. Let’s look at an example.</p>
<div class="example">
<p>Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><mi>x</mi></mrow><annotation encoding="application/x-tex">f(x) = x^T x</annotation></semantics></math>, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> is a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">2\times 1</annotation></semantics></math> matrix and the output is thus a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times 1</annotation></semantics></math> matrix. Confirm that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><msubsup><mi>x</mi><mn>0</mn><mi>T</mi></msubsup><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">2x_0^T dx</annotation></semantics></math> is indeed the differential of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>3</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>4</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">x_0 = \begin{pmatrix}  3 &amp; 4\end{pmatrix}^T</annotation></semantics></math>.</p>
</div>
<p>Firstly, let’s compute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x_0)</annotation></semantics></math>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msubsup><mi>x</mi><mn>0</mn><mi>T</mi></msubsup><msub><mi>x</mi><mn>0</mn></msub><mo>=</mo><msup><mn>3</mn><mn>2</mn></msup><mo>+</mo><msup><mn>4</mn><mn>2</mn></msup><mo>=</mo><mn>25</mn><mi>.</mi></mrow><annotation encoding="application/x-tex">f(x_0) = x_0^T x_0 = 3^2 + 4^2 = 25.</annotation></semantics></math> Then, suppose <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mn>.001</mn><mo>,</mo><mn>.002</mn><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">dx = [.001,.002]</annotation></semantics></math>. Then, we would have that <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>3.001</mn><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>+</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>4.002</mn><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>=</mo><mn>25</mn><mi>.</mi><mn>𝟎𝟐𝟐</mn><mn>005</mn><mi>.</mi></mrow><annotation encoding="application/x-tex">f(x+dx) = (3.001)^2 + (4.002)^2 = 25.\mathbf{022}005.</annotation></semantics></math> Then, notice that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><msubsup><mi>x</mi><mn>0</mn><mi>T</mi></msubsup><mspace width="0.167em"></mspace><mi>d</mi><mi>x</mi><mo>=</mo><mn>2</mn><msup><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>3</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>4</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mi>d</mi><mi>x</mi><mo>=</mo><mn>.022</mn></mrow><annotation encoding="application/x-tex">2x_0^T \,dx = 2\begin{pmatrix}
    3  &amp; 4
\end{pmatrix}^T dx = .022</annotation></semantics></math>. Hence, we have that <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo>+</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>≈</mo><mn>2</mn><msubsup><mi>x</mi><mn>0</mn><mi>T</mi></msubsup><mi>d</mi><mi>x</mi><mo>=</mo><mn>.022</mn><mi>.</mi></mrow><annotation encoding="application/x-tex">f(x_0 + dx) - f(x_0) \approx 2x_0^{T} dx = .022.</annotation></semantics></math> As we will see right now, the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><msubsup><mi>x</mi><mn>0</mn><mi>T</mi></msubsup><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">2x_0^Tdx</annotation></semantics></math> didn’t come from nowhere!</p>
</section>
<section id="intro-matrix-and-vector-product-rule" class="level2">
<h2 class="anchored" data-anchor-id="intro-matrix-and-vector-product-rule">Intro: Matrix and Vector Product Rule</h2>
<p>For matrices, we in fact still have a product rule! We will discuss this in much more detail in later chapters, but let’s begin here with a small taste.</p>
<div class="theorem">
<p>Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>,</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A,B</annotation></semantics></math> be two matrices. Then, we have the differential product rule for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>B</mi></mrow><annotation encoding="application/x-tex">AB</annotation></semantics></math>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>B</mi><mo>+</mo><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">d(AB) = (dA)B + A(dB).</annotation></semantics></math> By the differential of the matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>, we think of it as a small (unconstrained) change in the matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">A.</annotation></semantics></math> Later, constraints may be places on the allowed perturbations.</p>
</div>
<p>Notice however, that (by our table) the derivative of a matrix is a matrix! So generally speaking, the products will not commute.</p>
<p>If <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> is a vector, then by the differential product rule we have <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>x</mi><mi>T</mi></msup><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><msup><mi>x</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>x</mi><mo>+</mo><msup><mi>x</mi><mi>T</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">d(x^Tx) = (dx^T)x + x^T (dx).</annotation></semantics></math> However, notice that this is a dot product, and dot products commute (since <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∑</mo><msub><mi>a</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>b</mi><mi>i</mi></msub><mo>=</mo><mo>∑</mo><msub><mi>b</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>a</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\sum a_i \cdot b_i = \sum b_i \cdot a_i</annotation></semantics></math>), we have that <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>x</mi><mi>T</mi></msup><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mi>d</mi><mi>x</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">d(x^T x) = (2x)^T dx.</annotation></semantics></math></p>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>The way the product rule works for vectors as matrices is that transposes “go for the ride.” See the next example below.</p>
</div>
<div class="example">
<p>By the product rule. we have</p>
<ol type="1">
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>u</mi><mi>T</mi></msup><mi>v</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mi>v</mi><mo>+</mo><msup><mi>u</mi><mi>T</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>v</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>v</mi><mi>T</mi></msup><mi>d</mi><mi>u</mi><mo>+</mo><msup><mi>u</mi><mi>T</mi></msup><mi>d</mi><mi>v</mi></mrow><annotation encoding="application/x-tex">d(u^Tv) = (du)^T v + u^T (dv) = v^T du + u^T dv</annotation></semantics></math> since dot products commute.</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><msup><mi>v</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>v</mi><mi>T</mi></msup><mo>+</mo><mi>u</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>v</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">d(uv^T) = (du )v^T + u( dv)^T.</annotation></semantics></math></p></li>
</ol>
</div>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>The way to prove these sorts of statements can be seen in Section <a href="#sec1S" data-reference-type="ref" data-reference="sec1S">2</a>.</p>
</div>
</section>
</section>
<section id="sec1S" class="level1">
<h1>Derivatives as Linear Operators</h1>
<p>We are now going to revisit the notion of a derivative in a way that we can generalize to higher-order arrays and other vector spaces. We will get into more detail on differentiation as a linear operator, and in particular, will dive deeper into some of the facts we have stated thus far.</p>
<section id="revisiting-single-variable-calculus" class="level2">
<h2 class="anchored" data-anchor-id="revisiting-single-variable-calculus">Revisiting single-variable calculus</h2>
<div id="fig:derivative-linearization" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/derivative-linearization.pdf.svg" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>The essence of a derivative is <em>linearization</em>: predicting a small change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\delta f</annotation></semantics></math> in the output <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math> from a small change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math> in the input <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>, to <em>first order</em> in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math>.</figcaption>
</figure>
</div>
<p>In a first-semester single-variable calculus course (like 18.01 at MIT), the derivative <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)</annotation></semantics></math> is introduced as the slope of the tangent line at the point <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(x,f(x))</annotation></semantics></math>, which can also be viewed as a <em>linear approximation</em> of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> near <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>. In particular, as depicted in Fig.&nbsp;<a href="#fig:derivative-linearization" data-reference-type="ref" data-reference="fig:derivative-linearization">1</a>, this is equivalent to a prediction of the <em>change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\delta f</annotation></semantics></math> in the “output”</em> of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math> from a small <em>change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math> in the “input”</em> to first order (<em>linear</em>) in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>f</mi><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>δ</mi><mi>x</mi><mo>+</mo><munder><munder><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">higher-order terms</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo accent="true">⏟</mo></munder><mrow><mi>o</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></munder><mi>.</mi></mrow><annotation encoding="application/x-tex">\delta f = f(x+\delta x) - f(x) =  f'(x) \, \delta x  + \underbrace{(\text{higher-order terms})}_{o(\delta x)}.</annotation></semantics></math> We can more precisely express these higher-order terms using asymptotic “little-o” notation “<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>o</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">o(\delta x)</annotation></semantics></math>”, which denotes any function whose magnitude shrinks much faster than <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">|</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">|</mo></mrow><annotation encoding="application/x-tex">|\delta x|</annotation></semantics></math> as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi><mo>→</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\delta x \to 0</annotation></semantics></math>, so that for sufficiently small <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math> it is negligible compared to the linear <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">f'(x) \, \delta x</annotation></semantics></math> term. (Variants of this notation are commonly used in computer science, and there is a formal definition that we omit here.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>) Examples of such higher-order terms include <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><annotation encoding="application/x-tex">(\delta x)^2</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>3</mn></msup><annotation encoding="application/x-tex">(\delta x)^3</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>1.001</mn></msup><annotation encoding="application/x-tex">(\delta x)^{1.001}</annotation></semantics></math>, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">(\delta x)/\log(\delta x)</annotation></semantics></math>.</p>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>Here, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math> is <em>not</em> an infinitesimal but rather a small number. Note that our symbol “<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>δ</mi><annotation encoding="application/x-tex">\delta</annotation></semantics></math>” (a Greek lowercase “delta”) is <em>not</em> the same as the symbol “<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>∂</mi><annotation encoding="application/x-tex">\partial</annotation></semantics></math>” commonly used to denote partial derivatives.</p>
</div>
<p>This notion of a derivative may remind you of the first two terms in a Taylor series <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>δ</mi><mi>x</mi><mo>+</mo><mi>⋯</mi></mrow><annotation encoding="application/x-tex">f(x+\delta x) = f(x) + f'(x) \, \delta x + \cdots</annotation></semantics></math> (though in fact it is much more basic than Taylor series!), and the notation will generalize nicely to higher dimensions and other vector spaces. In differential notation, we can express the same idea as: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>d</mi><mi>x</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">df = f(x+dx) - f(x) = f'(x) \, dx.</annotation></semantics></math> In this notation we implicitly drop the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>o</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">o(\delta x)</annotation></semantics></math> term that vanishes in the limit as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math> becomes infinitesimally small.</p>
<p>We will use this as the more generalized definition of a derivative. In this formulation, we avoid <em>dividing</em> by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math>, because soon we will allow <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> (and hence <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math>) to be something other than a number—if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> is a vector, we won’t be <em>able</em> to divide by it!</p>
</section>
<section id="linear-operators" class="level2">
<h2 class="anchored" data-anchor-id="linear-operators">Linear operators</h2>
<p>From the perspective of linear algebra, given a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>, we consider the derivative of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>, to be the <em>linear operator</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)</annotation></semantics></math> such that <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">df = f(x+ dx) - f(x) = f'(x)[dx].</annotation></semantics></math></p>
<p>As above, you should think of the differential notation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> as representing an <em>arbitrary small</em> change in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>, where we are implicitly dropping any <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>o</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">o(dx)</annotation></semantics></math> terms, i.e.&nbsp;terms that decay faster than linearly as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi><mo>→</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">dx\to 0</annotation></semantics></math>. Often, we will omit the square brackets and write simply <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">f'(x)dx</annotation></semantics></math> instead of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)[dx]</annotation></semantics></math>, but this should be understood as the linear operator <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)</annotation></semantics></math> <em>acting on</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math>—don’t write <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi><mspace width="0.167em"></mspace><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">dx\,f'(x)</annotation></semantics></math>, which will generally be nonsense!</p>
<p>This definition will allow us to extend differentiation to <em>arbitrary vector spaces</em> of inputs <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> and outputs <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math>. (More technically, we will require vector spaces with a norm <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">‖</mo><mi>x</mi><mo stretchy="false" form="postfix">‖</mo></mrow><annotation encoding="application/x-tex">\Vert x \Vert</annotation></semantics></math>, called “Banach spaces,” in order to precisely define the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>o</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">o(\delta x)</annotation></semantics></math> terms that are dropped. We will come back to the subject of Banach spaces later.)</p>
<div class="recall">
<p>Loosely, a vector space (over <span class="math inline">$\R$</span>) is a set of elements in which addition and subtraction between elements is defined, along with multiplication by real scalars. For instance, while it does not make sense to multiply arbitrary vectors in <span class="math inline">$\R^n$</span>, we can certainly add them together, and we can certainly scale the vectors by a constant factor.</p>
</div>
<p>Some examples of vector spaces include:</p>
<ul>
<li><p><span class="math inline">$\R^n$</span>, as described in the above. More generally, <span class="math inline">$\R^{n\times m}$</span>, the space of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">n\times m</annotation></semantics></math> matrices with real entries. Notice again that, if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>≠</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">n\neq m</annotation></semantics></math>, then multiplication between elements is not defined.</p></li>
<li><p><span class="math inline">$C^0(\R^n)$</span>, the set of continuous functions over <span class="math inline">$\R^n$</span>, with addition defined pointwise.</p></li>
</ul>
<div class="recall">
<p>Recall that a linear operator is a map <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> from a vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> in vector space <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math> to a vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>v</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">L[v]</annotation></semantics></math> (sometimes denoted simply <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>v</mi></mrow><annotation encoding="application/x-tex">Lv</annotation></semantics></math>) in some other vector space. Specifically, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> is linear if <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>v</mi><mn>1</mn></msub><mo>+</mo><msub><mi>v</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>L</mi><msub><mi>v</mi><mn>1</mn></msub><mo>+</mo><mi>L</mi><msub><mi>v</mi><mn>2</mn></msub><mtext mathvariant="normal">&nbsp;&nbsp;and&nbsp;&nbsp;</mtext><mi>L</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>α</mi><mi>v</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>α</mi><mi>L</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>v</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">L[v_1+v_2] = Lv_1 + Lv_2 \text{~~and~~} L[\alpha v] = \alpha L[v]</annotation></semantics></math> for scalars <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">\alpha \in \mathbb{R}</annotation></semantics></math>.</p>
<p><em>Remark</em>: In this course, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math> is a map that takes in an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> and spits out a linear operator <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)</annotation></semantics></math> (the <strong>derivative</strong> of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>). Furthermore, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)</annotation></semantics></math> is a linear map that takes in an input direction <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> and gives an output vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>v</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)[v]</annotation></semantics></math> (which we will later interpret as a directional derivative, see Sec.&nbsp;<a href="#sec:directional" data-reference-type="ref" data-reference="sec:directional">2.2.1</a>). When the direction <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> is an infinitesimal <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">d x</annotation></semantics></math>, the output <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>d</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">f'(x)[dx] = df</annotation></semantics></math> is the <strong>differential</strong> of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> (the corresponding infinitesimal change in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>).</p>
</div>
<div class="notation">
<p>There are multiple notations for derivatives in common use, along with multiple related concepts of derivative, differentiation, and differentials. In the table below, we summarize several of these notations, and put <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><menclose notation="box"><mrow><mi mathvariant="normal">b</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">x</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">s</mi></mrow></menclose><annotation encoding="application/x-tex">\boxed{\mathrm{boxes}}</annotation></semantics></math> around the notations adopted for this course:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">name</th>
<th style="text-align: left;">notations</th>
<th style="text-align: left;">remark</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">derivative</td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><menclose notation="box"><mrow><mi>f</mi><mi>′</mi></mrow></menclose><annotation encoding="application/x-tex">\boxed{f'}</annotation></semantics></math>, also <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>d</mi><mi>f</mi></mrow><mrow><mi>d</mi><mi>x</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{df}{dx}</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">Df</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>f</mi><mi>x</mi></msub><annotation encoding="application/x-tex">f_{x}</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∂</mi><mi>x</mi></msub><mi>f</mi></mrow><annotation encoding="application/x-tex">\partial_{x}f</annotation></semantics></math>,</td>
<td style="text-align: left;">linear operator <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)</annotation></semantics></math> that maps a small change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> in the input to a small</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">…</td>
<td style="text-align: left;">change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">{df=f'(x)[dx]}</annotation></semantics></math> in the output</td>
</tr>
</tbody>
</table>
<pre><code>                                                                                         In single-variable calculus, this linear operator can be represented by a *single number*, the "slope," e.g. if $f(x) = \sin(x)$ then $f'(x) = \cos(x)$ is the number that we multiply by $dx$ to get $dy = \cos(x) dx$. In multi-variable calculus, the linear operator $f'(x)$ can be represented by a *matrix*, the **Jacobian** $J$ (see Sec.&nbsp;[3](#sec:kronecker){reference-type="ref" reference="sec:kronecker"}), so that $df = f'(x)[dx] = J\, dx$. But we will see that it is not always convenient to express $f'$ as a matrix, even if we can.</code></pre>
<p>differentiation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><menclose notation="box"><msup><mi></mi><mi>′</mi></msup></menclose><annotation encoding="application/x-tex">\boxed{^{\prime}}</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mi>d</mi><mrow><mi>d</mi><mi>x</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{d}{dx}</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>, … linear operator that maps a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> to its derivative <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math> difference <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><menclose notation="box"><mrow><mi>δ</mi><mi>x</mi></mrow></menclose><annotation encoding="application/x-tex">\boxed{\delta x}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><menclose notation="box"><mrow><mi>δ</mi><mi>f</mi></mrow></menclose><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\boxed{\delta f} = f(x+\delta x) - f(x)</annotation></semantics></math> small (but <em>not</em> infinitesimal) change in the input <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> and output <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> (depending implicitly on <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math>), respectively: an element of a vector space, <em>not</em> a linear operator differential <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><menclose notation="box"><mrow><mi>d</mi><mi>x</mi></mrow></menclose><annotation encoding="application/x-tex">\boxed{d x}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><menclose notation="box"><mrow><mi>d</mi><mi>f</mi></mrow></menclose><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\boxed{d f} = f(x+dx) - f(x)</annotation></semantics></math> arbitrarily small (“infinitesimal”<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> — we drop higher-order terms) change in the input <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> and output <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>, respectively: an element of a vector space, <em>not</em> a linear operator gradient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><menclose notation="box"><mrow><mi>∇</mi><mi>f</mi></mrow></menclose><annotation encoding="application/x-tex">\boxed{\nabla f}</annotation></semantics></math> the vector whose inner product <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mo stretchy="false" form="prefix">⟨</mo><mi>∇</mi><mi>f</mi><mo>,</mo><mi>d</mi><mi>x</mi><mo stretchy="false" form="postfix">⟩</mo></mrow><annotation encoding="application/x-tex">df = \langle \nabla f, dx \rangle</annotation></semantics></math> with a small change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> in the input gives the small change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">df</annotation></semantics></math> in the output. The “transpose of the derivative” <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>f</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\nabla f = (f')^T</annotation></semantics></math>. (See Sec.&nbsp;<a href="#sec:multivarPart1" data-reference-type="ref" data-reference="sec:multivarPart1">2.3</a>.) partial derivative <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><menclose notation="box"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></menclose><annotation encoding="application/x-tex">\boxed{\frac{\partial f}{\partial x}}</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>f</mi><mi>x</mi></msub><annotation encoding="application/x-tex">f_{x}</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∂</mi><mi>x</mi></msub><mi>f</mi></mrow><annotation encoding="application/x-tex">\partial_{x}f</annotation></semantics></math> linear operator that maps a small change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> in a <em>single argument</em> of a multi-argument function to the corresponding change in output, e.g.&nbsp;for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x,y)</annotation></semantics></math> we have <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><mi>x</mi></mrow></mfrac><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><mi>y</mi></mrow></mfrac><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>y</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">df=\frac{\partial f}{\partial x}[dx]+\frac{\partial f}{\partial y}[dy]</annotation></semantics></math>.</p>
</div>
<p>Some examples of linear operators include</p>
<ul>
<li><p>Multiplication by scalars <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>, i.e.&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>v</mi><mo>=</mo><mi>α</mi><mi>v</mi></mrow><annotation encoding="application/x-tex">Lv = \alpha v</annotation></semantics></math>. Also multiplication of column vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> by matrices <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>, i.e.&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>v</mi><mo>=</mo><mi>A</mi><mi>v</mi></mrow><annotation encoding="application/x-tex">Lv = Av</annotation></semantics></math>.</p></li>
<li><p>Some functions like <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>x</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">f(x)=x^2</annotation></semantics></math> are obviously nonlinear. But what about <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>x</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">f(x)=x+1</annotation></semantics></math>? This may <em>look</em> linear if you plot it, but it is <em>not</em> a linear operation, because <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>2</mn><mi>x</mi><mo>+</mo><mn>1</mn><mo>≠</mo><mn>2</mn><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(2x)=2x+1\ne 2f(x)</annotation></semantics></math>—such functions, which are linear <em>plus a nonzero constant</em>, are known as <em>affine</em>.</p></li>
<li><p>There are also many other examples of linear operations that are not so convenient or easy to write down as matrix–vector products. For example, if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math> matrix, then <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>A</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>A</mi><mi>B</mi><mo>+</mo><mi>C</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">L[A]=AB+CA</annotation></semantics></math> is a linear operator given <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3\times 3</annotation></semantics></math> matrices <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo>,</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">B,C</annotation></semantics></math>. The transpose <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">f(x)=x^T</annotation></semantics></math> of a column vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> is linear, but is not given by any matrix multiplied by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>. Or, if we consider vector spaces of <em>functions</em>, then the calculus operations of differentiation and integration are linear operators too!</p></li>
</ul>
<section id="sec:directional" class="level3">
<h3 class="anchored" data-anchor-id="sec:directional">Directional derivatives</h3>
<p>There is an equivalent way to interpret this linear-operator viewpoint of a derivative, which you may have seen before in multivariable calculus: as a <strong>directional derivative</strong>.</p>
<p>If we have a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math> of arbitrary vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>, then the directional derivative at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> in a direction (vector) <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> is defined as: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mfrac><mi>∂</mi><mrow><mi>∂</mi><mi>α</mi></mrow></mfrac><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>α</mi><mi>v</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mi>α</mi><mo>=</mo><mn>0</mn></mrow></msub><mo>=</mo><munder><mo>lim</mo><mrow><mi>δ</mi><mi>α</mi><mo>→</mo><mn>0</mn></mrow></munder><mfrac><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>α</mi><mspace width="0.167em"></mspace><mi>v</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>δ</mi><mi>α</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\left. \frac{\partial}{\partial \alpha} f(x+\alpha v) \right|_{\alpha = 0} = \lim_{\delta\alpha\to 0} \frac{f(x+\delta\alpha \, v) - f(x)}{\delta \alpha}</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> is a <em>scalar</em>. This transforms derivatives back into <em>single-variable calculus</em> from arbitrary vector spaces. It measures the rate of change of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> in the direction&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> from&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>. But it turns out that this has a very simple relationship to our linear operator <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)</annotation></semantics></math> from above, because (dropping higher-order terms due to the limit <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>α</mi><mo>→</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\delta\alpha \to 0</annotation></semantics></math>): <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><munder><munder><mrow><mi>d</mi><mi>α</mi><mspace width="0.167em"></mspace><mi>v</mi></mrow><mo accent="true">⏟</mo></munder><mrow><mi>d</mi><mi>x</mi></mrow></munder><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>d</mi><mi>α</mi><mspace width="0.167em"></mspace><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>v</mi><mo stretchy="true" form="postfix">]</mo></mrow><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">f(x + \underbrace{d\alpha \, v}_{d x}) - f(x) = f'(x)[dx] = d\alpha \, f'(x)[v] \, ,</annotation></semantics></math> where we have factored out the scalar <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>α</mi></mrow><annotation encoding="application/x-tex">d\alpha</annotation></semantics></math> in the last step thanks to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)</annotation></semantics></math> being a <em>linear</em> operator. Comparing with above, we immediately find that the directional derivative is: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><menclose notation="box"><mrow><msub><mrow><mfrac><mi>∂</mi><mrow><mi>∂</mi><mi>α</mi></mrow></mfrac><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>α</mi><mi>v</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mi>α</mi><mo>=</mo><mn>0</mn></mrow></msub><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>v</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow></menclose><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">\boxed{ \left. \frac{\partial}{\partial \alpha} f(x+\alpha v) \right|_{\alpha = 0} = f'(x)[v]  }\, .</annotation></semantics></math> It is <em>exactly equivalent</em> to our <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)</annotation></semantics></math> from before! (We can also see this as an instance of the chain rule from Sec.&nbsp;<a href="#sec:chainrule" data-reference-type="ref" data-reference="sec:chainrule">2.5</a>.) One lesson from this viewpoint is that it is perfectly reasonable to input an arbitrary <em>non-infinitesimal</em> vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> into <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>v</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)[v]</annotation></semantics></math>: the result is not a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">df</annotation></semantics></math>, but is simply a directional derivative.</p>
</section>
</section>
<section id="sec:multivarPart1" class="level2">
<h2 class="anchored" data-anchor-id="sec:multivarPart1">Revisiting multivariable calculus, Part 1: Scalar-valued functions</h2>
<div id="fig:gradient-uphill" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/gradient-uphill.pdf.svg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>For a real-valued <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math>, the gradient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> is defined so that it corresponds to the “uphill” direction at a point <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>, which is perpendicular to the contours of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>. Although this may not point exactly towards the nearest local maximum of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> (unless the contours are circular), “going uphill” is nevertheless the starting point for many computational-optimization algorithms to search for a maximum.</figcaption>
</figure>
</div>
<p>Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> be a scalar-valued function, which takes in “column” vectors <span class="math inline">$x \in \R^n$</span> and produces a scalar (in <span class="math inline">$\R$</span>). Then, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mtext mathvariant="normal">scalar</mtext><mi>.</mi></mrow><annotation encoding="application/x-tex">df = f(x+ dx) - f(x) = f'(x) [dx] = \text{scalar}.</annotation></semantics></math> Therefore, since <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> is a column vector (in an arbitrary direction, representing an arbitrary small change in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>), the linear operator <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)</annotation></semantics></math> that produces a scalar <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">df</annotation></semantics></math> must be a <strong>row vector</strong> (a “1-row matrix”, or more formally something called a <em>covector</em> or “dual” vector or “linear form”)! We call this row vector the <em>transpose of the gradient</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><annotation encoding="application/x-tex">(\nabla f)^T</annotation></semantics></math>, so that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">df</annotation></semantics></math> is the <em>dot (“inner”) product of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> with the gradient</em>. So we have that <span class="math display">$$df = \nabla f \cdot dx = \underbrace{(\nabla f)^T}_{f'(x)} dx \hspace{.25cm} \text{where} \hspace{.25cm} dx = \begin{pmatrix}
dx_1 \\  dx_2 \\ \vdots \\ dx_n.
\end{pmatrix} .$$</span> Some authors view the gradient as a row vector (equating it with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math> or the Jacobian), but treating it as a “column vector” (the transpose of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math>), as we do in this course, is a common and useful choice. As a column vector, the gradient can be viewed as the “uphill” (<em>steepest-ascent</em>) direction in the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> space, as depicted in Fig.&nbsp;<a href="#fig:gradient-uphill" data-reference-type="ref" data-reference="fig:gradient-uphill">2</a>. Furthermore, it is also easier to generalize to scalar functions of other vector spaces. In any case, for this class, we will <em>always</em> define <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> to <em>have the same “shape” as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math></em>, so that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">df</annotation></semantics></math> is a dot product (“inner product”) of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> with the gradient.</p>
<p>This is perfectly consistent with the viewpoint of the gradient that you may remember from multivariable calculus, in which the gradient was a vector of components <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mn>1</mn></msub></mrow></mfrac></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mn>2</mn></msub></mrow></mfrac></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mi>n</mi></msub></mrow></mfrac></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mo>;</mo></mrow><annotation encoding="application/x-tex">\nabla f = \begin{pmatrix}
\frac{\partial f}{\partial x_1} \\  \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n}
\end{pmatrix} \, ;</annotation></semantics></math> or, equivalently, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>∇</mi><mi>f</mi><mo>⋅</mo><mi>d</mi><mi>x</mi><mo>=</mo><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mn>1</mn></msub></mrow></mfrac><mi>d</mi><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mn>2</mn></msub></mrow></mfrac><mi>d</mi><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mi>⋯</mi><mo>+</mo><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mi>n</mi></msub></mrow></mfrac><mi>d</mi><msub><mi>x</mi><mi>n</mi></msub><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">df = f(x+dx)-f(x) = \nabla f \cdot dx = \frac{\partial f}{\partial x_1} dx_1 + \frac{\partial f}{\partial x_2} dx_2 + \cdots + \frac{\partial f}{\partial x_n} dx_n \, .</annotation></semantics></math> While a component-wise viewpoint may sometimes be convenient, we want to encourage you to view the vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> as a <em>whole</em>, not simply a collection of components, and to learn that it is often more convenient and elegant to differentiate expressions <em>without</em> taking the derivative component-by-component, a new approach that will generalize better to more complicated inputs/output vector spaces.</p>
<p>Let’s look at an example to see how we compute this differential.</p>
<div class="example">
<p>Consider <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><mi>A</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">f(x) = x^T Ax</annotation></semantics></math> where <span class="math inline">$x\in \R^n$</span> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is a square <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n\times n</annotation></semantics></math> matrix, and thus <span class="math inline">$f(x) \in \R$</span>. Compute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">df</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)</annotation></semantics></math>, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math>.</p>
</div>
<p>We can do this directly from the definition. <span class="math display">$$\begin{aligned}
    df &amp;= f(x+ dx) - f(x) \\
    &amp;=  (x+ dx)^T A (x+dx) - x^T A x \\
    &amp;= \cancel{x^T A x} + dx^T\, Ax + x^T A dx + \cancelto{\text{higher order}}{dx^T \,A dx} - \cancel{x^T A x} \\
    &amp;= \underbrace{x^T(A+A^T)}_{f'(x)=(\nabla f)^T }dx \implies \nabla f = (A+A^T)x \, .
\end{aligned}$$</span> Here, we dropped terms with more than one <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> factor as these are asymptotically negligible. Another trick was to combine <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>x</mi><mi>T</mi></msup><mi>A</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx^T A x</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mi>T</mi></msup><mi>A</mi><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">x^T A dx</annotation></semantics></math> by realizing that these are <em>scalars</em> and hence equal to their own transpose: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>x</mi><mi>T</mi></msup><mi>A</mi><mi>x</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><msup><mi>x</mi><mi>T</mi></msup><mi>A</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><msup><mi>A</mi><mi>T</mi></msup><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx^T A x = (dx^T A x)^T = x^T A^T dx</annotation></semantics></math>. Hence, we have found that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>+</mo><msup><mi>A</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">f'(x) = x^T(A+A^T) = (\nabla f)^T</annotation></semantics></math>, or equivalently <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">[</mo><msup><mi>x</mi><mi>T</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>+</mo><msup><mi>A</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mi>T</mi></msup><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>+</mo><msup><mi>A</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">\nabla f = [x^T(A+A^T)]^T = (A+A^T)x</annotation></semantics></math>.</p>
<p>It is, of course, also possible to compute the same gradient component-by-component, the way you probably learned to do in multivariable calculus. First, you would need to write <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math> explicitly in terms of the components of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>, as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><mi>A</mi><mi>x</mi><mo>=</mo><msub><mo>∑</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><msub><mi>x</mi><mi>i</mi></msub><msub><mi>A</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><msub><mi>x</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">f(x) = x^T A x = \sum_{i,j} x_i A_{i,j} x_j</annotation></semantics></math>. Then, you would compute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∂</mi><mi>f</mi><mi>/</mi><mi>∂</mi><msub><mi>x</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\partial f/\partial x_k</annotation></semantics></math> for each <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>, taking care that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> appears twice in the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> summation. However, this approach is awkward, error-prone, labor-intensive, and quickly becomes worse as we move on to more complicated functions. It is much better, we feel, to get used to treating vectors and matrices <em>as a whole</em>, not as mere collections of numbers.</p>
</section>
<section id="revisiting-multivariable-calculus-part-2-vector-valued-functions" class="level2">
<h2 class="anchored" data-anchor-id="revisiting-multivariable-calculus-part-2-vector-valued-functions">Revisiting multivariable calculus, Part 2: Vector-valued functions</h2>
<p>Next time, we will revisit multi-variable calculus (18.02 at MIT) again in a Part 2, where now <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> will be a vector-valued function, taking in vectors <span class="math inline">$x\in \R^n$</span> and giving vector outputs <span class="math inline">$f(x) \in \R^m$</span>. Then, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">df</annotation></semantics></math> will be a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>-component column vector, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> will be an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-component column vector, and we must get a linear operator <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)</annotation></semantics></math> satisfying <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><munder><munder><mrow><mi>d</mi><mi>f</mi></mrow><mo accent="true">⏟</mo></munder><mrow><mi>m</mi><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> components</mtext></mrow></mrow></munder><mo>=</mo><munder><munder><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mo accent="true">⏟</mo></munder><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></munder><munder><munder><mrow><mi>d</mi><mi>x</mi></mrow><mo accent="true">⏟</mo></munder><mrow><mi>n</mi><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> components</mtext></mrow></mrow></munder><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">\underbrace{df}_{m\text{ components}} = \underbrace{f'(x)}_{m \times n} \underbrace{dx}_{n\text{ components}} \, ,</annotation></semantics></math> so <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)</annotation></semantics></math> must be an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m \times n</annotation></semantics></math> <em>matrix</em> called the <em>Jacobian</em> of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>!</p>
<p>The Jacobian matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>J</mi><annotation encoding="application/x-tex">J</annotation></semantics></math> represents the linear operator that takes <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">df</annotation></semantics></math>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>J</mi><mi>d</mi><mi>x</mi><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">df = J dx \, .</annotation></semantics></math> The matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>J</mi><annotation encoding="application/x-tex">J</annotation></semantics></math> has entries <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>J</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>∂</mi><msub><mi>f</mi><mi>i</mi></msub></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mi>j</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">J_{ij}=\frac{\partial f_i}{\partial x_j}</annotation></semantics></math> (corresponding to the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>-th row and the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>-th column of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>J</mi><annotation encoding="application/x-tex">J</annotation></semantics></math>).</p>
<p>So now, suppose that <span class="math inline">$f: \R^2 \to \R^2$</span>. Let’s understand how we would compute the differential of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>∂</mi><msub><mi>f</mi><mn>1</mn></msub></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mn>1</mn></msub></mrow></mfrac></mtd><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>∂</mi><msub><mi>f</mi><mn>1</mn></msub></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mn>2</mn></msub></mrow></mfrac></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>∂</mi><msub><mi>f</mi><mn>2</mn></msub></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mn>1</mn></msub></mrow></mfrac></mtd><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>∂</mi><msub><mi>f</mi><mn>2</mn></msub></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mn>2</mn></msub></mrow></mfrac></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>d</mi><msub><mi>x</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>d</mi><msub><mi>x</mi><mn>2</mn></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>∂</mi><msub><mi>f</mi><mn>1</mn></msub></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mn>1</mn></msub></mrow></mfrac><mi>d</mi><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mfrac><mrow><mi>∂</mi><msub><mi>f</mi><mn>1</mn></msub></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mn>2</mn></msub></mrow></mfrac><mi>d</mi><msub><mi>x</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>∂</mi><msub><mi>f</mi><mn>2</mn></msub></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mn>1</mn></msub></mrow></mfrac><mi>d</mi><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mfrac><mrow><mi>∂</mi><msub><mi>f</mi><mn>2</mn></msub></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mn>2</mn></msub></mrow></mfrac><mi>d</mi><msub><mi>x</mi><mn>2</mn></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">df = \begin{pmatrix}
   \frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_1}{\partial x_2} \\ \frac{\partial f_2}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_2}
\end{pmatrix} \begin{pmatrix}
     dx_1 \\ dx_2
\end{pmatrix} = \begin{pmatrix}
    \frac{\partial f_1}{\partial x_1}dx_1 +  \frac{\partial f_1}{\partial x_2}dx_2 \\ \frac{\partial f_2}{\partial x_1}dx_1 +  \frac{\partial f_2}{\partial x_2} dx_2
\end{pmatrix}.</annotation></semantics></math></p>
<p>Let’s compute an example.</p>
<div class="example">
<p><span id="ex1" data-label="ex1"></span> Consider the function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>A</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">f(x) = Ax</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is a constant <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m\times n</annotation></semantics></math> matrix. Then, by applying the distributive law for matrix–vector products, we have <span class="math display">$$\begin{aligned}
    df &amp;= f(x+dx ) - f(x) =  A(x + dx) - Ax  \\
    &amp;= \cancel{Ax} + Adx - \cancel{Ax} = Adx = f'(x) dx.
\end{aligned}$$</span> Therefore, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>A</mi></mrow><annotation encoding="application/x-tex">f'(x) = A</annotation></semantics></math>.</p>
</div>
<p>Notice then that the linear operator <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is its own Jacobian matrix!</p>
<p>Let’s now consider some derivative rules.</p>
<ul>
<li><p><strong>Sum Rule</strong>: Given <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x) = g(x) + h(x)</annotation></semantics></math>, we get that <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>d</mi><mi>g</mi><mo>+</mo><mi>d</mi><mi>h</mi><mo>⟹</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>x</mi><mo>=</mo><mi>g</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>x</mi><mo>+</mo><mi>h</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>x</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">df = dg + dh \implies f'(x) dx = g'(x) dx + h'(x)dx.</annotation></semantics></math> Hence, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mo>=</mo><mi>g</mi><mi>′</mi><mo>+</mo><mi>h</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f' = g'+h'</annotation></semantics></math> as we should expect. This is the linear operator <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>v</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>g</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>v</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><mi>h</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>v</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)[v] = g'(x)[v] + h'(x)[v]</annotation></semantics></math>, and note that we can sum linear operators (like <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">g'</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">h'</annotation></semantics></math>) just like we can sum matrices! In this way, linear operators form a vector space.</p></li>
<li><p><strong>Product Rule</strong>: Suppose <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x) = g(x) h(x)</annotation></semantics></math>. Then, $$</p>
<span class="math display">$$\begin{aligned}
        df &amp;= f(x+ dx) - f(x) \\
        &amp;= g(x+dx) h(x+dx) - g(x) h(x) \\
        &amp;= (g(x) + \underbrace{g'(x) dx}_{dg}) (h(x) + \underbrace{h'(x)dx}_{dh}) - g(x)h(x) \\
        &amp;= g h + dg\, h + g\,dh + \cancelto{0}{dg \,dh} - gh \\
        &amp;= dg \, h + g \,dh \, ,

\end{aligned}$$</span>
<p>$$ where the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>g</mi><mspace width="0.167em"></mspace><mi>d</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">dg \, dh</annotation></semantics></math> term is higher-order and hence dropped in infinitesimal notation. Note, as usual, that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>g</mi></mrow><annotation encoding="application/x-tex">dg</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math> may not commute now as they may no longer be scalars!</p></li>
</ul>
<p>Let’s look at some short examples of how we can apply the product rule nicely.</p>
<div class="example">
<p><span id="ex:Ax" data-label="ex:Ax"></span> Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>A</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">f(x) = Ax</annotation></semantics></math> (mapping <span class="math inline">$\R^n \to \R^m$</span>) where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is a constant <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m\times n</annotation></semantics></math> matrix. Then, <span class="math display">$$df = d(Ax) =  \cancelto{0}{dA} \, x + Adx = Adx \implies f'(x) = A.$$</span> We have <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi>d</mi><mi>A</mi></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">{dA} = 0</annotation></semantics></math> here because <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> does not change when we change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>.</p>
</div>
<div class="example">
<p>Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><mi>A</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">f(x) = x^T A x</annotation></semantics></math> (mapping <span class="math inline">$\R^n \to \R$</span>). Then, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>d</mi><msup><mi>x</mi><mi>T</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msup><mi>x</mi><mi>T</mi></msup><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munder><munder><mrow><mi>d</mi><msup><mi>x</mi><mi>T</mi></msup><mspace width="0.167em"></mspace><mi>A</mi><mi>x</mi></mrow><mo accent="true">⏟</mo></munder><mrow><mo>=</mo><mspace width="0.167em"></mspace><msup><mi>x</mi><mi>T</mi></msup><msup><mi>A</mi><mi>T</mi></msup><mi>d</mi><mi>x</mi></mrow></munder><mo>+</mo><msup><mi>x</mi><mi>T</mi></msup><mi>A</mi><mi>d</mi><mi>x</mi><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>+</mo><msup><mi>A</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>x</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mi>d</mi><mi>x</mi><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">df = dx^T(Ax) + x^T d(Ax) = \underbrace{dx^T \, Ax}_{=\, x^T A^Tdx} + x^T Adx = x^T(A+A^T)dx  = (\nabla f)^T dx\, ,</annotation></semantics></math> and hence <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>+</mo><msup><mi>A</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x) = x^T(A + A^T)</annotation></semantics></math>. (In the common case where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is symmetric, this simplifies to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>2</mn><msup><mi>x</mi><mi>T</mi></msup><mi>A</mi></mrow><annotation encoding="application/x-tex">f'(x) = 2x^T A</annotation></semantics></math>.) Note that here we have applied Example&nbsp;<a href="#ex:Ax" data-reference-type="ref" data-reference="ex:Ax">[ex:Ax]</a> in computing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>A</mi><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">d(Ax) = A dx</annotation></semantics></math>. Furthermore, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> is a scalar valued function, so we may also obtain the gradient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>+</mo><msup><mi>A</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">\nabla f = (A+A^T) x</annotation></semantics></math> as before (which simplifies to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>A</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">2Ax</annotation></semantics></math> if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is symmetric).</p>
</div>
<div class="example">
<p>Given <span class="math inline">$x,y\in \R^m$</span>, define <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mrow><mi>.</mi><mo>*</mo></mrow><mi>y</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>1</mn></msub><msub><mi>y</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mi>m</mi></msub><msub><mi>y</mi><mi>m</mi></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munder><munder><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>1</mn></msub></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mn>2</mn></msub></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"><mo>⋱</mo></mtd><mtd columnalign="center" style="text-align: center"></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>x</mi><mi>m</mi></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo accent="true">⏟</mo></munder><mrow><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">g</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></munder><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>y</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>y</mi><mi>m</mi></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">x\mathbin{.*}y = \begin{pmatrix}
        x_1y_1 \\ \vdots \\ x_my_m
    \end{pmatrix} = \underbrace{\begin{pmatrix} x_1 &amp; &amp; &amp; \\ &amp; x_2 &amp; &amp; \\ &amp; &amp; \ddots &amp; \\ &amp; &amp; &amp; x_m \end{pmatrix}}_{\mathrm{diag}(x)} \begin{pmatrix}
        y_1 \\ \vdots \\ y_m
    \end{pmatrix},</annotation></semantics></math> the element-wise product of vectors (also called the <a href="https://en.wikipedia.org/wiki/Hadamard_product_(matrices)">Hadamard product</a>), where for convenience below we also define <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">g</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathrm{diag}(x)</annotation></semantics></math> as the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">m \times m</annotation></semantics></math> diagonal matrix with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> on the diagonal. Then, given <span class="math inline">$A \in \R^{m,n}$</span>, define <span class="math inline">$f:\R^n \to \R^m$</span> via <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mrow><mi>.</mi><mo>*</mo></mrow><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">f(x) = A(x\mathbin{.*}x).</annotation></semantics></math></p>
<p>As an exercise, one can verify the following:</p>
<ul>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mrow><mi>.</mi><mo>*</mo></mrow><mi>y</mi><mo>=</mo><mi>y</mi><mrow><mi>.</mi><mo>*</mo></mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x\mathbin{.*}y = y\mathbin{.*}x</annotation></semantics></math>,</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mrow><mi>.</mi><mo>*</mo></mrow><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>A</mi><mspace width="0.167em"></mspace><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">g</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>y</mi></mrow><annotation encoding="application/x-tex">A(x\mathbin{.*}y) = A \, \mathrm{diag}(x)\, y</annotation></semantics></math>.</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mrow><mi>.</mi><mo>*</mo></mrow><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>.</mi><mo>*</mo></mrow><mi>y</mi><mo>+</mo><mi>x</mi><mrow><mi>.</mi><mo>*</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">d(x\mathbin{.*}y) = (dx)\mathbin{.*}y + x\mathbin{.*}(dy)</annotation></semantics></math>. So if we take <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math> to be a constant and define <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>y</mi><mrow><mi>.</mi><mo>*</mo></mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">g(x) = y \mathbin{.*}x</annotation></semantics></math>, its Jacobian matrix is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">g</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathrm{diag}(y)</annotation></semantics></math>.</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mi>x</mi><mrow><mi>.</mi><mo>*</mo></mrow><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>2</mn><mi>A</mi><mspace width="0.167em"></mspace><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">g</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>d</mi><mi>x</mi><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">df = A(2x\mathbin{.*}dx) = 2A \, \mathrm{diag}(x) \, dx = f'(x)[dx]</annotation></semantics></math>, so the Jacobian matrix is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo>=</mo><mn>2</mn><mi>A</mi><mspace width="0.167em"></mspace><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">g</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">J = 2A \, \mathrm{diag}(x)</annotation></semantics></math>.</p></li>
<li><p>Notice that the directional derivative (Sec.&nbsp;<a href="#sec:directional" data-reference-type="ref" data-reference="sec:directional">2.2.1</a>) of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> in the direction <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> is simply given by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>v</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mn>2</mn><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mrow><mi>.</mi><mo>*</mo></mrow><mi>v</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)[v] = 2A (x \mathbin{.*}v)</annotation></semantics></math>. One could also check numerically for some arbitrary <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>v</mi></mrow><annotation encoding="application/x-tex">A,x,v</annotation></semantics></math> that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><msup><mn>10</mn><mrow><mi>−</mi><mn>8</mn></mrow></msup><mi>v</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>≈</mo><msup><mn>10</mn><mrow><mi>−</mi><mn>8</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mrow><mi>.</mi><mo>*</mo></mrow><mi>v</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x + 10^{-8} v) - f(x) \approx 10^{-8} ( 2A (x \mathbin{.*}v))</annotation></semantics></math>.</p></li>
</ul>
</div>
</section>
<section id="sec:chainrule" class="level2">
<h2 class="anchored" data-anchor-id="sec:chainrule">The Chain Rule</h2>
<p>One of the most important rules from differential calculus is the chain rule, because it allows us to differentiate complicated functions built out of compositions of simpler functions. This chain rule can also be generalized to our differential notation in order to work for functions on arbitrary vector spaces:</p>
<ul>
<li><p><strong>Chain Rule</strong>: Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x) = g(h(x))</annotation></semantics></math>. Then, $$</p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>d</mi><mi>f</mi><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>g</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>g</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>h</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>g</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>h</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    df = f(x + dx) - f(x) &amp;= g(h( x + dx)) - g(h(x)) \\
    &amp;= g'(h(x))[h(x + dx) - h(x)] \\
    &amp;= g'(h(x))[h'(x)[dx]] \\
    &amp;= g'(h(x)) h'(x) [dx]

\end{aligned}</annotation></semantics></math>
<p>$$ where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>h</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g'(h(x))h'(x)</annotation></semantics></math> is a composition of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">g'</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">h'</annotation></semantics></math> as matrices.</p>
<p>In other words, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>g</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>h</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x) = g'(h(x)) h'(x)</annotation></semantics></math>: the Jacobian (linear operator) <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math> is simply the <em>product (composition) of the Jacobians</em>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mi>′</mi><mi>h</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">g' h'</annotation></semantics></math>. Ordering matters because linear operators do not generally commute: left-to-right = outputs-to-inputs.</p></li>
</ul>
<p>Let’s look more carefully at the <em>shapes</em> of these Jacobian matrices in an example where each function maps a column vector to a column vector:</p>
<div class="example">
<p>Let <span class="math inline">$x\in \R^n$</span>, <span class="math inline">$h(x) \in \R^p$</span>, and <span class="math inline">$g(h(x)) \in \R^m$</span>. Then, let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x) = g(h(x))</annotation></semantics></math> mapping from <span class="math inline">$\R^n$</span> to <span class="math inline">$\R^m$</span>. The chain rule then states that <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>g</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>h</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">f'(x) = g'(h(x)) h'(x),</annotation></semantics></math> which makes sense as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">g'</annotation></semantics></math> is an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">m\times p</annotation></semantics></math> matrix and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">h'</annotation></semantics></math> is a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">p\times n</annotation></semantics></math> matrix, so that the product gives an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m \times n</annotation></semantics></math> matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math>! However, notice that this is <em>not</em> the same as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>g</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">h'(x) g'(h(x))</annotation></semantics></math> as you cannot (if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>≠</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">n\neq m</annotation></semantics></math>) multiply a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">p\times n</annotation></semantics></math> and an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">m\times p</annotation></semantics></math> matrix together, and even if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">n = m</annotation></semantics></math> you will get the wrong answer since they probably won’t commute.</p>
</div>
<p>Not only does the order of the multiplication matter, but the associativity of matrix multiplication matters <em>practically</em>. Let’s consider a function <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>a</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>b</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>c</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)= a(b(c(x)))</annotation></semantics></math> where <span class="math inline">$c: \R^n \to \R^p$</span>, <span class="math inline">$b:\R^p \to \R^q$</span>, and <span class="math inline">$a: \R^q\to \R^m$</span>. Then, we have that, by the chain rule, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>a</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>b</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>c</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>b</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>c</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>c</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">f'(x) = a'(b(c(x))) b'(c(x))c'(x).</annotation></semantics></math> Notice that this is the same as <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>a</mi><mi>′</mi><mi>b</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>c</mi><mi>′</mi><mo>=</mo><mi>a</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>b</mi><mi>′</mi><mi>c</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'  = (a' b')c' = a'(b'c')</annotation></semantics></math> by associativity (omitting the function arguments for brevity). The left-hand side is multiplication from left to right, and the right-hand side is multiplication from right to left.</p>
<p>But who cares? Well it turns out that associativity is deeply important. So important that the two orderings have names: multiplying left-to-right is called “reverse mode” and multiplying right-to-left is called “forward mode” in the field of <em>automatic differentiation</em> (AD). Reverse-mode differentation is also known as an “adjoint method” or “backpropagation” in some contexts, which we will explore in more detail later. Why does this matter? Let’s think about the computational cost of matrix multiplication.</p>
<section id="sec:cost-matrix-mult" class="level3">
<h3 class="anchored" data-anchor-id="sec:cost-matrix-mult">Cost of Matrix Multiplication</h3>
<div id="fig:matrix-mult-assoc" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/matrix-mult-assoc.pdf.svg" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Matrix multiplication is <em>associative</em>—that is, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>C</mi><mo>=</mo><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><mi>C</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">(AB)C = A(BC)</annotation></semantics></math> for all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>,</mo><mi>B</mi><mo>,</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">A,B,C</annotation></semantics></math>—but multiplying left-to-right can be much more efficient than right-to-left if the leftmost matrix has only one (or few) rows, as shown here. Correspondingly, the order in which you carry out the chain rule has dramatic consequences for the computational effort required. Left-to-right is known as “reverse mode” or “backpropagation”, and is best suited to situations where there are many fewer outputs than inputs.</figcaption>
</figure>
</div>
<p>If you multiply a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>q</mi></mrow><annotation encoding="application/x-tex">m\times q</annotation></semantics></math> matrix by a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo>×</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">q\times p</annotation></semantics></math> matrix, you normally do it by computing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">mp</annotation></semantics></math> dot products of length <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>q</mi><annotation encoding="application/x-tex">q</annotation></semantics></math> (or some equivalent re-ordering of these operations). To do a dot product of length <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>q</mi><annotation encoding="application/x-tex">q</annotation></semantics></math> requires <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>q</mi><annotation encoding="application/x-tex">q</annotation></semantics></math> multiplications and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">q-1</annotation></semantics></math> additions of scalars. Overall, this is approximately <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>m</mi><mi>p</mi><mi>q</mi></mrow><annotation encoding="application/x-tex">2mpq</annotation></semantics></math> scalar operations in total. In computer science, you would write that this is “<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>m</mi><mi>p</mi><mi>q</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Theta(mpq)</annotation></semantics></math>”: the computational effort is <em>asymptotically proportional</em> to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>p</mi><mi>q</mi></mrow><annotation encoding="application/x-tex">mpq</annotation></semantics></math> for large <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>,</mo><mi>p</mi><mo>,</mo><mi>q</mi></mrow><annotation encoding="application/x-tex">m,p,q</annotation></semantics></math>.</p>
<p>So why does the order of the chain rule matter? Consider the following two examples.</p>
<div class="example">
<p>Suppose you have a lot of inputs <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>≫</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n \gg 1</annotation></semantics></math>, and only one output <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">m=1</annotation></semantics></math>, with lots of intermediate values, i.e.&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo>=</mo><mi>p</mi><mo>=</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">q=p=n</annotation></semantics></math>. Then reverse mode (left-to-right) will cost <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Theta(n^2)</annotation></semantics></math> scalar operations while forward mode (right-to-left) would cost <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>n</mi><mn>3</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Theta(n^3)</annotation></semantics></math>! This is a huge cost difference, depicted schematically in Fig.&nbsp;<a href="#fig:matrix-mult-assoc" data-reference-type="ref" data-reference="fig:matrix-mult-assoc">3</a>.</p>
<p>Conversely, suppose you have a lot of outputs <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>≫</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">m \gg 1</annotation></semantics></math> and only one input <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n=1</annotation></semantics></math>, with lots of intermediate values <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo>=</mo><mi>p</mi><mo>=</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">q=p=m</annotation></semantics></math>. Then reverse mode would cost <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>m</mi><mn>3</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Theta(m^3)</annotation></semantics></math> operations but forward mode would be only <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>m</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Theta(m^2)</annotation></semantics></math>!</p>
<p>Moral: If you have a lot of inputs and few outputs (the usual case in machine learning and optimization), compute the chain rule left-to-right (reverse mode). If you have a lot of outputs and few inputs, compute the chain rule right-to-left (forward mode). We return to this in Sec.&nbsp;<a href="#sec:forward-vs-reverse" data-reference-type="ref" data-reference="sec:forward-vs-reverse">8.4</a>.</p>
</div>
</section>
</section>
<section id="beyond-multi-variable-derivatives" class="level2">
<h2 class="anchored" data-anchor-id="beyond-multi-variable-derivatives">Beyond Multi-Variable Derivatives</h2>
<p>Now let’s compute some derivatives that go beyond first-year calculus, where the inputs and outputs are in more general vector spaces. For instance, consider the following examples:</p>
<div class="example">
<p>Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> be an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n\times n</annotation></semantics></math> matrix. You could have the following matrix-valued functions. For example:</p>
<ul>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>A</mi><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">f(A) = A^3</annotation></semantics></math>,</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">f(A) = A^{-1}</annotation></semantics></math> if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is invertible,</p></li>
<li><p>or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>U</mi><annotation encoding="application/x-tex">U</annotation></semantics></math>, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>U</mi><annotation encoding="application/x-tex">U</annotation></semantics></math> is the resulting matrix after applying Gaussian elimination to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>!</p></li>
</ul>
<p>You could also have scalar outputs. For example:</p>
<ul>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>det</mo><mi>A</mi></mrow><annotation encoding="application/x-tex">f(A) = \det A</annotation></semantics></math>,</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo></mrow><annotation encoding="application/x-tex">f(A) =</annotation></semantics></math> trace <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>,</p></li>
<li><p>or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>σ</mi><mn>1</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(A) = \sigma_1(A)</annotation></semantics></math>, the largest singular value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>.</p></li>
</ul>
</div>
<p>Let’s focus on two simpler examples for this lecture.</p>
<div class="example">
<p>Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>A</mi><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">f(A) = A^3</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is a square matrix. Compute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">df</annotation></semantics></math>.</p>
</div>
<p>Here, we apply the chain rule one step at a time: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>d</mi><mi>A</mi><mspace width="0.167em"></mspace><msup><mi>A</mi><mn>2</mn></msup><mo>+</mo><mi>A</mi><mspace width="0.167em"></mspace><mi>d</mi><mi>A</mi><mspace width="0.167em"></mspace><mi>A</mi><mo>+</mo><msup><mi>A</mi><mn>2</mn></msup><mspace width="0.167em"></mspace><mi>d</mi><mi>A</mi><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>A</mi><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">df = dA\, A^2 + A \,dA \, A + A^2 \,dA = f'(A) [dA].</annotation></semantics></math> Notice that this is not equal to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><msup><mi>A</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">3A^2</annotation></semantics></math> (unless <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">dA</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> commute, which won’t generally be true since <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">dA</annotation></semantics></math> represents an <em>arbitrary</em> small change in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>). The right-hand side is a linear operator <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(A)</annotation></semantics></math> acting on <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">dA</annotation></semantics></math>, but it is not so easy to interpret it as simply a single “Jacobian” matrix multiplying <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">dA</annotation></semantics></math>!</p>
<div class="example">
<p>Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">f(A) = A^{-1}</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is a square invertible matrix. Compute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">df = d(A^{-1})</annotation></semantics></math>.</p>
</div>
<p>Here, we use a slight trick. Notice that <span class="math inline">$AA^{-1} = \Id$</span>, the identity matrix. Thus, we can compute the differential using the product rule (noting that <span class="math inline">$d\Id = 0$</span>, since changing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> does not change <span class="math inline">$\Id$</span>) so <span class="math display">$$d(AA^{-1}) = dA\, A^{-1} + A\,d(A^{-1}) = d(\Id) = 0\implies d(A^{-1}) = -A^{-1} \,dA \,A^{-1}.$$</span></p>
</section>
</section>
<section id="sec:kronecker" class="level1">
<h1>Jacobians of Matrix Functions</h1>
<p>When we have a function that has <em>matrices</em> as inputs and/or outputs, we have already seen in the previous lectures that we can still define the derivative as a linear operator by a <em>formula</em> for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math> mapping a small change in input to the corresponding small change in output. However, when you first learned linear algebra, probably most linear operations were represented by matrices multiplying vectors, and it may take a while to get used to thinking of linear operations more generally. In this chapter, we discuss how it is still <em>possible</em> to represent <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math> by a <strong>Jacobian matrix</strong> even for matrix inputs/outputs, and how the most common technique to do this involves <strong>matrix “vectorization”</strong> and a new type of matrix operation, a <strong>Kronecker product</strong>. This gives us another way to think about our <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math> linear operators that is occasionally convenient, but at the same time it is important to become comfortable with other ways of writing down linear operators too—sometimes, the explicit Jacobian-matrix approach can obscure key structure, and it is often computationally inefficient as well.</p>
<p>For this section of the notes, we refer to the linked <a href="https://rawcdn.githack.com/mitmath/matrixcalc/3f6758996e40c5c1070279f89f7f65e76e08003d/notes/2x2Jacobians.jl.html">Pluto Notebook</a> for computational demonstrations of this material in Julia, illustrating multiple views of the derivative of the square <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>A</mi><mn>2</mn></msup><annotation encoding="application/x-tex">A^{2}</annotation></semantics></math> of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2\times2</annotation></semantics></math> matrices <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>.</p>
<section id="derivatives-of-matrix-functions-linear-operators" class="level2">
<h2 class="anchored" data-anchor-id="derivatives-of-matrix-functions-linear-operators">Derivatives of matrix functions: Linear operators</h2>
<p>As we have already emphasized, the derivative <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math> is the linear operator that maps a small change in the input to a small change in the output. This idea can take an unfamiliar form, however, when applied to functions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(A)</annotation></semantics></math> that map matrix inputs <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> to matrix outputs. For example, we’ve already considered the following functions on square <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">m\times m</annotation></semantics></math> matrices:</p>
<ul>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>A</mi><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">f(A)=A^{3}</annotation></semantics></math>, which gives <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>A</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>d</mi><mi>A</mi><mspace width="0.167em"></mspace><msup><mi>A</mi><mn>2</mn></msup><mo>+</mo><mi>A</mi><mspace width="0.167em"></mspace><mi>d</mi><mi>A</mi><mspace width="0.167em"></mspace><mi>A</mi><mo>+</mo><msup><mi>A</mi><mn>2</mn></msup><mspace width="0.167em"></mspace><mi>d</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">df=f'(A)[dA]=dA\,A^{2}+A\,dA\,A+A^{2}\,dA</annotation></semantics></math>.</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">f(A)=A^{-1}</annotation></semantics></math>, which gives <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>A</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>−</mi><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mspace width="0.167em"></mspace><mi>d</mi><mi>A</mi><mspace width="0.167em"></mspace><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">df=f'(A)[dA]=-A^{-1}\,dA\,A^{-1}</annotation></semantics></math></p></li>
</ul>
<div class="example">
<p>An even simpler example is the <em>matrix-square</em> function:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>A</mi><mn>2</mn></msup><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">f(A)=A^{2}\,,</annotation></semantics></math> which by the product rule gives <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>A</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>d</mi><mi>A</mi><mspace width="0.167em"></mspace><mi>A</mi><mo>+</mo><mi>A</mi><mspace width="0.167em"></mspace><mi>d</mi><mi>A</mi><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">df=f'(A)[dA]=dA\,A+A\,dA\,.</annotation></semantics></math> You can also work this out explicitly from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>+</mo><mi>d</mi><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>+</mo><mi>d</mi><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>−</mo><msup><mi>A</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">df=f(A+dA)-f(A)=(A+dA)^{2}-A^{2}</annotation></semantics></math>, dropping the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><annotation encoding="application/x-tex">(dA)^{2}</annotation></semantics></math> term.</p>
</div>
<p>In all of these examples, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(A)</annotation></semantics></math> is described by a simple formula for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>A</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">f'(A)[dA]</annotation></semantics></math> that relates an arbitrary change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">dA</annotation></semantics></math> in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> to the change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>+</mo><mi>d</mi><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">df=f(A+dA)-f(A)</annotation></semantics></math> in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>, to first order. If the differential is distracting you, realize that we can plug any matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> we want into this formula, not just an “infinitesimal” change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">dA</annotation></semantics></math>, e.g.&nbsp;in our matrix-square example we have <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>X</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>X</mi><mi>A</mi><mo>+</mo><mi>A</mi><mi>X</mi></mrow><annotation encoding="application/x-tex">f'(A)[X]=XA+AX</annotation></semantics></math> for an arbitrary <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> (a directional derivative, from Sec.&nbsp;<a href="#sec:directional" data-reference-type="ref" data-reference="sec:directional">2.2.1</a>). This is <em>linear</em> in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>: if we scale or add inputs, it scales or adds outputs, respectively: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mn>2</mn><mi>X</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mn>2</mn><mi>X</mi><mi>A</mi><mo>+</mo><mi>A</mi><mspace width="0.167em"></mspace><mn>2</mn><mi>X</mi><mo>=</mo><mn>2</mn><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mi>A</mi><mo>+</mo><mi>A</mi><mi>X</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>2</mn><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>X</mi><mo stretchy="true" form="postfix">]</mo></mrow><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">f'(A)[2X]=2XA+A\,2X=2(XA+AX)=2f'(A)[X]\,,</annotation></semantics></math> <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>X</mi><mo>+</mo><mi>Y</mi><mo stretchy="true" form="postfix">]</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>+</mo><mi>Y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>A</mi><mo>+</mo><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>+</mo><mi>Y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>X</mi><mi>A</mi><mo>+</mo><mi>Y</mi><mi>A</mi><mo>+</mo><mi>A</mi><mi>X</mi><mo>+</mo><mi>A</mi><mi>Y</mi><mo>=</mo><mi>X</mi><mi>A</mi><mo>+</mo><mi>A</mi><mi>X</mi><mo>+</mo><mi>Y</mi><mi>A</mi><mo>+</mo><mi>A</mi><mi>Y</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>X</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>Y</mi><mo stretchy="true" form="postfix">]</mo></mrow><mspace width="0.167em"></mspace><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
f'(A)[X+Y] &amp; =(X+Y)A+A(X+Y)=XA+YA+AX+AY=XA+AX+YA+AY\\
 &amp; =f'(A)[X]+f'(A)[Y]\,.
\end{aligned}</annotation></semantics></math> This is a perfectly good way to define a linear operation! We are <em>not</em> expressing it here in the familiar form <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>X</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">some matrix?</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>×</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> vector?</mtext></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(A)[X]=(\text{some matrix?})\times(X\text{ vector?})</annotation></semantics></math>, and that’s okay! A formula like <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi>A</mi><mo>+</mo><mi>A</mi><mi>X</mi></mrow><annotation encoding="application/x-tex">XA+AX</annotation></semantics></math> is easy to write down, easy to understand, and easy to compute with.</p>
<p>But sometimes you still may want to think of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math> as a single “Jacobian” matrix, using the most familiar language of linear algebra, and it is possible to do that! If you took a sufficiently abstract linear-algebra course, you may have learned that <em>any</em> linear operator can be represented by a matrix once you choose a basis for the input and output vector spaces. Here, however, we will be much more concrete, because there is a conventional “Cartesian” basis for matrices <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> called “vectorization”, and in this basis linear operators like <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>X</mi><mo>+</mo><mi>X</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">AX+XA</annotation></semantics></math> are particularly easy to represent in matrix form once we introduce a new type of matrix product that has widespread applications in “multidimensional” linear algebra.</p>
</section>
<section id="a-simple-example-the-two-by-two-matrix-square-function" class="level2">
<h2 class="anchored" data-anchor-id="a-simple-example-the-two-by-two-matrix-square-function">A simple example: The two-by-two matrix-square function</h2>
<p>To begin with, let’s look in more detail at our matrix-square function <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>A</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">f(A)=A^{2}</annotation></semantics></math> for the simple case of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2\times2</annotation></semantics></math> matrices, which are described by only four scalars, so that we can look at every term in the derivative explicitly. In particular,</p>
<div class="example">
<p>For a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2\times2</annotation></semantics></math> matrix <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>p</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>r</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>q</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>s</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">A=\begin{pmatrix}p &amp; r\\
q &amp; s
\end{pmatrix},</annotation></semantics></math> the matrix-square function is <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>A</mi><mn>2</mn></msup><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>p</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>r</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>q</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>s</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>p</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>r</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>q</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>s</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msup><mi>p</mi><mn>2</mn></msup><mo>+</mo><mi>q</mi><mi>r</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>p</mi><mi>r</mi><mo>+</mo><mi>r</mi><mi>s</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>p</mi><mi>q</mi><mo>+</mo><mi>q</mi><mi>s</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>q</mi><mi>r</mi><mo>+</mo><msup><mi>s</mi><mn>2</mn></msup></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">f(A)=A^{2}=\begin{pmatrix}p &amp; r\\
q &amp; s
\end{pmatrix}\begin{pmatrix}p &amp; r\\
q &amp; s
\end{pmatrix}=\begin{pmatrix}p^{2}+qr &amp; pr+rs\\
pq+qs &amp; qr+s^{2}
\end{pmatrix}.</annotation></semantics></math></p>
</div>
<p>Written out explicitly in terms of the matrix entries <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>q</mi><mo>,</mo><mi>r</mi><mo>,</mo><mi>s</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(p,q,r,s)</annotation></semantics></math> in this way, it is natural to think of our function as mapping 4 scalar inputs to 4 scalar outputs. That is, we can think of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> as equivalent to a “vectorized” function <span class="math inline">$\tilde{f}:\R^{4}\to\R^{4}$</span> given by <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>f</mi><mo accent="true">̃</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>p</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>q</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>r</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>s</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msup><mi>p</mi><mn>2</mn></msup><mo>+</mo><mi>q</mi><mi>r</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>p</mi><mi>q</mi><mo>+</mo><mi>q</mi><mi>s</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>p</mi><mi>r</mi><mo>+</mo><mi>r</mi><mi>s</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>q</mi><mi>r</mi><mo>+</mo><msup><mi>s</mi><mn>2</mn></msup></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">\tilde{f}(\left(\begin{array}{c}
p\\
q\\
r\\
s
\end{array}\right))=\left(\begin{array}{c}
p^{2}+qr\\
pq+qs\\
pr+rs\\
qr+s^{2}
\end{array}\right)\,.</annotation></semantics></math> Converting a matrix into a column vector in this way is called <strong>vectorization</strong>, and is commonly denoted by the operation “<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>vec</mo><annotation encoding="application/x-tex">\operatorname{vec}</annotation></semantics></math>”: <span class="math display">$$\begin{aligned}
\operatorname{vec}A &amp; =\operatorname{vec}\begin{pmatrix}p &amp; r\\
q &amp; s
\end{pmatrix}=%
\begingroup
    %   \br@kwd depends on font size, so compute it now.
    \setbox 0=\hbox{$\left(\right.$}
    \setlength{\br@kwd}{\wd 0}
        %   Compute the array strut based on current value of \arraystretch.
    \setbox\@arstrutbox\hbox{\vrule
        \@height\arraystretch\ht\strutbox   
        \@depth\arraystretch\dp\strutbox
        \@width\z@}
        %   Compute height of first row and extra space.
    \setlength{\k@bordht}{\kbrowsep}
    \addtolength{\k@bordht}{\ht\@arstrutbox}
    \addtolength{\k@bordht}{\dp\@arstrutbox}
        %   turn off mathsurround
    \m@th
        %   Set the first row style
    \def\@kbrowstyle{\scriptstyle}
        %   Swallow the alignment into box0:
    \setbox 0=\vbox{%
            %   Define \cr for first row to include the \kbrowsep
            %   and to reset the row style
        \def\cr{\crcr\noalign{\kern\kbrowsep
            \global\let\cr=\endline
            \global\let\@kbrowstyle=\relax}}
            %   Redefine \\ a la LaTeX:
        \let\\\@arraycr
            %   The following are needed to make a solid \vrule with no gaps
            %   between the lines.
        \lineskip\z@skip    
        \baselineskip\z@skip    
            %   Compute the length of the skip after the first column
        \dimen 0\kbcolsep \advance\dimen 0\br@kwd
            %   Here begins the alignment:
        \ialign{\tabskip\dimen 0        %   This space will show up after the first column
            \kern\arraycolsep\hfil\@arstrut$\scriptstyle##$\hfil\kern\arraycolsep&amp;
            \tabskip\z@skip         %   Cancel extra space for other columns
            \kern\arraycolsep\hfil$\@kbrowstyle ##$\hfil\kern\arraycolsep&amp;&amp;
            \kern\arraycolsep\hfil$\@kbrowstyle ##$\hfil\kern\arraycolsep\crcr
                %   That ends the template.
                %   Here is the argument:
             &amp; \\
A_{1,1} &amp; p\\
A_{2,1} &amp; q\\
A_{1,2} &amp; r\\
A_{2,2} &amp; s
\crcr}%     End \ialign
    }%  End \setbox0.
        %   \box0 now holds the array.
        %
        %   This next line uses \box2 to hold a throwaway
        %   copy of \box0, leaving \box0 intact,
        %   while putting the last row in \box5.
    \setbox 2=\vbox{\unvcopy 0 \global\setbox 5=\lastbox}
        %   We want the width of the first column,
        %   so we lop off columns until there is only one left.
        %   It's not elegant or efficient, but at 1 gHz, who cares.
    \loop
        \setbox 2=\hbox{\unhbox 5 \unskip \global\setbox 3=\lastbox}
        \ifhbox 3
            \global\setbox 5=\box 2
            \global\setbox 1=\box 3
    \repeat
        %   \box1 now holds the first column of last row.
        %
        %   This next line stores the alignment in \box2,
        %   while calculating the proper
        %   delimiter height and placement.
    \setbox 2=\hbox{$\kern\wd 1\kern\kbcolsep\kern-\arraycolsep
        \left(
        \kern-\wd 1\kern-\kbcolsep\kern-\br@kwd
            %
            %   Here is the output.  The \vcenter aligns the array with the "math axis."
            %   The negative vertical \kern only shrinks the delimiter's height.
            %   BTW, I didn't find this in the TeXbook,
            %   I had to try various \kerns to see what they did in a
            %   \left[\vcenter{}\right].
    \vcenter{\kern-\k@bordht\vbox{\unvbox 0}}
    \right)$}
    \null\vbox{\kern\k@bordht\box 2}
        %
    \endgroup
\, ,\\
\operatorname{vec}f(A) &amp; =\operatorname{vec}\begin{pmatrix}p^{2}+qr &amp; pr+rs\\
pq+qs &amp; qr+s^{2}
\end{pmatrix}=\left(\begin{array}{c}
p^{2}+qr\\
pq+qs\\
pr+rs\\
qr+s^{2}
\end{array}\right) \, .
\end{aligned}$$</span> In terms of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>vec</mo><annotation encoding="application/x-tex">\operatorname{vec}</annotation></semantics></math>, our “vectorized” matrix-squaring function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>f</mi><mo accent="true">̃</mo></mover><annotation encoding="application/x-tex">\tilde{f}</annotation></semantics></math> is defined by <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>f</mi><mo accent="true">̃</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mo>vec</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>vec</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>vec</mo><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>A</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">\tilde{f}(\operatorname{vec}A)=\operatorname{vec}f(A)=\operatorname{vec}(A^{2})\,.</annotation></semantics></math> More generally,</p>
<div class="definition">
<p>The <strong>vectorization</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>vec</mo><mi>A</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\operatorname{vec}A\in\mathbb{R}^{mn}</annotation></semantics></math> of any <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m\times n</annotation></semantics></math> matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">A\in\mathbb{R}^{m\times n}</annotation></semantics></math> is a defined by simply <strong>stacking the columns</strong> of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>, from left to right, into a column vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>vec</mo><mi>A</mi></mrow><annotation encoding="application/x-tex">\operatorname{vec}A</annotation></semantics></math>. That is, if we denote the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> columns of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>-component vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>a</mi><mo accent="true">→</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover><mi>a</mi><mo accent="true">→</mo></mover><mn>2</mn></msub><mo>,</mo><mi>…</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>m</mi></msup></mrow><annotation encoding="application/x-tex">\vec{a}_{1},\vec{a}_{2},\ldots\in\mathbb{R}^{m}</annotation></semantics></math>, then <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>vec</mo><mi>A</mi><mo>=</mo><mo>vec</mo><munder><munder><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mover><mi>a</mi><mo accent="true">→</mo></mover><mn>1</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mover><mi>a</mi><mo accent="true">→</mo></mover><mn>2</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><mi>⋯</mi></mtd><mtd columnalign="center" style="text-align: center"><msub><mover><mi>a</mi><mo accent="true">→</mo></mover><mi>n</mi></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo accent="true">⏟</mo></munder><mrow><mi>A</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow></munder><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mover><mi>a</mi><mo accent="true">→</mo></mover><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mover><mi>a</mi><mo accent="true">→</mo></mover><mn>2</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mover><mi>a</mi><mo accent="true">→</mo></mover><mi>n</mi></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>m</mi><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\operatorname{vec}A=\operatorname{vec}\underbrace{\left(\begin{array}{cccc}
\vec{a}_{1} &amp; \vec{a}_{2} &amp; \cdots &amp; \vec{a}_{n}\end{array}\right)}_{A\in\mathbb{R}^{m\times n}}=\left(\begin{array}{c}
\vec{a}_{1}\\
\vec{a}_{2}\\
\vdots\\
\vec{a}_{n}
\end{array}\right)\in\mathbb{R}^{mn}</annotation></semantics></math> is an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">mn</annotation></semantics></math>-component column vector containing all of the entries of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>.</p>
<p>On a computer, matrix entries are typically stored in a consecutive sequence of memory locations, which can be viewed a form of vectorization. In fact, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>vec</mo><mi>A</mi></mrow><annotation encoding="application/x-tex">\operatorname{vec}A</annotation></semantics></math> corresponds exactly to what is known as “column-major” storage, in which the column entries are stored consecutively; this is the default format in Fortran, Matlab, and Julia, for example, and the venerable Fortran heritage means that column major is widely used in linear-algebra libraries.</p>
</div>
<p>.</p>
<div class="problem">
<p>The vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>vec</mo><mi>A</mi></mrow><annotation encoding="application/x-tex">\operatorname{vec}A</annotation></semantics></math> corresponds to the coefficients you get when you express the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m\times n</annotation></semantics></math> matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> in a <em>basis</em> of matrices. What is that basis?</p>
</div>
<p>Vectorization turns unfamilar things (like matrix functions and derivatives thereof) into familiar things (like vector functions and Jacobians or gradients thereof). In that way, it can be a very attractive tool, almost <em>too</em> attractive—why do “matrix calculus” if you can turn everything back into ordinary multivariable calculus? Vectorization has its drawbacks, however: conceptually, it can obscure the underlying mathematical structure (e.g.&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>f</mi><mo accent="true">̃</mo></mover><annotation encoding="application/x-tex">\tilde{f}</annotation></semantics></math> above doesn’t look much like a matrix square <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>A</mi><mn>2</mn></msup><annotation encoding="application/x-tex">A^{2}</annotation></semantics></math>), and computationally this loss of structure can sometimes lead to severe inefficiencies (e.g.&nbsp;forming huge <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>m</mi><mn>2</mn></msup><mo>×</mo><msup><mi>m</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">m^2\times m^2</annotation></semantics></math> Jacobian matrices as discussed below). Overall, we believe that the <em>primary</em> way to study matrix functions like this should be to view them as having matrix inputs (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>) and matrix outputs (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>A</mi><mn>2</mn></msup><annotation encoding="application/x-tex">A^{2}</annotation></semantics></math>), and that one should likewise generally view the derivatives as linear operators on matrices, not vectorized versions thereof. However, it is still useful to be familiar with the vectorization viewpoint in order to have the benefit of an alternative perspective.</p>
<section id="the-matrix-squaring-four-by-four-jacobian-matrix" class="level3">
<h3 class="anchored" data-anchor-id="the-matrix-squaring-four-by-four-jacobian-matrix">The matrix-squaring four-by-four Jacobian matrix</h3>
<p>To understand Jacobians of functions (from matrices to matrices), let’s begin by considering a basic question:</p>
<div class="question">
<p>What is the <em>size</em> of the Jacobian of the matrix-square function?</p>
</div>
<p>Well, if we view the matrix squaring function via its vectorized equivalent <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>f</mi><mo accent="true">̃</mo></mover><annotation encoding="application/x-tex">\tilde{f}</annotation></semantics></math>, mapping <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ℝ</mi><mn>4</mn></msup><mo>↦</mo><msup><mi>ℝ</mi><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">\mathbb{R}^{4}\mapsto\mathbb{R}^{4}</annotation></semantics></math> (4-component column vectors to 4-component column vectors), the Jacobian would be a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>×</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">4\times4</annotation></semantics></math> matrix (formed from the derivatives of each output component with respect to each input component). Now let’s think about a more general square matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>: an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">m\times m</annotation></semantics></math> matrix. If we wanted to find the Jacobian of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>A</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">f(A)=A^{2}</annotation></semantics></math>, we could do so by the same process and (symbolically) obtain an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>m</mi><mn>2</mn></msup><mo>×</mo><msup><mi>m</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">m^{2}\times m^{2}</annotation></semantics></math> matrix (since there are <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>m</mi><mn>2</mn></msup><annotation encoding="application/x-tex">m^{2}</annotation></semantics></math> inputs, the entries of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>m</mi><mn>2</mn></msup><annotation encoding="application/x-tex">m^{2}</annotation></semantics></math> outputs, the entries of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>A</mi><mn>2</mn></msup><annotation encoding="application/x-tex">A^{2}</annotation></semantics></math>). Explicit computation of these <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>m</mi><mn>4</mn></msup><annotation encoding="application/x-tex">m^{4}</annotation></semantics></math> partial derivatives is rather tedious even for small <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>, but is a task that symbolic computational tools in e.g.&nbsp;Julia or Mathematica can handle. In fact, as seen in the <a href="https://rawcdn.githack.com/mitmath/matrixcalc/3f6758996e40c5c1070279f89f7f65e76e08003d/notes/2x2Jacobians.jl.html">Notebook</a>, Julia spits out the Jacobian quite easily. For the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">m=2</annotation></semantics></math> case that we wrote out explicitly above, you can either take the derivative of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>f</mi><mo accent="true">̃</mo></mover><annotation encoding="application/x-tex">\tilde{f}</annotation></semantics></math> by hand or use Julia’s symbolic tools to obtain the Jacobian: <span class="math display">$$\tilde{f}'=%
\begingroup
    %   \br@kwd depends on font size, so compute it now.
    \setbox 0=\hbox{$\left(\right.$}
    \setlength{\br@kwd}{\wd 0}
        %   Compute the array strut based on current value of \arraystretch.
    \setbox\@arstrutbox\hbox{\vrule
        \@height\arraystretch\ht\strutbox   
        \@depth\arraystretch\dp\strutbox
        \@width\z@}
        %   Compute height of first row and extra space.
    \setlength{\k@bordht}{\kbrowsep}
    \addtolength{\k@bordht}{\ht\@arstrutbox}
    \addtolength{\k@bordht}{\dp\@arstrutbox}
        %   turn off mathsurround
    \m@th
        %   Set the first row style
    \def\@kbrowstyle{\scriptstyle}
        %   Swallow the alignment into box0:
    \setbox 0=\vbox{%
            %   Define \cr for first row to include the \kbrowsep
            %   and to reset the row style
        \def\cr{\crcr\noalign{\kern\kbrowsep
            \global\let\cr=\endline
            \global\let\@kbrowstyle=\relax}}
            %   Redefine \\ a la LaTeX:
        \let\\\@arraycr
            %   The following are needed to make a solid \vrule with no gaps
            %   between the lines.
        \lineskip\z@skip    
        \baselineskip\z@skip    
            %   Compute the length of the skip after the first column
        \dimen 0\kbcolsep \advance\dimen 0\br@kwd
            %   Here begins the alignment:
        \ialign{\tabskip\dimen 0        %   This space will show up after the first column
            \kern\arraycolsep\hfil\@arstrut$\scriptstyle##$\hfil\kern\arraycolsep&amp;
            \tabskip\z@skip         %   Cancel extra space for other columns
            \kern\arraycolsep\hfil$\@kbrowstyle ##$\hfil\kern\arraycolsep&amp;&amp;
            \kern\arraycolsep\hfil$\@kbrowstyle ##$\hfil\kern\arraycolsep\crcr
                %   That ends the template.
                %   Here is the argument:
            &amp; (1,1) &amp; (2,1) &amp; (1,2) &amp; (2,2) \\
(1,1) &amp; 2p &amp; r &amp; q &amp; 0\\
(2,1) &amp; q &amp; p+s &amp; 0 &amp; q\\
(1,2) &amp; r &amp; 0 &amp; p+s &amp; r\\
(2,2) &amp; 0 &amp; r &amp; q &amp; 2s
\crcr}%     End \ialign
    }%  End \setbox0.
        %   \box0 now holds the array.
        %
        %   This next line uses \box2 to hold a throwaway
        %   copy of \box0, leaving \box0 intact,
        %   while putting the last row in \box5.
    \setbox 2=\vbox{\unvcopy 0 \global\setbox 5=\lastbox}
        %   We want the width of the first column,
        %   so we lop off columns until there is only one left.
        %   It's not elegant or efficient, but at 1 gHz, who cares.
    \loop
        \setbox 2=\hbox{\unhbox 5 \unskip \global\setbox 3=\lastbox}
        \ifhbox 3
            \global\setbox 5=\box 2
            \global\setbox 1=\box 3
    \repeat
        %   \box1 now holds the first column of last row.
        %
        %   This next line stores the alignment in \box2,
        %   while calculating the proper
        %   delimiter height and placement.
    \setbox 2=\hbox{$\kern\wd 1\kern\kbcolsep\kern-\arraycolsep
        \left(
        \kern-\wd 1\kern-\kbcolsep\kern-\br@kwd
            %
            %   Here is the output.  The \vcenter aligns the array with the "math axis."
            %   The negative vertical \kern only shrinks the delimiter's height.
            %   BTW, I didn't find this in the TeXbook,
            %   I had to try various \kerns to see what they did in a
            %   \left[\vcenter{}\right].
    \vcenter{\kern-\k@bordht\vbox{\unvbox 0}}
    \right)$}
    \null\vbox{\kern\k@bordht\box 2}
        %
    \endgroup
\,.$$</span> For example, the first row of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>f</mi><mo accent="true">̃</mo></mover><mi>′</mi></mrow><annotation encoding="application/x-tex">\tilde{f}'</annotation></semantics></math> consists of the partial derivatives of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>p</mi><mn>2</mn></msup><mo>+</mo><mi>q</mi><mi>r</mi></mrow><annotation encoding="application/x-tex">p^{2}+qr</annotation></semantics></math> (the first output) with respect to the 4 inputs <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>,</mo><mi>q</mi><mo>,</mo><mi>r</mi><mo>,</mo><mrow><mtext mathvariant="normal">and </mtext><mspace width="0.333em"></mspace></mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">p,q,r,\mbox{and }s</annotation></semantics></math>. Here, we have labeled the rows by the (row,column) indices <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>j</mi><mrow><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mrow></msub><mo>,</mo><msub><mi>k</mi><mrow><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(j_\mathrm{out}, k_\mathrm{out})</annotation></semantics></math> of the entries in the “output” matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>A</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">d(A^2)</annotation></semantics></math>, and have labeled the columns by the indices <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>j</mi><mrow><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi></mrow></msub><mo>,</mo><msub><mi>k</mi><mrow><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(j_\mathrm{in}, k_\mathrm{in})</annotation></semantics></math> of the entries in the “input” matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>. Although we have written the Jacobian <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>f</mi><mo accent="true">̃</mo></mover><mi>′</mi></mrow><annotation encoding="application/x-tex">\tilde{f}'</annotation></semantics></math> as a “2d” matrix, you can therefore also imagine it to be a “4d” matrix indexed by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>j</mi><mrow><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mrow></msub><mo>,</mo><msub><mi>k</mi><mrow><mi mathvariant="normal">o</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mrow></msub><mo>,</mo><msub><mi>j</mi><mrow><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi></mrow></msub><mo>,</mo><msub><mi>k</mi><mrow><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">j_\mathrm{out}, k_\mathrm{out}, j_\mathrm{in}, k_\mathrm{in}</annotation></semantics></math>.</p>
<p>However, the matrix-calculus approach of viewing the derivative <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(A)</annotation></semantics></math> as a <em>linear transformation on matrices</em> (as we derived above), <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>X</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>X</mi><mi>A</mi><mo>+</mo><mi>A</mi><mi>X</mi><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">f'(A)[X]=XA+AX\,,</annotation></semantics></math> seems to be much more revealing than writing out an explicit component-by-component “vectorized” Jacobian <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>f</mi><mo accent="true">̃</mo></mover><mi>′</mi></mrow><annotation encoding="application/x-tex">\tilde{f}'</annotation></semantics></math>, and gives a formula for any <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">m\times m</annotation></semantics></math> matrix without laboriously requiring us to take <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>m</mi><mn>4</mn></msup><annotation encoding="application/x-tex">m^{4}</annotation></semantics></math> partial derivatives one-by-one. If we really want to pursue the vectorization perspective, we need a way to recapture some of the structure that is obscured by tedious componentwise differentiation. A key tool to bridge the gap between the two perspectives is a type of matrix operation that you may not be familiar with: <strong>Kronecker products</strong> (denoted <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>⊗</mi><annotation encoding="application/x-tex">\otimes</annotation></semantics></math>).</p>
</section>
</section>
<section id="kronecker-products" class="level2">
<h2 class="anchored" data-anchor-id="kronecker-products">Kronecker Products</h2>
<p>A linear operation like <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>X</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>X</mi><mi>A</mi><mo>+</mo><mi>A</mi><mi>X</mi></mrow><annotation encoding="application/x-tex">f'(A)[X]=XA+AX</annotation></semantics></math> can be thought of as a “higher-dimensional matrix:” ordinary “2d” matrices map “1d” column vectors to 1d column vectors, whereas to map 2d matrices to 2d matrices you might imagine a “4d” matrix (sometimes called a <em>tensor</em>). To transform 2d matrices back into 1d vectors, we already saw the concept of vectorization (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>vec</mo><mi>A</mi></mrow><annotation encoding="application/x-tex">\operatorname{vec}A</annotation></semantics></math>). A closely related tool, which transforms “higher dimensional” linear operations on matrices back into “2d” matrices for the vectorized inputs/outputs, is the Kronecker product <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>⊗</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A\otimes B</annotation></semantics></math>. Although they don’t often appear in introductory linear-algebra courses, Kronecker products show up in a wide variety of mathematical applications where multidimensional data arises, such as multivariate statistics and data science or multidimensional scientific/engineering problems.</p>
<div class="definition">
<p>If <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m\times n</annotation></semantics></math> matrix with entries <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>a</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">a_{ij}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> is a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>×</mo><mi>q</mi></mrow><annotation encoding="application/x-tex">p\times q</annotation></semantics></math> matrix, then their <strong>Kronecker product</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>⊗</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A\otimes B</annotation></semantics></math> is defined by <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>a</mi><mn>11</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><mi>⋯</mi></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>a</mi><mrow><mn>1</mn><mi>n</mi></mrow></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd><mtd columnalign="center" style="text-align: center"><mo>⋱</mo></mtd><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>a</mi><mrow><mi>m</mi><mn>1</mn></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><mi>⋯</mi></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>a</mi><mrow><mi>m</mi><mi>n</mi></mrow></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>⇒</mo><munder><munder><mi>A</mi><mo accent="true">⏟</mo></munder><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></munder><mo>⊗</mo><munder><munder><mi>B</mi><mo accent="true">⏟</mo></munder><mrow><mi>p</mi><mo>×</mo><mi>q</mi></mrow></munder><mo>=</mo><munder><munder><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>a</mi><mn>11</mn></msub><mi>B</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>⋯</mi></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>a</mi><mrow><mn>1</mn><mi>n</mi></mrow></msub><mi>B</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd><mtd columnalign="center" style="text-align: center"><mo>⋱</mo></mtd><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>a</mi><mrow><mi>m</mi><mn>1</mn></mrow></msub><mi>B</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>⋯</mi></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>a</mi><mrow><mi>m</mi><mi>n</mi></mrow></msub><mi>B</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo accent="true">⏟</mo></munder><mrow><mi>m</mi><mi>p</mi><mo>×</mo><mi>n</mi><mi>q</mi></mrow></munder><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">A=\left(\begin{array}{ccc}
a_{11} &amp; \cdots &amp; a_{1n}\\
\vdots &amp; \ddots &amp; \vdots\\
a_{m1} &amp; \cdots &amp; a_{mn}
\end{array}\right)\Longrightarrow\underbrace{A}_{m\times n}\otimes\underbrace{B}_{p\times q}=\underbrace{\left(\begin{array}{ccc}
a_{11}B &amp; \cdots &amp; a_{1n}B\\
\vdots &amp; \ddots &amp; \vdots\\
a_{m1}B &amp; \cdots &amp; a_{mn}B
\end{array}\right)}_{mp\times nq}\,,</annotation></semantics></math> so that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>⊗</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A\otimes B</annotation></semantics></math> is an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>p</mi><mo>×</mo><mi>n</mi><mi>q</mi></mrow><annotation encoding="application/x-tex">mp\times nq</annotation></semantics></math> matrix formed by “pasting in” a copy of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> multiplying every element of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>.</p>
</div>
<p>For example, consider <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2\times2</annotation></semantics></math> matrices <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>p</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>r</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>q</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>s</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mtext mathvariant="normal">&nbsp;&nbsp;and&nbsp;&nbsp;</mtext><mi>B</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>a</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>c</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>b</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>d</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">A=\begin{pmatrix}p &amp; r\\
q &amp; s
\end{pmatrix}\text{~~and~~}B=\begin{pmatrix}a &amp; c\\
b &amp; d
\end{pmatrix} \, .</annotation></semantics></math> Then <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>⊗</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A\otimes B</annotation></semantics></math> is a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>×</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">4\times4</annotation></semantics></math> matrix containing all possible products of entries <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> with entries of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>. Note that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>⊗</mo><mi>B</mi><mo>≠</mo><mi>B</mi><mo>⊗</mo><mi>A</mi></mrow><annotation encoding="application/x-tex">A\otimes B\ne B\otimes A</annotation></semantics></math> (but the two are related by a re-ordering of the entries): <span class="math display">$$A\otimes B=\begin{pmatrix}p\textcolor{red}{B} &amp; rB\\
qB &amp; sB
\end{pmatrix}=\begin{pmatrix}p\textcolor{red}{a} &amp; p\textcolor{red}{c} &amp; ra &amp; rc\\
p\textcolor{red}{b} &amp; p\textcolor{red}{d} &amp; rb &amp; rd\\
qa &amp; qc &amp; sa &amp; sc\\
qb &amp; qd &amp; sb &amp; sd
\end{pmatrix}\qquad\ne\qquad B\otimes A=\begin{pmatrix}aA &amp; cA\\
bA &amp; dA
\end{pmatrix}=\begin{pmatrix}\textcolor{red}{a}p &amp; ar &amp; \textcolor{red}{c}p &amp; cr\\
aq &amp; as &amp; cq &amp; cs\\
\textcolor{red}{b}p &amp; br &amp; \textcolor{red}{d}p &amp; dr\\
bq &amp; bs &amp; dq &amp; ds
\end{pmatrix} \, ,$$</span> where we’ve colored one copy of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> red for illustration. See the <a href="https://rawcdn.githack.com/mitmath/matrixcalc/3f6758996e40c5c1070279f89f7f65e76e08003d/notes/2x2Jacobians.jl.html">Notebook</a> for more examples of Kronecker products of matrices (including some with pictures rather than numbers!).</p>
<p>Above, we saw that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>A</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">f(A)=A^{2}</annotation></semantics></math> at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>p</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>r</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>q</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>s</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">A=\begin{pmatrix}p &amp; r\\
q &amp; s
\end{pmatrix}</annotation></semantics></math> could be thought of as an equivalent function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>f</mi><mo accent="true">̃</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mo>vec</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\tilde{f}(\operatorname{vec}A)</annotation></semantics></math> mapping column vectors of 4 inputs to 4 outputs (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ℝ</mi><mn>4</mn></msup><mo>↦</mo><msup><mi>ℝ</mi><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">\mathbb{R}^{4}\mapsto\mathbb{R}^{4}</annotation></semantics></math>), with a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>×</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">4\times4</annotation></semantics></math> Jacobian that we (or the computer) laboriously computed as 16 element-by-element partial derivatives. It turns out that this result can be obtained <em>much</em> more elegantly once we have a better understanding of Kronecker products. We will find that the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>×</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">4\times4</annotation></semantics></math> “vectorized” Jacobian is simply <span class="math display">$$\tilde{f}'=\Id_{2}\otimes A+A^{T}\otimes\Id_{2}\,,$$</span> where <span class="math inline">$\Id_{2}$</span> is the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2\times2</annotation></semantics></math> identity matrix. That is, the matrix linear operator <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>A</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>d</mi><mi>A</mi><mspace width="0.167em"></mspace><mi>A</mi><mo>+</mo><mi>A</mi><mspace width="0.167em"></mspace><mi>d</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">f'(A)[dA]=dA\,A+A\,dA</annotation></semantics></math> is equivalent, after vectorization, to:</p>
<p><span class="math display">$$\operatorname{vec}\underbrace{f'(A)[dA]}_{dA\,A+A\,dA}=\underbrace{(\Id_{2}\otimes A+A^{T}\otimes\Id_{2})}_{\tilde{f}'}\operatorname{vec}dA=\underbrace{\begin{pmatrix}2p &amp; r &amp; q &amp; 0\\
q &amp; p+s &amp; 0 &amp; q\\
r &amp; 0 &amp; p+s &amp; r\\
0 &amp; r &amp; q &amp; 2s
\end{pmatrix}}_{\tilde{f}'}\underbrace{\begin{pmatrix}dp\\
dq\\
dr\\
ds
\end{pmatrix}}_{\operatorname{vec}dA}.$$</span> In order to understand <em>why</em> this is the case, however, we must first build up some understanding of the algebra of Kronecker products. To start with, a good exercise is to convince yourself of a few simpler properties of Kronecker products:</p>
<div class="problem">
<p>From the definition of the Kronecker product, derive the following identities:</p>
<ol type="1">
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>⊗</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mo>=</mo><msup><mi>A</mi><mi>T</mi></msup><mo>⊗</mo><msup><mi>B</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">(A\otimes B)^{T}=A^{T}\otimes B^{T}</annotation></semantics></math>.</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>⊗</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>C</mi><mo>⊗</mo><mi>D</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mi>C</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>⊗</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><mi>D</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">(A\otimes B)(C\otimes D)=(AC)\otimes(BD)</annotation></semantics></math>.</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>⊗</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo>=</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo>⊗</mo><msup><mi>B</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">(A\otimes B)^{-1}=A^{-1}\otimes B^{-1}</annotation></semantics></math>. (Follows from property&nbsp;2.)</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>⊗</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A\otimes B</annotation></semantics></math> is orthogonal (its transpose is its inverse) if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> are orthogonal. (From properties 1&nbsp;&amp;&nbsp;3.)</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>det</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>⊗</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>det</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>m</mi></msup><mo>det</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">\det(A\otimes B)=\det(A)^{m}\det(B)^{n}</annotation></semantics></math>, where <span class="math inline">$A\in\R^{n,n}$</span> and <span class="math inline">$B\in\R^{m,m}$</span>.</p></li>
<li><p><span class="math inline">$\tr(A\otimes B)=(\tr A)(\tr B)$</span>.</p></li>
<li><p>Given eigenvectors/values <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>u</mi><mo>=</mo><mi>λ</mi><mi>u</mi></mrow><annotation encoding="application/x-tex">Au=\lambda u</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mi>v</mi><mo>=</mo><mi>μ</mi><mi>v</mi></mrow><annotation encoding="application/x-tex">Bv=\mu v</annotation></semantics></math> of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>, then <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mi>μ</mi></mrow><annotation encoding="application/x-tex">\lambda\mu</annotation></semantics></math> is an eigenvalue of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>⊗</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A\otimes B</annotation></semantics></math> with eigenvector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>⊗</mo><mi>v</mi></mrow><annotation encoding="application/x-tex">u\otimes v</annotation></semantics></math>. (Also, since <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>⊗</mo><mi>v</mi><mo>=</mo><mo>vec</mo><mi>X</mi></mrow><annotation encoding="application/x-tex">u\otimes v=\operatorname{vec}X</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mi>v</mi><msup><mi>u</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">X=vu^{T}</annotation></semantics></math>, you can relate this via Prop.&nbsp;<a href="#prop5000" data-reference-type="ref" data-reference="prop5000">[prop5000]</a> below to the identity <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mi>X</mi><msup><mi>A</mi><mi>T</mi></msup><mo>=</mo><mi>B</mi><mi>v</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mo>=</mo><mi>λ</mi><mi>μ</mi><mi>X</mi></mrow><annotation encoding="application/x-tex">BXA^{T}=Bv(Au)^T=\lambda\mu X</annotation></semantics></math>.)</p></li>
</ol>
</div>
<section id="key-kronecker-product-identity" class="level3">
<h3 class="anchored" data-anchor-id="key-kronecker-product-identity">Key Kronecker-product identity</h3>
<p>In order to convert linear operations like <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>X</mi><mo>+</mo><mi>X</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">AX+XA</annotation></semantics></math> into Kronecker products via vectorization, the key identity is:</p>
<div class="proposition">
<p><span id="prop5000" data-label="prop5000"></span> Given (compatibly sized) matrices <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>,</mo><mi>B</mi><mo>,</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">A,B,C</annotation></semantics></math>, we have <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>⊗</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>vec</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>C</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>vec</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><mi>C</mi><msup><mi>A</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">(A\otimes B)\operatorname{vec}(C)=\operatorname{vec}(BCA^{T}).</annotation></semantics></math> We can thus view <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>⊗</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A\otimes B</annotation></semantics></math> as a vectorized equivalent of the linear operation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>↦</mo><mi>B</mi><mi>C</mi><msup><mi>A</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">C\mapsto BCA^{T}</annotation></semantics></math>. We are tempted to introduce a parallel notation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>⊗</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>C</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>B</mi><mi>C</mi><msup><mi>A</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">(A\otimes B)[C]=BCA^{T}</annotation></semantics></math> for the “non-vectorized” version of this operation, although this notation is not standard.</p>
<p>One possible mnemonic for this identity is that the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> is just to the left of the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math> while the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> “circles around” to the right and gets transposed.</p>
</div>
<p>Where does this identity come from? We can break it into simpler pieces by first considering the cases where either <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> is an identity matrix <span class="math inline">$\Id$</span> (of the appropriate size). To start with, suppose that <span class="math inline">$A=\Id$</span>, so that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mi>C</mi><msup><mi>A</mi><mi>T</mi></msup><mo>=</mo><mi>B</mi><mi>C</mi></mrow><annotation encoding="application/x-tex">BCA^{T}=BC</annotation></semantics></math>. What is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>vec</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><mi>C</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{vec}(BC)</annotation></semantics></math>? If we let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>c</mi><mo accent="true">→</mo></mover><mn>1</mn></msub><mo>,</mo><msub><mover><mi>c</mi><mo accent="true">→</mo></mover><mn>2</mn></msub><mo>,</mo><mi>…</mi></mrow><annotation encoding="application/x-tex">\vec{c}_{1},\vec{c}_{2},\ldots</annotation></semantics></math> denote the columns of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math>, then recall that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mi>C</mi></mrow><annotation encoding="application/x-tex">BC</annotation></semantics></math> simply multiples <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> on the left with each of the columns of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mi>C</mi><mo>=</mo><mi>B</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mover><mi>c</mi><mo accent="true">→</mo></mover><mn>1</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mover><mi>c</mi><mo accent="true">→</mo></mover><mn>2</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><mi>⋯</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>B</mi><msub><mover><mi>c</mi><mo accent="true">→</mo></mover><mn>1</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><mi>B</mi><msub><mover><mi>c</mi><mo accent="true">→</mo></mover><mn>2</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><mi>⋯</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>⇒</mo><mo>vec</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><mi>C</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>B</mi><msub><mover><mi>c</mi><mo accent="true">→</mo></mover><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>B</mi><msub><mover><mi>c</mi><mo accent="true">→</mo></mover><mn>2</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">BC=B\left(\begin{array}{ccc}
\vec{c}_{1} &amp; \vec{c}_{2} &amp; \cdots\end{array}\right)=\left(\begin{array}{ccc}
B\vec{c}_{1} &amp; B\vec{c}_{2} &amp; \cdots\end{array}\right)\Longrightarrow\operatorname{vec}(BC)=\left(\begin{array}{c}
B\vec{c}_{1}\\
B\vec{c}_{2}\\
\vdots
\end{array}\right).</annotation></semantics></math> Now, how can we get this <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>vec</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><mi>C</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{vec}(BC)</annotation></semantics></math> vector as something multiplying <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>vec</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">\operatorname{vec}C</annotation></semantics></math>? It should be immediately apparent that <span class="math display">$$\operatorname{vec}(BC)=\left(\begin{array}{c}
B\vec{c}_{1}\\
B\vec{c}_{2}\\
\vdots
\end{array}\right)=\underbrace{\left(\begin{array}{ccc}
B\\
&amp; B\\
&amp;  &amp; \ddots
\end{array}\right)}_{\Id\otimes B}\underbrace{\left(\begin{array}{c}
\vec{c}_{1}\\
\vec{c}_{2}\\
\vdots
\end{array}\right)}_{\operatorname{vec}C},$$</span> but this matrix is exactly the Kronecker product <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo>⊗</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">I\otimes B</annotation></semantics></math>! Hence, we have derived that <span class="math display">$$(\Id\otimes B)\operatorname{vec}C=\operatorname{vec}(BC).$$</span> What about the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>A</mi><mi>T</mi></msup><annotation encoding="application/x-tex">A^{T}</annotation></semantics></math> term? This is a little trickier, but again let’s simplify to the case where <span class="math inline">$B=\Id$</span>, in which case <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mi>C</mi><msup><mi>A</mi><mi>T</mi></msup><mo>=</mo><mi>C</mi><msup><mi>A</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">BCA^{T}=CA^{T}</annotation></semantics></math>. To vectorize this, we need to look at the columns of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><msup><mi>A</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">CA^{T}</annotation></semantics></math>. What is the first column of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><msup><mi>A</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">CA^{T}</annotation></semantics></math>? It is a linear combination of the columns of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math> whose coefficients are given by the first column of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>A</mi><mi>T</mi></msup><annotation encoding="application/x-tex">A^{T}</annotation></semantics></math> (=&nbsp;first row of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>): <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mtext mathvariant="normal">column 1 of </mtext><mspace width="0.333em"></mspace></mrow><mi>C</mi><msup><mi>A</mi><mi>T</mi></msup><mo>=</mo><munder><mo>∑</mo><mi>j</mi></munder><msub><mi>a</mi><mrow><mn>1</mn><mi>j</mi></mrow></msub><msub><mover><mi>c</mi><mo accent="true">→</mo></mover><mi>j</mi></msub><mspace width="0.222em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">\text{column 1 of }CA^{T}=\sum_{j}a_{1j}\vec{c}_{j}\:.</annotation></semantics></math> Similarly for column&nbsp;2, etc, and we then “stack” these columns to get <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>vec</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>C</mi><msup><mi>A</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{vec}(CA^{T})</annotation></semantics></math>. But this is exactly the formula for multipling a matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> by a vector, if the “elements” of the vector were the columns <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>c</mi><mo accent="true">→</mo></mover><mi>j</mi></msub><annotation encoding="application/x-tex">\vec{c}_{j}</annotation></semantics></math>. Written out explicitly, this becomes: <span class="math display">$$\operatorname{vec}(CA^{T})=\left(\begin{array}{c}
\sum_{j}a_{1j}\vec{c}_{j}\\
\sum_{j}a_{2j}\vec{c}_{j}\\
\vdots
\end{array}\right)=\underbrace{\left(\begin{array}{ccc}
a_{11}\Id &amp; a_{12}\Id &amp; \cdots\\
a_{21}\Id &amp; a_{22}\Id &amp; \cdots\\
\vdots &amp; \vdots &amp; \ddots
\end{array}\right)}_{A\otimes\Id}\underbrace{\left(\begin{array}{c}
\vec{c}_{1}\\
\vec{c}_{2}\\
\vdots
\end{array}\right)}_{\operatorname{vec}C},$$</span> and hence we have derived <span class="math display">$$(A\otimes\Id)\operatorname{vec}C=\operatorname{vec}(CA^{T}).$$</span> The full identity <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>⊗</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>vec</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>C</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>vec</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><mi>C</mi><msup><mi>A</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">(A\otimes B)\operatorname{vec}(C)=\operatorname{vec}(BCA^{T})</annotation></semantics></math> can then be obtained by straightforwardly combining these two derivations: replace <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><msup><mi>A</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">CA^T</annotation></semantics></math> with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mi>C</mi><msup><mi>A</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">BCA^T</annotation></semantics></math> in the second derivation, which replaces <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>c</mi><mo accent="true">→</mo></mover><mi>j</mi></msub><annotation encoding="application/x-tex">\vec{c}_j</annotation></semantics></math> with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><msub><mover><mi>c</mi><mo accent="true">→</mo></mover><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">B\vec{c}_j</annotation></semantics></math> and hence <span class="math inline">$\Id$</span> with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>.</p>
</section>
<section id="the-jacobian-in-kronecker-product-notation" class="level3">
<h3 class="anchored" data-anchor-id="the-jacobian-in-kronecker-product-notation">The Jacobian in Kronecker-product notation</h3>
<p>So now we want to use Prop.&nbsp;<a href="#prop5000" data-reference-type="ref" data-reference="prop5000">[prop5000]</a> to calculate the Jacobian of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>A</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">f(A)=A^{2}</annotation></semantics></math> in terms of the Kronecker product. Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">dA</annotation></semantics></math> be our <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math> in Prop.&nbsp;<a href="#prop5000" data-reference-type="ref" data-reference="prop5000">[prop5000]</a>. We can now immediately see that <span class="math display">$$\operatorname{vec}(A\,dA+dA\,A)=\underbrace{(\Id\otimes A+A^{T}\otimes\Id)}_{\mbox{Jacobian }\tilde{f}'(\operatorname{vec}A)}\operatorname{vec}(dA) \, ,$$</span> where <span class="math inline">$\Id$</span> is the identity matrix of the same size as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>. We can also write this in our “non-vectorized” linear-operator notation: <span class="math display">$$A\,dA+dA\,A=(\Id\otimes A+A^{T}\otimes\Id)[dA] \, .$$</span> In the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2\times2</annotation></semantics></math> example, these Kronecker products can be computed explicitly: <span class="math display">$$\begin{aligned}
\underbrace{\begin{pmatrix}1 &amp; \\
&amp; 1
\end{pmatrix}}_{\Id}\otimes\underbrace{\begin{pmatrix}p &amp; r\\
q &amp; s
\end{pmatrix}}_{A}+\underbrace{\begin{pmatrix}p &amp; q\\
r &amp; s
\end{pmatrix}}_{A^{T}}\otimes\underbrace{\begin{pmatrix}1 &amp; \\
&amp; 1
\end{pmatrix}}_{\Id} &amp; =\underbrace{\left(\begin{array}{cccc}
p &amp; r &amp;  &amp; \\
q &amp; s &amp;  &amp; \\
&amp;  &amp; p &amp; r\\
&amp;  &amp; q &amp; s
\end{array}\right)}_{\Id\otimes A}+\underbrace{\left(\begin{array}{cccc}
p &amp;  &amp; q &amp; \\
&amp; p &amp;  &amp; q\\
r &amp;  &amp; s &amp; \\
&amp; r &amp;  &amp; s
\end{array}\right)}_{A^{T}\otimes\Id}\\
&amp; =\left(\begin{array}{cccc}
2p &amp; r &amp; q &amp; 0\\
q &amp; p+s &amp; 0 &amp; q\\
r &amp; 0 &amp; p+s &amp; r\\
0 &amp; r &amp; q &amp; 2s
\end{array}\right)=\tilde{f}'\,,
\end{aligned}$$</span> which exactly matches our laboriously computed Jacobian <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>f</mi><mo accent="true">̃</mo></mover><mi>′</mi></mrow><annotation encoding="application/x-tex">\tilde{f}'</annotation></semantics></math> from earlier!</p>
<div class="example">
<p>For the matrix-cube function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>A</mi><mn>3</mn></msup><annotation encoding="application/x-tex">A^{3}</annotation></semantics></math>, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">m\times m</annotation></semantics></math> square matrix, compute the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>m</mi><mn>2</mn></msup><mo>×</mo><msup><mi>m</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">m^{2}\times m^{2}</annotation></semantics></math> Jacobian of the vectorized function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>vec</mo><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>A</mi><mn>3</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{vec}(A^{3})</annotation></semantics></math>.</p>
</div>
<p>Let’s use the same trick for the matrix-cube function. Sure, we could laboriously compute the Jacobian via element-by-element partial derivatives (which is done nicely by symbolic computing in the notebook), but it’s much easier and more elegant to use Kronecker products. Recall that our “non-vectorized” matrix-calculus derivative is the linear operator: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>A</mi><mn>3</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>A</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>d</mi><mi>A</mi><mspace width="0.167em"></mspace><msup><mi>A</mi><mn>2</mn></msup><mo>+</mo><mi>A</mi><mspace width="0.167em"></mspace><mi>d</mi><mi>A</mi><mspace width="0.167em"></mspace><mi>A</mi><mo>+</mo><msup><mi>A</mi><mn>2</mn></msup><mspace width="0.167em"></mspace><mi>d</mi><mi>A</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">(A^{3})'[dA]=dA\,A^{2}+A\,dA\,A+A^{2}\,dA,</annotation></semantics></math> which now vectorizes by three applications of the Kronecker identity: <span class="math display">$$\operatorname{vec}(dA\,A^{2}+A\,dA\,A+A^{2}\,dA)=\underbrace{\left((A^{2})^{T}\otimes\Id+A^{T}\otimes A+\Id\otimes A^{2}\right)}_{\text{vectorized Jacobian}}\operatorname{vec}(dX)\,.$$</span> You could go on to find the Jacobians of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>A</mi><mn>4</mn></msup><annotation encoding="application/x-tex">A^{4}</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>A</mi><mn>5</mn></msup><annotation encoding="application/x-tex">A^{5}</annotation></semantics></math>, and so on, or any linear combination of matrix powers. Indeed, you could imagine applying a similar process to the Taylor series of any (analytic) matrix function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(A)</annotation></semantics></math>, but it starts to become awkward. Later on (and in homework), we will discuss more elegant ways to differentiate other matrix functions, not as vectorized Jacobians but as linear operators on matrices.</p>
</section>
<section id="the-computational-cost-of-kronecker-products" class="level3">
<h3 class="anchored" data-anchor-id="the-computational-cost-of-kronecker-products">The computational cost of Kronecker products</h3>
<p>One must be cautious about using Kronecker products as a <em>computational</em> tool, rather than as more of a <em>conceptual</em> tool, because they can easily cause the computational cost of matrix problems to explode far beyond what is necessary.</p>
<p>Suppose that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math> are all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">m\times m</annotation></semantics></math> matrices. The cost of multiplying two <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">m\times m</annotation></semantics></math> matrices (by the usual methods) scales proportional to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∼</mo><msup><mi>m</mi><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">\sim m^{3}</annotation></semantics></math>, what the computer scientists call <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>m</mi><mn>3</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Theta(m^{3})</annotation></semantics></math> “complexity.” Hence, the cost of the linear operation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>↦</mo><mi>B</mi><mi>C</mi><msup><mi>A</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">C\mapsto BCA^{T}</annotation></semantics></math> scales as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∼</mo><msup><mi>m</mi><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">\sim m^{3}</annotation></semantics></math> (two <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">m\times m</annotation></semantics></math> multiplications). However, if we instead compute the <em>same answer</em> via <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>vec</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><mi>C</mi><msup><mi>A</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>⊗</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>vec</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">\operatorname{vec}(BCA^{T})=(A\otimes B)\operatorname{vec}C</annotation></semantics></math>, then we must:</p>
<ol type="1">
<li><p>Form the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>m</mi><mn>2</mn></msup><mo>×</mo><msup><mi>m</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">m^{2}\times m^{2}</annotation></semantics></math> matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>⊗</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A\otimes B</annotation></semantics></math>. This requires <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>m</mi><mn>4</mn></msup><annotation encoding="application/x-tex">m^{4}</annotation></semantics></math> multiplications (all entries of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> by all entries of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>), and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∼</mo><msup><mi>m</mi><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">\sim m^{4}</annotation></semantics></math> memory storage. (Compare to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∼</mo><msup><mi>m</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\sim m^{2}</annotation></semantics></math> memory to store <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>. If <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> is 1000, this is a <em>million</em> times more storage, terabytes instead of megabytes!)</p></li>
<li><p>Multiply <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>⊗</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A\otimes B</annotation></semantics></math> by the vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>vec</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">\operatorname{vec}C</annotation></semantics></math> of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>m</mi><mn>2</mn></msup><annotation encoding="application/x-tex">m^{2}</annotation></semantics></math> entries. Multiplying an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n\times n</annotation></semantics></math> matrix by a vector requires <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∼</mo><msup><mi>n</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\sim n^{2}</annotation></semantics></math> operations, and here <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><msup><mi>m</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">n=m^{2}</annotation></semantics></math>, so this is again <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∼</mo><msup><mi>m</mi><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">\sim m^{4}</annotation></semantics></math> arithmetic operations.</p></li>
</ol>
<p>So, instead of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∼</mo><msup><mi>m</mi><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">\sim m^{3}</annotation></semantics></math> operations and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∼</mo><msup><mi>m</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\sim m^{2}</annotation></semantics></math> storage to compute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mi>C</mi><msup><mi>A</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">BCA^{T}</annotation></semantics></math>, using <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>⊗</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>vec</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">(A\otimes B)\operatorname{vec}C</annotation></semantics></math> requires <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∼</mo><msup><mi>m</mi><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">\sim m^{4}</annotation></semantics></math> operations and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∼</mo><msup><mi>m</mi><mn>4</mn></msup></mrow><annotation encoding="application/x-tex">\sim m^{4}</annotation></semantics></math> storage, vastly worse! Essentially, this is because <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>⊗</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">A\otimes B</annotation></semantics></math> has a lot of structure that we are not exploiting (it is a <em>very special</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>m</mi><mn>2</mn></msup><mo>×</mo><msup><mi>m</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">m^{2}\times m^{2}</annotation></semantics></math> matrix).</p>
<p>There are many examples of this nature. Another famous one involves solving the linear <em>matrix</em> equations <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>X</mi><mo>+</mo><mi>X</mi><mi>B</mi><mo>=</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">AX+XB=C</annotation></semantics></math> for an unknown matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>, given <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>,</mo><mi>B</mi><mo>,</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">A,B,C</annotation></semantics></math>, where all of these are <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">m\times m</annotation></semantics></math> matrices. This is called a “Sylvester equation.” These are <em>linear</em> equations in our unknown <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>, and we can convert them to an ordinary system of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>m</mi><mn>2</mn></msup><annotation encoding="application/x-tex">m^{2}</annotation></semantics></math> linear equations by Kronecker products: <span class="math display">$$\operatorname{vec}(AX+XB)=(\Id\otimes A+B^{T}\otimes\Id)\operatorname{vec}X=\operatorname{vec}C,$$</span> which you can then solve for the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>m</mi><mn>2</mn></msup><annotation encoding="application/x-tex">m^{2}</annotation></semantics></math> unknowns <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>vec</mo><mi>X</mi></mrow><annotation encoding="application/x-tex">\operatorname{vec}X</annotation></semantics></math> using Gaussian elimination. But the cost of solving an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>m</mi><mn>2</mn></msup><mo>×</mo><msup><mi>m</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">m^{2}\times m^{2}</annotation></semantics></math> system of equations by Gaussian elimination is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∼</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>m</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mn>3</mn></msup><mo>=</mo><msup><mi>m</mi><mn>6</mn></msup></mrow><annotation encoding="application/x-tex">\sim (m^2)^3 = m^{6}</annotation></semantics></math>. It turns out, however, that there are clever algorithms to solve <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>X</mi><mo>+</mo><mi>X</mi><mi>B</mi><mo>=</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">AX+XB=C</annotation></semantics></math> in only <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∼</mo><msup><mi>m</mi><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">\sim m^{3}</annotation></semantics></math> operations (with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∼</mo><msup><mi>m</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\sim m^{2}</annotation></semantics></math> memory)—for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>=</mo><mn>1000</mn></mrow><annotation encoding="application/x-tex">m=1000</annotation></semantics></math>, this saves a factor of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∼</mo><msup><mi>m</mi><mn>3</mn></msup><mo>=</mo><msup><mn>10</mn><mn>9</mn></msup></mrow><annotation encoding="application/x-tex">\sim m^3 = {10}^9</annotation></semantics></math> (a <em>billion</em>) in computational effort.</p>
<p>(Kronecker products can be a more practical computational tool for <em>sparse</em> matrices: matrices that are mostly zero, e.g.&nbsp;having only a few nonzero entries per row. That’s because the Kronecker product of two sparse matrices is also sparse, avoiding the huge storage requirements for Kronecker products of non-sparse “dense” matrices. This can be a convenient way to assemble large sparse systems of equations for things like multidimensional PDEs.)</p>
</section>
</section>
</section>
<section id="sec:finitedifference" class="level1">
<h1>Finite-Difference Approximations</h1>
<p>In this section, we will be referring to this <a href="https://github.com/mitmath/matrixcalc/blob/main/notes/Finite%20difference%20checks.ipynb">Julia notebook</a> for calculations that are not included here.</p>
<section id="sec:approximation" class="level2">
<h2 class="anchored" data-anchor-id="sec:approximation">Why compute derivatives approximately instead of exactly?</h2>
<p>Working out derivatives by hand is a notoriously error-prone procedure for complicated functions. Even if every individual step is straightforward, there are so many opportunities to make a mistake, either in the derivation or in its implementation on a computer. Whenever you implement a derivatives, you should <strong>always double-check</strong> for mistakes by comparing it to an independent calculation. The simplest such check is a <em>finite-difference approximation</em>, in which we <em>estimate</em> the derivative(s) by comparing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x + \delta x)</annotation></semantics></math> for one or more “finite” (non-infinitesimal) perturbations <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math>.</p>
<p>There are many finite-difference techniques at varying levels of sophistication, as we will discuss below. They all incur an intrinsic <strong>truncation error</strong> due to the fact that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math> is not infinitesimal. (And we will also see that you can’t make <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math> too small, either, or <em>roundoff errors</em> start exploding!) Moreover, finite differences become expensive for higher-dimensional <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> (in which you need a separate finite difference for each input dimension to compute the full Jacobian). This makes them an approach of last resort for computing derivatives accurately. On the other hand, they are the <em>first</em> method you generally employ in order to <em>check</em> derivatives: if you have a bug in your analytical derivative calculation, usually the answer is completely wrong, so even a crude finite-difference approximation for a single small <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math> (chosen at random in higher dimensions) will typically reveal the problem.</p>
<p>Another alternative is <strong>automatic differentiation</strong> (AD), software/compilers perform <em>analytical</em> derivatives for you. This is extremely reliable and, with modern AD software, can be very efficient. Unfortunately, there is still lots of code, e.g.&nbsp;code calling external libraries in other languages, that AD tools can’t comprehend. And there are other cases where AD is inefficient, typically because it misses some mathematical structure of the problem. Even in such cases, you can often fix AD by defining the derivative of one small piece of your program by hand,<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> which is much easier than differentiating the whole thing. In such cases, you still will typically want a finite-difference check to ensure that you have not made a mistake.</p>
<p>It turns out that finite-difference approximations are a surprisingly complicated subject, with rich connections to many areas of numerical analysis; in this lecture we will just scratch the surface.</p>
</section>
<section id="finite-difference-approximations-easy-version" class="level2">
<h2 class="anchored" data-anchor-id="finite-difference-approximations-easy-version">Finite-Difference Approximations: Easy Version</h2>
<p>The simplest way to check a derivative is to recall that the definition of a differential: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">df = f(x+dx) - f(x) = f'(x) dx</annotation></semantics></math> came from dropping higher-order terms from a small but finite difference: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>f</mi><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>δ</mi><mi>x</mi><mo>+</mo><mi>o</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo stretchy="false" form="postfix">‖</mo><mi>δ</mi><mi>x</mi><mo stretchy="false" form="postfix">‖</mo><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">\delta f = f(x+\delta x) - f(x) = f'(x) \delta x + o(\Vert \delta x \Vert) \, .</annotation></semantics></math> So, we can just compare the <strong>finite difference</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><menclose notation="box"><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></menclose><annotation encoding="application/x-tex">\boxed{f(x+\delta x) - f(x)}</annotation></semantics></math> to our <strong>(directional) derivative operator</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">f'(x) \delta x</annotation></semantics></math> (i.e.&nbsp;the derivative in the direction <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math>). <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x+\delta x) - f(x)</annotation></semantics></math> is also called a <strong>forward difference</strong> approximation. The antonym of a forward difference is a <strong>backward difference</strong> approximation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>−</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>≈</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">f(x) - f(x - \delta x) \approx f'(x) \delta x</annotation></semantics></math>. If you just want to compute a derivative, there is not much practical distinction between forward and backward differences. The distinction becomes more important when discretizing (approximating) differential equations. We’ll look at other possibilities below.</p>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>Note that this definition of forward and backward difference is <strong>not</strong> the same as forward- and backward-mode differentiation—these are <strong>unrelated</strong> concepts.</p>
</div>
<p>If <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> is a scalar, we can also divide both sides by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math> to get an approximation for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)</annotation></semantics></math> instead of for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">df</annotation></semantics></math>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>≈</mo><mfrac><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>δ</mi><mi>x</mi></mrow></mfrac><mo>+</mo><mtext mathvariant="normal">(higher-order corrections)</mtext><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">f'(x) \approx \frac{f(x+\delta x) - f(x)}{\delta x} + \text{(higher-order corrections)} \, .</annotation></semantics></math> This is a more common way to write the forward-difference approximation, but it only works for scalar <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>, whereas in this class we want to think of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> as perhaps belonging to some other vector space.</p>
<p>Finite-difference approximations come in many forms, but they are generally a <strong>last resort</strong> in cases where it’s too much effort to work out an analytical derivative and AD fails. But they are also useful to <strong>check</strong> your analytical derivatives and to quickly <strong>explore</strong>.</p>
</section>
<section id="example-matrix-squaring" class="level2">
<h2 class="anchored" data-anchor-id="example-matrix-squaring">Example: Matrix squaring</h2>
<p>Let’s try the finite-difference approximation for the square function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>A</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">f(A) = A^2</annotation></semantics></math>, where here <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is a square matrix in <span class="math inline">$\R^{m,m}$</span>. By hand, we obtain the product rule <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>A</mi><mspace width="0.167em"></mspace><mi>d</mi><mi>A</mi><mo>+</mo><mi>d</mi><mi>A</mi><mspace width="0.167em"></mspace><mi>A</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">df = A \,dA + dA \, A,</annotation></semantics></math> i.e.&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(A)</annotation></semantics></math> is the <strong>linear operator</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><menclose notation="box"><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>δ</mi><mi>A</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>A</mi><mspace width="0.167em"></mspace><mi>δ</mi><mi>A</mi><mo>+</mo><mi>δ</mi><mi>A</mi><mspace width="0.167em"></mspace><mi>A</mi><mi>.</mi></mrow></menclose><annotation encoding="application/x-tex">\boxed{f'(A)[\delta A] = A \,\delta A + \delta A \,A.}</annotation></semantics></math> This is <em>not equal to</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>A</mi><mspace width="0.167em"></mspace><mi>δ</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">2A \,\delta A</annotation></semantics></math> because <em>in general</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">\delta A</annotation></semantics></math> do not commute. So let’s check this difference against a finite difference. We’ll try it for a <em>random</em> input A and a <em>random small</em> perturbation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">\delta A</annotation></semantics></math>.</p>
<p>Using a random matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>, let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>A</mi><mo>=</mo><mi>A</mi><mo>⋅</mo><msup><mn>10</mn><mrow><mi>−</mi><mn>8</mn></mrow></msup></mrow><annotation encoding="application/x-tex">dA = A \cdot 10^{-8}</annotation></semantics></math>. Then, you can compare <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>+</mo><mi>d</mi><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(A+dA) - f(A)</annotation></semantics></math> to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mspace width="0.167em"></mspace><mi>d</mi><mi>A</mi><mo>+</mo><mi>d</mi><mi>A</mi><mspace width="0.167em"></mspace><mi>A</mi></mrow><annotation encoding="application/x-tex">A \,dA + dA \, A</annotation></semantics></math>. If the matrix you chose was really random, you would get that the approximation minus the exact equality from the product rule has entries with order of magnitude around <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>10</mn><mrow><mi>−</mi><mn>16</mn></mrow></msup><mi>!</mi></mrow><annotation encoding="application/x-tex">10^{-16}!</annotation></semantics></math> However, compared to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>A</mi><mi>d</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">2A dA</annotation></semantics></math>, you’d obtain entries of order <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mn>10</mn><mrow><mi>−</mi><mn>8</mn></mrow></msup><annotation encoding="application/x-tex">10^{-8}</annotation></semantics></math>.</p>
<p>To be more quantitative, we might compute that "norm" <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">‖</mo><mtext mathvariant="normal">approx</mtext><mo>−</mo><mtext mathvariant="normal">exact</mtext><mo stretchy="true" form="postfix">‖</mo></mrow><annotation encoding="application/x-tex">\lVert \text{approx} - \text{exact} \rVert</annotation></semantics></math> which we want to be small. But small <strong>compared to what</strong>? The natural answer is <strong>small compared to the correct answer.</strong> This is called the <a href="https://en.wikipedia.org/wiki/Approximation_error">relative error</a> (or "fractional error") and is computed via <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">relative error</mtext><mo>=</mo><mfrac><mrow><mo stretchy="true" form="prefix">‖</mo><mtext mathvariant="normal">approx</mtext><mo>−</mo><mtext mathvariant="normal">exact</mtext><mo stretchy="true" form="postfix">‖</mo></mrow><mrow><mo stretchy="true" form="prefix">‖</mo><mtext mathvariant="normal">exact</mtext><mo stretchy="true" form="postfix">‖</mo></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">\text{relative error} = \frac{\lVert \text{approx} - \text{exact} \rVert}{\lVert \text{exact} \rVert}.</annotation></semantics></math> Here, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">‖</mo><mi>⋅</mi><mo stretchy="true" form="postfix">‖</mo></mrow><annotation encoding="application/x-tex">\lVert \cdot \rVert</annotation></semantics></math> is a <a href="https://en.wikipedia.org/wiki/Norm_(mathematics)">norm</a>, like the length of a vector. This allows us to understand the size of the error in the finite difference approximation, i.e.&nbsp;it allows us to answer how accurate this approximation is (recall Sec.&nbsp;<a href="#sec:approximation" data-reference-type="ref" data-reference="sec:approximation">4.1</a>).</p>
<p>So, as above, you can compute that the relative error between the approximation and the exact answer is about <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mn>10</mn><mrow><mi>−</mi><mn>8</mn></mrow></msup><annotation encoding="application/x-tex">10^{-8}</annotation></semantics></math>, where as the relative error between <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>A</mi><mi>d</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">2A dA</annotation></semantics></math> and the exact answer is about <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mn>10</mn><mn>0</mn></msup><annotation encoding="application/x-tex">10^{0}</annotation></semantics></math>. This shows that our exact answer is likely correct! Getting a good match up between a random input and small displacement isn’t a proof of correctness of course, but it is always a good thing to check. This kind of randomized comparison will almost always <strong>catch major bugs</strong> where you have calculated the symbolic derivative incorrectly, like in our <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>A</mi><mi>d</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">2A dA</annotation></semantics></math> example.</p>
<div class="definition">
<p>Note that the norm of a matrix that we are using, computed by <code>norm(A)</code> in Julia, is just the direct analogue of the familiar Euclidean norm for the case of vectors. It is simply the square root of the sum of the matrix entries squared: <span class="math display">$$\lVert A \rVert := \sqrt{\sum_{i,j} |A_{ij}|^2} = \sqrt{\tr (A^T A)} \, .$$</span> This is called the <a href="https://mathworld.wolfram.com/FrobeniusNorm.html">Frobenius norm</a>.</p>
</div>
</section>
<section id="accuracy-of-finite-differences" class="level2">
<h2 class="anchored" data-anchor-id="accuracy-of-finite-differences">Accuracy of Finite Differences</h2>
<p>Now how accurate is our finite-difference approximation above? How should we choose the size of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math>?</p>
<p>Let’s again consider the example <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>A</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">f(A) = A^2</annotation></semantics></math>, and plot the relative error as a function of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">‖</mo><mi>δ</mi><mi>A</mi><mo stretchy="true" form="postfix">‖</mo></mrow><annotation encoding="application/x-tex">\lVert \delta A \rVert</annotation></semantics></math>. This plot will be done <em>logarithmically</em> (on a log–log scale) so that we can see power-law relationships as straight lines.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="figures/fig1.pdf.svg" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Forward-difference accuracy for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>A</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">f(A) = A^2</annotation></semantics></math>, showing the relative error in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>f</mi><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>+</mo><mi>δ</mi><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\delta f = f(A + \delta A) - f(A)</annotation></semantics></math> versus the linearization <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>δ</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">f'(A) \delta A</annotation></semantics></math>, as a function of the magnitude <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">‖</mo><mi>δ</mi><mi>A</mi><mo stretchy="false" form="postfix">‖</mo></mrow><annotation encoding="application/x-tex">\Vert \delta A\Vert</annotation></semantics></math>. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>×</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">4 \times 4</annotation></semantics></math> matrix with unit-variance Gaussian random entries, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">\delta A</annotation></semantics></math> is similarly a unit-variance Gaussian random perturbation scaled by a factor <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math> ranging from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math> to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mn>10</mn><mrow><mi>−</mi><mn>16</mn></mrow></msup><annotation encoding="application/x-tex">10^{-16}</annotation></semantics></math>.</figcaption>
</figure>
</div>
<p>We notice two main features as we decrease <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">\delta A</annotation></semantics></math>:</p>
<ol type="1">
<li><p>The relative error at first decreases linearly with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">‖</mo><mi>δ</mi><mi>A</mi><mo stretchy="true" form="postfix">‖</mo></mrow><annotation encoding="application/x-tex">\lVert \delta A\rVert</annotation></semantics></math>. This is called <strong>first-order accuracy</strong>. Why?</p></li>
<li><p>When <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">\delta A</annotation></semantics></math> gets too small, the error increases. Why?</p></li>
</ol>
</section>
<section id="order-of-accuracy" class="level2">
<h2 class="anchored" data-anchor-id="order-of-accuracy">Order of accuracy</h2>
<p>The <strong>truncation error</strong> is the inaccuracy arising from the fact that the input perturbation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math> is not infinitesimal: we are computing a difference, not a derivative. If the truncation error in the derivative scales proportional <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">‖</mo><mi>δ</mi><mi>x</mi><msup><mo stretchy="false" form="postfix">‖</mo><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">\Vert \delta x \Vert^n</annotation></semantics></math>, we call the approximation <strong>n-th order accurate</strong>. For forward differences, here, the order is <strong>n=1</strong>. Why?</p>
<p>For any <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math> with a nonzero second derivative (think of the Taylor series), we have <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>δ</mi><mi>x</mi><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mtext mathvariant="normal">terms proportional to </mtext><mspace width="0.333em"></mspace></mrow><mo stretchy="false" form="postfix">‖</mo><mi>δ</mi><mi>x</mi><msup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><munder><munder><mrow><mi>o</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo stretchy="false" form="postfix">‖</mo><mi>δ</mi><mi>x</mi><msup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mo accent="true">⏟</mo></munder><mtext mathvariant="normal">i.e. higher-order terms</mtext></munder></mrow><annotation encoding="application/x-tex">f(x + \delta x) = f(x) + f'(x) \delta x + (\text{terms proportional to }\Vert \delta x \Vert^2) + \underbrace{o(\Vert \delta x \Vert^2)}_\text{i.e. higher-order terms}</annotation></semantics></math> That is, the terms we <em>dropped</em> in our forward-difference approximations are proportional to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">‖</mo><mi>δ</mi><mi>x</mi><msup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\Vert \delta x\Vert^2</annotation></semantics></math>. But that means that the <strong>relative error is linear</strong>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mtext mathvariant="normal">relative error</mtext></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mrow><mo stretchy="false" form="postfix">‖</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>δ</mi><mi>x</mi><mo stretchy="false" form="postfix">‖</mo></mrow><mrow><mo stretchy="false" form="postfix">‖</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>δ</mi><mi>x</mi><mo stretchy="false" form="postfix">‖</mo></mrow></mfrac></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mtext mathvariant="normal">terms proportional to </mtext><mspace width="0.333em"></mspace></mrow><mo stretchy="false" form="postfix">‖</mo><mi>δ</mi><mi>x</mi><msup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>o</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo stretchy="false" form="postfix">‖</mo><mi>δ</mi><mi>x</mi><msup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mrow><mtext mathvariant="normal">proportional to </mtext><mspace width="0.333em"></mspace></mrow><mo stretchy="false" form="postfix">‖</mo><mi>δ</mi><mi>x</mi><mo stretchy="false" form="postfix">‖</mo></mrow></mfrac><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mtext mathvariant="normal">terms proportional to </mtext><mspace width="0.333em"></mspace></mrow><mo stretchy="false" form="postfix">‖</mo><mi>δ</mi><mi>x</mi><mo stretchy="false" form="postfix">‖</mo><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>o</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo stretchy="false" form="postfix">‖</mo><mi>δ</mi><mi>x</mi><mo stretchy="false" form="postfix">‖</mo><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    \text{relative error} &amp;= \frac{\Vert f(x+\delta x) - f(x) - f'(x) \delta x \Vert}{\Vert f'(x) \delta x \Vert} \\
    &amp;= \frac{(\text{terms proportional to }\Vert \delta x \Vert^2) + o(\Vert \delta x \Vert^2)}{\text{proportional to }\Vert \delta x \Vert} = (\text{terms proportional to }\Vert \delta x \Vert) + o(\Vert \delta x \Vert)
\end{aligned}</annotation></semantics></math> This is <strong>first-order accuracy</strong>. Truncation error in a finite-difference approximation is the <strong>inherent</strong> error in the formula for <strong>non-infinitesimal</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math>. Does that mean we should just make <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math> as small as we possibly can?</p>
</section>
<section id="roundoff-error" class="level2">
<h2 class="anchored" data-anchor-id="roundoff-error">Roundoff error</h2>
<p>The reason why the error <em>increased</em> for very small <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">\delta A</annotation></semantics></math> was due to <strong>roundoff errors</strong>. The computer only stores a <strong>finite number of significant digits</strong> (about 15 decimal digits) for each real number and rounds off the rest on each operation — this is called <a href="https://en.wikipedia.org/wiki/Floating-point_arithmetic">floating-point arithmetic</a>. If <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math> is too small, then the difference <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x+\delta x) - f(x)</annotation></semantics></math> gets rounded off to zero (some or all of the <em>significant digits cancel</em>). This is called <a href="https://en.wikipedia.org/wiki/Catastrophic_cancellation">catastrophic cancellation</a>.</p>
<p>Floating-point arithmetic is much like scientific notation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>*</mi><mi>.</mi><mo>*</mo><mi>*</mi><mo>*</mo><mi>*</mi><mo>*</mo><mi>×</mi><msup><mn>10</mn><mi>e</mi></msup></mrow><annotation encoding="application/x-tex">{*}.{*}{*}{*}{*}{*} \times 10^e</annotation></semantics></math>: a finite-precision coefficient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>*</mi><mi>.</mi><mo>*</mo><mi>*</mi><mo>*</mo><mi>*</mi><mo>*</mo></mrow><annotation encoding="application/x-tex">{*}.{*}{*}{*}{*}{*}</annotation></semantics></math> scaled by a power of&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>10</mn><annotation encoding="application/x-tex">10</annotation></semantics></math> (or, on a computer, a power of&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>2</mn><annotation encoding="application/x-tex">2</annotation></semantics></math>). The number of digits in the coefficient (the “significant digits”) is the “precision,” which in the usual 64-bit floating-point arithmetic is charactized by a quantity <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi><mo>=</mo><msup><mn>2</mn><mrow><mi>−</mi><mn>52</mn></mrow></msup><mo>≈</mo><mn>2.22</mn><mo>×</mo><msup><mn>10</mn><mrow><mi>−</mi><mn>16</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\epsilon = 2^{-52} \approx 2.22 \times 10^{-16}</annotation></semantics></math>, called the <a href="https://en.wikipedia.org/wiki/Machine_epsilon">machine epsilon</a>. When an arbitrary real number <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">y \in \mathbb{R}</annotation></semantics></math> is rounded to the closest floating-point value <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>y</mi><mo accent="true">̃</mo></mover><annotation encoding="application/x-tex">\tilde{y}</annotation></semantics></math>, the roundoff error is bounded by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">|</mo><mover><mi>y</mi><mo accent="true">̃</mo></mover><mo>−</mo><mi>y</mi><mo stretchy="true" form="postfix">|</mo></mrow><mo>≤</mo><mi>ϵ</mi><mrow><mo stretchy="true" form="prefix">|</mo><mi>y</mi><mo stretchy="true" form="postfix">|</mo></mrow></mrow><annotation encoding="application/x-tex">|\tilde{y} - y| \le \epsilon |y|</annotation></semantics></math>. Equivalently, the computer keeps only about 15–16 <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≈</mo><mi>−</mi><msub><mo>log</mo><mn>10</mn></msub><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\approx -\log_{10} \epsilon</annotation></semantics></math> decimal digits, or really <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>53</mn><mo>=</mo><mn>1</mn><mo>−</mo><msub><mo>log</mo><mn>2</mn></msub><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">53 = 1 - \log_2 \epsilon</annotation></semantics></math> <em>binary</em> digits, for each number.</p>
<p>In our finite-difference example, for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">‖</mo><mi>δ</mi><mi>A</mi><mo stretchy="false" form="postfix">‖</mo><mi>/</mi><mo stretchy="false" form="postfix">‖</mo><mi>A</mi><mo stretchy="false" form="postfix">‖</mo></mrow><annotation encoding="application/x-tex">\Vert \delta A \Vert / \Vert A \Vert</annotation></semantics></math> of roughly <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>10</mn><mrow><mi>−</mi><mn>8</mn></mrow></msup><mo>≈</mo><msqrt><mi>ϵ</mi></msqrt><mrow><mo stretchy="true" form="prefix">‖</mo><mi>A</mi><mo stretchy="true" form="postfix">‖</mo></mrow></mrow><annotation encoding="application/x-tex">10^{-8} \approx \sqrt{\epsilon} \lVert A\rVert</annotation></semantics></math> or larger, the approximation for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(A)</annotation></semantics></math> is dominated by the truncation error, but if we go smaller than that the relative error starts increasing due to roundoff. Experience has shown that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">‖</mo><mi>δ</mi><mi>x</mi><mo stretchy="false" form="postfix">‖</mo><mo>≈</mo><msqrt><mi>ϵ</mi></msqrt><mo stretchy="false" form="postfix">‖</mo><mi>x</mi><mo stretchy="false" form="postfix">‖</mo></mrow><annotation encoding="application/x-tex">\Vert \delta x \Vert \approx \sqrt{\epsilon} \Vert x \Vert</annotation></semantics></math> is often a good rule of thumb—about half the significant digits is the most that is reasonably safe to rely on—but the precise crossover point of minimum error depends on the function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> and the finite-difference method. But, like all rules of thumb, this may not always be completely reliable.</p>
</section>
<section id="other-finite-difference-methods" class="level2">
<h2 class="anchored" data-anchor-id="other-finite-difference-methods">Other finite-difference methods</h2>
<p>There are more sophisticated finite-difference methods, such as Richardson extrapolation, which consider a sequence of progressively smaller <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math> values in order to adaptively determine the best possible estimate for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math> (<em>extrapolating</em> to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi><mo>→</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\delta x \to 0</annotation></semantics></math> using progressively higher degree polynomials). One can also use higher-order difference formulas than the simple forward-difference method here, so that the truncation error decreases faster than than linearly with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math>. The most famous higher-order formula is the “centered difference” <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>δ</mi><mi>x</mi><mo>≈</mo><mrow><mo stretchy="true" form="prefix">[</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>−</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">f'(x) \delta x \approx [f(x + \delta x) - f(x - \delta x)]/2</annotation></semantics></math>, which has <em>second</em>-order accuracy (relative truncation error proportional to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">‖</mo><mi>δ</mi><mi>x</mi><msup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\Vert \delta x\Vert^2</annotation></semantics></math>).</p>
<p>Higher-dimensional inputs <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> pose a fundamental computational challenge for finite-difference techniques, because if you want to know what happens for every possible direction <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math> then you need many finite differences: one for each dimension of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math>. For example, suppose <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">x \in \mathbb{R}^n</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">f(x) \in \mathbb{R}</annotation></semantics></math>, so that you are computing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">\nabla f \in \mathbb{R}^n</annotation></semantics></math>; if you want to know the whole gradient, you need <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> <em>separate</em> finite differences. The net result is that finite differences in higher dimensions are expensive, quickly becoming impractical for high-dimensional optimization (e.g.&nbsp;neural networks) where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> might be huge. On the other hand, if you are just using finite differences as a check for bugs in your code, it is usually sufficient to compare <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x+\delta x) - f(x)</annotation></semantics></math> to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)[\delta x]</annotation></semantics></math> in a few random directions, i.e.&nbsp;for a few random small <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math>.</p>
</section>
</section>
<section id="sec:generalvectorspaces" class="level1">
<h1>Derivatives in General Vector Spaces</h1>
<p>Matrix calculus requires us to generalize concepts of derivative and gradient further, to functions whose inputs and/or outputs are not simply scalars or column vectors. To achieve this, we extend the notion of the ordinary vector <strong>dot product</strong> and ordinary Euclidean vector “length” to general <strong>inner products</strong> and <strong>norms</strong> on <strong>vector spaces</strong>. Our first example will consider familiar matrices from this point of view.</p>
<p>Recall from linear algebra that we can call any set <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math> a “vector space” if its elements can be added/subtracted <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>±</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">x \pm y</annotation></semantics></math> and multiplied by scalars <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\alpha x</annotation></semantics></math> (subject to some basic arithmetic axioms, e.g.&nbsp;the distributive law). For example, the set of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m \times n</annotation></semantics></math> matrices themselves form a vector space, or even the set of continuous functions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(x)</annotation></semantics></math> (mapping <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ℝ</mi><mo>→</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">\mathbb{R} \to \mathbb{R}</annotation></semantics></math>)—the key fact is that we can add/subtract/scale them and get elements of the same set. It turns out to be extraordinarily useful to extend differentiation to such spaces, e.g.&nbsp;for functions that map matrices to matrices or functions to numbers. Doing so crucially relies on our input/output vector spaces <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math> having a <strong>norm</strong> and, ideally, an <strong>inner product</strong>.</p>
<section id="a-simple-matrix-dot-product-and-norm" class="level2">
<h2 class="anchored" data-anchor-id="a-simple-matrix-dot-product-and-norm">A Simple Matrix Dot Product and Norm</h2>
<p>Recall that for <em>scalar-valued</em> functions <span class="math inline">$f(x) \in \R$</span> with <em>vector inputs</em> <span class="math inline">$x\in \R^n$</span> (i.e.&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-component “column vectors") we have that <span class="math display">$$df = f(x + dx) - f(x) = f'(x) [dx] \in \R.$$</span> Therefore, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)</annotation></semantics></math> is a linear operator taking in the vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> in and giving a scalar value out. Another way to view this is that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)</annotation></semantics></math> is the row vector<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><annotation encoding="application/x-tex">(\nabla f)^T</annotation></semantics></math>. Under this viewpoint, it follows that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">df</annotation></semantics></math> is the dot product (or”inner product”): <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>∇</mi><mi>f</mi><mo>⋅</mo><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">df = \nabla f \cdot dx</annotation></semantics></math></p>
<p>We can generalize this to any vector space <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math> with inner products! Given <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">x\in V</annotation></semantics></math>, and a scalar-valued function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>, we obtain the linear operator <span class="math inline">$f'(x) [dx] \in \R$</span>, called a “linear form.” In order to define the gradient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math>, we need an inner product for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math>, the vector-space generalization of the familiar dot product!</p>
<p>Given <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>,</mo><mi>y</mi><mo>∈</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">x,y \in V</annotation></semantics></math>, the inner product <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">⟩</mo></mrow><annotation encoding="application/x-tex">\langle x, y \rangle</annotation></semantics></math> is a map (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>⋅</mi><annotation encoding="application/x-tex">\cdot</annotation></semantics></math>) such that <span class="math inline">$\langle x, y \rangle \in \R$</span>. This is also commonly denoted <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>⋅</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">x \cdot y</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>x</mi><mo>∣</mo><mi>y</mi><mo stretchy="false" form="postfix">⟩</mo></mrow><annotation encoding="application/x-tex">\langle x \mid y \rangle</annotation></semantics></math>. More technically, an inner product is a map that is</p>
<ol type="1">
<li><p><strong>Symmetric</strong>: i.e.&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">⟩</mo><mo>=</mo><mo stretchy="false" form="prefix">⟨</mo><mi>y</mi><mo>,</mo><mi>x</mi><mo stretchy="false" form="postfix">⟩</mo></mrow><annotation encoding="application/x-tex">\langle x, y \rangle = \langle y, x \rangle</annotation></semantics></math> (or conjugate-symmetric,<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">⟩</mo><mo>=</mo><mover><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>y</mi><mo>,</mo><mi>x</mi><mo stretchy="false" form="postfix">⟩</mo></mrow><mo accent="true">¯</mo></mover></mrow><annotation encoding="application/x-tex">\langle x, y \rangle = \overline{\langle y, x \rangle}</annotation></semantics></math>, if we were using complex numbers),</p></li>
<li><p><strong>Linear</strong>: i.e.&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>x</mi><mo>,</mo><mi>α</mi><mi>y</mi><mo>+</mo><mi>β</mi><mi>z</mi><mo stretchy="false" form="postfix">⟩</mo><mo>=</mo><mi>α</mi><mo stretchy="false" form="prefix">⟨</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">⟩</mo><mo>+</mo><mi>β</mi><mo stretchy="false" form="prefix">⟨</mo><mi>x</mi><mo>,</mo><mi>z</mi><mo stretchy="false" form="postfix">⟩</mo></mrow><annotation encoding="application/x-tex">\langle x, \alpha y + \beta z\rangle = \alpha \langle x, y \rangle + \beta \langle x, z \rangle</annotation></semantics></math>, and</p></li>
<li><p><strong>Non-negative</strong>: i.e.&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>x</mi><mo>,</mo><mi>x</mi><mo stretchy="false" form="postfix">⟩</mo><mo>:=</mo><msup><mrow><mo stretchy="true" form="prefix">‖</mo><mi>x</mi><mo stretchy="true" form="postfix">‖</mo></mrow><mn>2</mn></msup><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\langle x, x \rangle := \lVert x \rVert^2 \geq 0</annotation></semantics></math>, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">=0</annotation></semantics></math> if and only if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">x = 0</annotation></semantics></math>.</p></li>
</ol>
<p>Note that the combination of the first two properties means that it must also be linear in the left vector (or conjugate-linear, if we were using complex numbers). Another useful consequence of these three properties, which is a bit trickier to derive, is the <em>Cauchy–Schwarz inequality</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">|</mo><mo stretchy="false" form="prefix">⟨</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">⟩</mo><mo stretchy="true" form="postfix">|</mo></mrow><mo>≤</mo><mo stretchy="false" form="postfix">‖</mo><mi>x</mi><mo stretchy="false" form="postfix">‖</mo><mspace width="0.167em"></mspace><mo stretchy="false" form="postfix">‖</mo><mi>y</mi><mo stretchy="false" form="postfix">‖</mo></mrow><annotation encoding="application/x-tex">|\langle x, y \rangle| \le \Vert x \Vert \, \Vert y \Vert</annotation></semantics></math>.</p>
<div class="definition">
<p>A (complete) vector space with an inner product is called a <em>Hilbert space</em>. (The technical requirement of “completeness” essentially means that you can take limits in the space, and is important for rigorous proofs.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>)</p>
</div>
<p>Once we have a Hilbert space, we can define the gradient for scalar-valued functions. Given <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">x\in V</annotation></semantics></math> a Hilbert space, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math> scalar, then we have the linear form <span class="math inline">$f'(x) [dx] \in \R$</span>. Then, under these assumptions, there is a theorem known as the “Riesz representation theorem” stating that <em>any</em> linear form (including <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math>) must be an inner product with <em>something</em>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">⟨</mo><munder><munder><mtext mathvariant="normal">(some vector)</mtext><mo accent="true">⏟</mo></munder><mrow><mrow><mtext mathvariant="normal">gradient </mtext><mspace width="0.333em"></mspace></mrow><mi>∇</mi><mi>f</mi><msub><mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">|</mo><mi>x</mi></msub></mrow></munder><mo>,</mo><mi>d</mi><mi>x</mi><mo minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">⟩</mo><mo>=</mo><mi>d</mi><mi>f</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">f'(x) [dx] = \big\langle \underbrace{\text{(some vector)}}_{\text{gradient } \nabla f\bigr|_x} , dx \big\rangle = df.</annotation></semantics></math> That is, the gradient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> is <em>defined</em> as the thing you take the inner product of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> with to get <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">df</annotation></semantics></math>. Note that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> always has the “same shape” as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>.</p>
<p>The first few examples we look at involve the usual Hilbert space <span class="math inline">$V = \R^n$</span> with different inner products.</p>
<div class="example">
<p>Given <span class="math inline">$V = \R^n$</span> with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-column vectors, we have the familiar Euclidean dot product <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">⟩</mo><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><mi>y</mi></mrow><annotation encoding="application/x-tex">\langle x, y \rangle = x^T y</annotation></semantics></math>. This leads to the usual <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math>.</p>
</div>
<div class="example">
<p>We can have different inner products on <span class="math inline">$\R^n$</span>. For instance, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>x</mi><mo>,</mo><mi>y</mi><msub><mo stretchy="false" form="postfix">⟩</mo><mi>W</mi></msub><mo>=</mo><msub><mi>w</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><msub><mi>y</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>2</mn></msub><msub><mi>x</mi><mn>2</mn></msub><msub><mi>y</mi><mn>2</mn></msub><mo>+</mo><mi>…</mi><msub><mi>w</mi><mi>n</mi></msub><msub><mi>x</mi><mi>n</mi></msub><msub><mi>y</mi><mi>n</mi></msub><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><munder><munder><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>w</mi><mn>1</mn></msub></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"><mo>⋱</mo></mtd><mtd columnalign="center" style="text-align: center"></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>w</mi><mi>n</mi></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo accent="true">⏟</mo></munder><mi>W</mi></munder><mi>y</mi></mrow><annotation encoding="application/x-tex">\langle x, y\rangle_W = w_1 x_1 y_1 + w_2 x_2 y_2 + \dots w_n x_n y_n = x^T \underbrace{\begin{pmatrix}
        w_1 &amp; &amp; \\
         &amp; \ddots &amp; \\
         &amp; &amp; w_n
    \end{pmatrix}}_{W} y</annotation></semantics></math> for weights <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>w</mi><mi>n</mi></msub><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">w_1,\dots, w_n &gt;0</annotation></semantics></math>.</p>
<p>More generally we can define a weighted dot product <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>x</mi><mo>,</mo><mi>y</mi><msub><mo stretchy="false" form="postfix">⟩</mo><mi>W</mi></msub><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><mi>W</mi><mi>y</mi></mrow><annotation encoding="application/x-tex">\langle x, y\rangle_W= x^T W y</annotation></semantics></math> for any symmetric-positive-definite matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics></math> (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>=</mo><msup><mi>W</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">W = W^T</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics></math> is positive definite, which is sufficient for this to be a valid inner product).</p>
<p>If we change the definition of the inner product, then we change the definition of the gradient! For example, with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><mi>A</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">f(x) = x^T A x</annotation></semantics></math> we previously found that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>+</mo><msup><mi>A</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">df = x^T (A + A^T) dx</annotation></semantics></math>. With the ordinary Euclidean inner product, this gave a gradient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>+</mo><msup><mi>A</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">\nabla f = (A+A^T)x</annotation></semantics></math>. However, if we use the weighted inner product <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mi>T</mi></msup><mi>W</mi><mi>y</mi></mrow><annotation encoding="application/x-tex">x^T W y</annotation></semantics></math>, then we would obtain a different “gradient” <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>∇</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>W</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mi>f</mi><mo>=</mo><msup><mi>W</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>+</mo><msup><mi>A</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">\nabla^{(W)} f = W^{-1} (A+A^T)x</annotation></semantics></math> so that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mo stretchy="false" form="prefix">⟨</mo><msup><mi>∇</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>W</mi><mo stretchy="true" form="postfix">)</mo></mrow></msup><mi>f</mi><mo>,</mo><mi>d</mi><mi>x</mi><mo stretchy="false" form="postfix">⟩</mo></mrow><annotation encoding="application/x-tex">df = \langle \nabla^{(W)}  f , dx \rangle</annotation></semantics></math>.</p>
<p>In these notes, we will employ the Euclidean inner product for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">x \in \mathbb{R}^n</annotation></semantics></math>, and hence the usual <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math>, unless noted otherwise. However, weighted inner products are useful in lots of cases, especially when the components of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> have different scales/units.</p>
</div>
<p>We can also consider the space of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m\times n</annotation></semantics></math> matrices <span class="math inline">$V = \R^{m \times n}$</span>. There, is of course, a vector-space isomorphism from <span class="math inline">$V \ni A \to \mathrm{vec}(A) \in \R^{mn}$</span>. Thus, in this space we have the analogue of the familiar (“Frobenius") Euclidean inner product, which is convenient to rewrite in terms of matrix operations via the trace:</p>
<div class="definition">
<p>The <strong>Frobenius inner product</strong> of two <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m \times n</annotation></semantics></math> matrices <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> is: <span class="math display">$$\langle A, B \rangle_F = \sum_{ij} A_{ij} B_{ij} = \mathrm{vec}(A)^T \mathrm{vec}(B) = \tr(A^T B) \, .$$</span> Given this inner product, we also have the corresponding <strong>Frobenius norm</strong>: <span class="math display">$$\lVert A \rVert_F = \sqrt{\langle A,A \rangle_F} = \sqrt{\tr(A^TA)} = \lVert \mathrm{vec} A\rVert = \sqrt{\sum_{i,j} |A_{ij}|^2} \, .$$</span> Using this, we can now define the gradient of scalar functions with <em>matrix inputs</em>! This will be our default matrix inner product (hence defining our default matrix gradient) in these notes (sometimes dropping the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math> subscript).</p>
</div>
<div class="example">
<p>Consider the function <span class="math display">$$f(A) = \lVert A \rVert_F = \sqrt{\tr(A^T A)}.$$</span> What is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">df</annotation></semantics></math>?</p>
</div>
<p>Firstly, by the familiar scalar-differentiation chain and power rules we have that <span class="math display">$$df = \frac{1}{2 \sqrt{\tr(A^T A)}} d(\tr A^T A).$$</span> Then, note that (by linearity of the trace) <span class="math display">$$d( \tr B) = \tr(B+dB) - \tr(B) = \tr(B) + \tr(dB) - \tr(B) = \tr(dB).$$</span> Hence, <span class="math display">$$\begin{aligned}
    df &amp;= \frac{1}{2\lVert A\rVert_F} \tr(d(A^T A)) \\
    &amp;= \frac{1}{2\lVert A\rVert_F} \tr( dA^T\, A + A^T\, dA) \\
    &amp;= \frac{1}{2 \lVert A\rVert_F} (\tr(dA^T\, A) + \tr(A^T\, dA)) \\
    &amp;= \frac{1}{\lVert A\rVert_F} \tr(A^T \, dA) = \big\langle \frac{A}{\lVert A\rVert_F} , dA \big\rangle.
\end{aligned}$$</span> Here, we used the fact that <span class="math inline">$\tr B = \tr B^T$</span>, and in the last step we connected <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">df</annotation></semantics></math> with a Frobenius inner product. In other words, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi><mo>=</mo><mi>∇</mi><msub><mrow><mo stretchy="true" form="prefix">‖</mo><mi>A</mi><mo stretchy="true" form="postfix">‖</mo></mrow><mi>F</mi></msub><mo>=</mo><mfrac><mi>A</mi><msub><mrow><mo stretchy="true" form="prefix">‖</mo><mi>A</mi><mo stretchy="true" form="postfix">‖</mo></mrow><mi>F</mi></msub></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">\nabla f = \nabla \lVert A \rVert_F = \frac{A}{\lVert A \rVert_F}.</annotation></semantics></math> Note that one obtains exactly the same result for column vectors&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>, i.e.&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mo stretchy="false" form="postfix">‖</mo><mi>x</mi><mo stretchy="false" form="postfix">‖</mo><mo>=</mo><mi>x</mi><mi>/</mi><mo stretchy="false" form="postfix">‖</mo><mi>x</mi><mo stretchy="false" form="postfix">‖</mo></mrow><annotation encoding="application/x-tex">\nabla \Vert x\Vert = x/\Vert x \Vert</annotation></semantics></math> (and in fact this is equivalent via <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mo>vec</mo><mi>A</mi></mrow><annotation encoding="application/x-tex">x = \operatorname{vec}A</annotation></semantics></math>).</p>
<p>Let’s consider another simple example:</p>
<div class="example">
<p>Fix some constant <span class="math inline">$x \in \R^m$</span>, <span class="math inline">$y\in \R^n$</span>, and consider the function <span class="math inline">$f:\R^{m\times n} \to \R$</span> given by <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><mi>A</mi><mi>y</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">f(A) = x^T A y.</annotation></semantics></math> What is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math>?</p>
</div>
<p>We have that <span class="math display">$$\begin{aligned}
    df &amp;= x^T \,dA \,y \\
    &amp;= \tr( x^T \,dA\, y) \\
    &amp;= \tr(y x^T \, dA) \\
    &amp;= \big\langle \underbrace{x y^T}_{\nabla f} , \, dA \big\rangle.
\end{aligned}$$</span></p>
<p>More generally, for any scalar-valued function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(A)</annotation></semantics></math>, from the definition of Frobenius inner product it follows that: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>+</mo><mi>d</mi><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo stretchy="false" form="prefix">⟨</mo><mi>∇</mi><mi>f</mi><mo>,</mo><mspace width="0.167em"></mspace><mi>d</mi><mi>A</mi><mo stretchy="false" form="postfix">⟩</mo><mo>=</mo><munder><mo>∑</mo><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></munder><msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mspace width="0.167em"></mspace><mi>d</mi><msub><mi>A</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">df = f(A+dA)-f(A) = \langle \nabla f , \, dA \rangle = \sum_{i,j} (\nabla f)_{i,j} \, dA_{i,j} \, ,</annotation></semantics></math> and hence the components of the gradient are exactly the elementwise derivatives <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><msub><mi>A</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></mrow></mfrac><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">(\nabla f)_{i,j} = \frac{\partial f}{\partial A_{i,j}} \, ,</annotation></semantics></math> similar to the component-wise definition of the gradient vector from multivariable calculus! But for non-trivial matrix-input functions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(A)</annotation></semantics></math> it can be extremely awkward to take the derivative with respect to each entry of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> individually. Using the “holistic” matrix inner-product definition, we will soon be able to compute even more complicated matrix-valued gradients, including <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>det</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla (\det A)</annotation></semantics></math>!</p>
</section>
<section id="sec:banach" class="level2">
<h2 class="anchored" data-anchor-id="sec:banach">Derivatives, Norms, and Banach spaces</h2>
<p>We have been using the term “norm” throughout this class, but what technically is a norm? Of course, there are familiar examples such as the Euclidean (“<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mo>ℓ</mo><mn>2</mn></msup><annotation encoding="application/x-tex">\ell^2</annotation></semantics></math>”) norm <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">‖</mo><mi>x</mi><mo stretchy="false" form="postfix">‖</mo><mo>=</mo><msqrt><mrow><msub><mo>∑</mo><mi>k</mi></msub><msubsup><mi>x</mi><mi>k</mi><mn>2</mn></msubsup></mrow></msqrt></mrow><annotation encoding="application/x-tex">\Vert x \Vert = \sqrt{\sum_k x_k^2}</annotation></semantics></math> for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">x\in \mathbb{R}^n</annotation></semantics></math>, but it is useful to consider how this concept generalizes to other vector spaces. It turns out, in fact, that norms are crucial to the definition of a derivative!</p>
<p>Given a vector space <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math>, a norm <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">‖</mo><mi>⋅</mi><mo stretchy="true" form="postfix">‖</mo></mrow><annotation encoding="application/x-tex">\lVert \cdot \rVert</annotation></semantics></math> on <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>V</mi><annotation encoding="application/x-tex">V</annotation></semantics></math> is a map <span class="math inline">$\lVert \cdot \rVert: V\to \R$</span> satisfying the following three properties:</p>
<ol type="1">
<li><p><strong>Non-negative</strong>: i.e.&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">‖</mo><mi>v</mi><mo stretchy="true" form="postfix">‖</mo></mrow><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lVert v \rVert \geq 0</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">‖</mo><mi>v</mi><mo stretchy="true" form="postfix">‖</mo></mrow><mo>=</mo><mn>0</mn><mo>⇔</mo><mi>v</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lVert v \rVert = 0 \iff v = 0</annotation></semantics></math>,</p></li>
<li><p><strong>Homogeneity</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">‖</mo><mi>α</mi><mi>v</mi><mo stretchy="true" form="postfix">‖</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">|</mo><mi>α</mi><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mo stretchy="true" form="prefix">‖</mo><mi>v</mi><mo stretchy="true" form="postfix">‖</mo></mrow></mrow><annotation encoding="application/x-tex">\lVert \alpha v \rVert = |\alpha |\lVert v \rVert</annotation></semantics></math> for any <span class="math inline">$\alpha \in \R$</span>, and</p></li>
<li><p><strong>Triangle inequality</strong>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">‖</mo><mi>u</mi><mo>+</mo><mi>v</mi><mo stretchy="true" form="postfix">‖</mo></mrow><mo>≤</mo><mrow><mo stretchy="true" form="prefix">‖</mo><mi>u</mi><mo stretchy="true" form="postfix">‖</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">‖</mo><mi>v</mi><mo stretchy="true" form="postfix">‖</mo></mrow></mrow><annotation encoding="application/x-tex">\lVert u + v\rVert \leq \lVert u \rVert + \lVert v \rVert</annotation></semantics></math>.</p></li>
</ol>
<p>A vector space that has a norm is called an <em>normed vector space</em>. Often, mathematicians technically want a slightly more precise type of normed vector space with a less obvious name: a <em>Banach</em> space.</p>
<div class="definition">
<p>A (complete) vector space with a norm is called a <em>Banach space</em>. (As with Hilbert spaces, “completeness” is a technical requirement for some types of rigorous analysis, essentially allowing you to take limits.)</p>
</div>
<p>For example, given any inner product <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy="false" form="postfix">⟩</mo></mrow><annotation encoding="application/x-tex">\langle u , v \rangle</annotation></semantics></math>, there is a corresponding norm <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">‖</mo><mi>u</mi><mo stretchy="true" form="postfix">‖</mo></mrow><mo>=</mo><msqrt><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>u</mi><mo>,</mo><mi>u</mi><mo stretchy="false" form="postfix">⟩</mo></mrow></msqrt></mrow><annotation encoding="application/x-tex">\lVert u \rVert = \sqrt{\langle u , u \rangle}</annotation></semantics></math>. (Thus, every Hilbert space is also a Banach space.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>)</p>
<p>To define derivatives, we technically need both the input <em>and</em> the output to be Banach spaces. To see this, recall our formalism <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munder><munder><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><mo accent="true">⏟</mo></munder><mtext mathvariant="normal">linear</mtext></munder><mspace width="0.278em"></mspace><mo>+</mo><munder><munder><mrow><mi>o</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mo accent="true">⏟</mo></munder><mtext mathvariant="normal">smaller</mtext></munder><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">f(x + \delta x) - f(x) = \underbrace{f'(x) [\delta x]}_{\mbox{linear}} \; + \underbrace{o(\delta x)}_{\mbox{smaller}}\, .</annotation></semantics></math> To precisely define the sense in which the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>o</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">o(\delta x)</annotation></semantics></math> terms are “smaller” or “higher-order,” we need norms. In particular, the “little-<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>o</mi><annotation encoding="application/x-tex">o</annotation></semantics></math>” notation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>o</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">o(\delta x)</annotation></semantics></math> denotes any function such that <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><munder><mo>lim</mo><mrow><mi>δ</mi><mi>x</mi><mo>→</mo><mn>0</mn></mrow></munder><mfrac><mrow><mo stretchy="true" form="prefix">‖</mo><mi>o</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">‖</mo></mrow><mrow><mo stretchy="true" form="prefix">‖</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">‖</mo></mrow></mfrac><mo>=</mo><mn>0</mn><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">\lim_{\delta x\to 0} \frac{\lVert o (\delta x) \rVert}{\lVert \delta x\rVert} = 0 \, ,</annotation></semantics></math> i.e.&nbsp;which goes to zero faster than linearly in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math>. This requires both the input <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math> and the output (the function) to have norms. This extension of differentiation to arbitrary normed/Banach spaces is sometimes called the <strong>Fréchet derivative</strong>.</p>
</section>
</section>
<section id="nonlinear-root-finding-optimization-and-adjoint-differentiation" class="level1">
<h1>Nonlinear Root-Finding, Optimization, and Adjoint Differentiation</h1>
<p>The next part is based on these <a href="https://docs.google.com/presentation/d/1U1lB5bhscjbxEuH5FcFwMl5xbHl0qIEkMf5rm0MO8uE/edit#slide=id.p">slides</a>. Today, we want to talk about why we are computing derivatives in the first place. In particular, we will drill down on this a little bit and then talk about computation of derivatives.</p>
<section id="sec:newton-roots" class="level2">
<h2 class="anchored" data-anchor-id="sec:newton-roots">Newton’s Method</h2>
<p>One common application of derivatives is to solve nonlinear equations via linearization.</p>
<section id="scalar-functions" class="level3">
<h3 class="anchored" data-anchor-id="scalar-functions">Scalar Functions</h3>
<p>For instance, suppose we have a scalar function <span class="math inline">$f: \R \to \R$</span> and we wanted to solve <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">f(x) = 0</annotation></semantics></math> for a root&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>. Of course, we could solve such an equation explicitly in simple cases, such as when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> is linear or quadratic, but if the function is something more arbitrary like <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>x</mi><mn>3</mn></msup><mo>−</mo><mo>sin</mo><mrow><mo stretchy="true" form="prefix">(</mo><mo>cos</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x) = x^3 - \sin (\cos x)</annotation></semantics></math> you might not be able to obtain closed-form solutions. However, there is a nice way to obtain the solution approximately to any accuracy you want, as long if you know approximately where the root is. The method we are talking about is known as <em>Newton’s method</em>, which is really a linear-algebra technique. It takes in the function and a guess for the root, approximates it by a straight line (whose root is easy to find), which is then an approximate root that we can use as a new guess. In particular, the method (depicted in Fig.&nbsp;<a href="#fig:newton-step" data-reference-type="ref" data-reference="fig:newton-step">4</a>) is as follows:</p>
<ul>
<li><p>Linearize <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math> near some <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> using the approximation <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>≈</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>δ</mi><mi>x</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">f(x + \delta x) \approx f(x) + f'(x) \delta x,</annotation></semantics></math></p></li>
<li><p>solve the linear equation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>δ</mi><mi>x</mi><mo>=</mo><mn>0</mn><mo>⟹</mo><mi>δ</mi><mi>x</mi><mo>=</mo><mi>−</mi><mfrac><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">f(x) + f'(x) \delta x = 0 \implies \delta x = -\frac{f(x)}{f'(x)}</annotation></semantics></math>,</p></li>
<li><p>and then use this to update the value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> we linearized near—i.e.,&nbsp;letting the new <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> be <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mtext mathvariant="normal">new</mtext></msub><mo>=</mo><mi>x</mi><mo>−</mo><mi>δ</mi><mi>x</mi><mo>=</mo><mi>x</mi><mo>+</mo><mfrac><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">x_\text{new} = x - \delta x = x + \frac{f(x)}{f'(x)} \, .</annotation></semantics></math></p></li>
</ul>
<p>Once you are close to the root, Newton’s method converges amazingly quickly. As discussed below, it asymptotically <em>doubles</em> the number of correct digits on every step!</p>
<p>One may ask what happens when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)</annotation></semantics></math> is not invertible, for instance here if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">f'(x) = 0</annotation></semantics></math>. If this happens, then Newton’s method may break down! See <a href="https://en.wikipedia.org/wiki/Newton%27s_method#Failure_analysis">here</a> for examples of when Newton’s method breaks down.</p>
<div id="fig:newton-step" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/newton-step.pdf.svg" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption>Single step of the scalar Newton’s method to solve <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">f(x)=0</annotation></semantics></math> for an example nonlinear function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>2</mn><mo>cos</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>x</mi><mo>+</mo><msup><mi>x</mi><mn>2</mn></msup><mi>/</mi><mn>10</mn></mrow><annotation encoding="application/x-tex">f(x) = 2\cos(x) - x + x^2/10</annotation></semantics></math>. Given a starting guess (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mn>2.3</mn></mrow><annotation encoding="application/x-tex">x = 2.3</annotation></semantics></math> in this example), we use <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)</annotation></semantics></math> to form a linear (affine) approximation of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>, and then our next step <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mrow><mi mathvariant="normal">n</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">w</mi></mrow></msub><annotation encoding="application/x-tex">x_\mathrm{new}</annotation></semantics></math> is the root of this approximation. As long as the initial guess is not too far from the root, Newton’s method converges extremely rapidly to the exact root (black dot).</figcaption>
</figure>
</div>
</section>
<section id="multidimensional-functions" class="level3">
<h3 class="anchored" data-anchor-id="multidimensional-functions">Multidimensional Functions</h3>
<p>We can generalize Newton’s method to multidimensional functions! Let <span class="math inline">$f: \R^n \to \R^n$</span> be a function which takes in a vector and spits out a vector of the same size&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>. We can then apply a Newton approach in higher dimensions:</p>
<ul>
<li><p>Linearize <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math> near some <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> using the first-derivative approximation <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>≈</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><munder><munder><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mo accent="true">⏟</mo></munder><mtext mathvariant="normal">Jacobian</mtext></munder><mi>δ</mi><mi>x</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">f(x + \delta x) \approx f(x) + \underbrace{f'(x)}_\text{Jacobian} \delta x,</annotation></semantics></math></p></li>
<li><p>solve the linear equation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>δ</mi><mi>x</mi><mo>=</mo><mn>0</mn><mo>⟹</mo><mi>δ</mi><mi>x</mi><mo>=</mo><mi>−</mi><munder><munder><mrow><mi>f</mi><mi>′</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><mo accent="true">⏟</mo></munder><mtext mathvariant="normal">inverse Jacobian</mtext></munder><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x) + f'(x) \delta x = 0 \implies \delta x = -\underbrace{f'(x)^{-1}}_\text{inverse Jacobian} f(x)</annotation></semantics></math>,</p></li>
<li><p>and then use this to update the value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> we linearized near—i.e.,&nbsp;letting the new <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> be <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mtext mathvariant="normal">new</mtext></msub><mo>=</mo><msub><mi>x</mi><mtext mathvariant="normal">old</mtext></msub><mo>−</mo><mi>f</mi><mi>′</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">x_\text{new} = x_\text{old} - f'(x)^{-1}f(x)\, .</annotation></semantics></math></p></li>
</ul>
<p>That’s it! Once we have the Jacobian, we can just solve a linear system on each step. This again converges amazingly fast, doubling the number of digits of accuracy in each step. (This is known as “quadratic convergence.”) However, there is a caveat: we <em>need</em> some starting guess for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>, and the guess needs to be sufficiently close to the root for the algorithm to make reliable progress. (If you start with an initial <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> far from a root, Newton’s method can fail to converge and/or it can jump around in intricate and surprising ways—google “Newton fractal” for some fascinating examples.) This is a widely used and very practical application of Jacobians and derivatives!</p>
</section>
</section>
<section id="optimization" class="level2">
<h2 class="anchored" data-anchor-id="optimization">Optimization</h2>
<section id="nonlinear-optimization" class="level3">
<h3 class="anchored" data-anchor-id="nonlinear-optimization">Nonlinear Optimization</h3>
<p>A perhaps even more famous application of large-scale differentiation is to nonlinear optimization. Suppose we have a scalar-valued function <span class="math inline">$f: \R^n \to \R$</span>, and suppose we want to minimize (or maximize) <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>. For instance, in machine learning, we could have a big neural network (NN) with a vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> of a million parameters, and one tries to minimize a “loss” function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> that compares the NN output to the desired results on “training” data. The most basic idea in optimization is to go “downhill” (see diagram) to make <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> as small as possible. If we can take the gradient of this function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>, to go “downhill” we consider <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>−</mi><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">- \nabla f</annotation></semantics></math>, the direction of <em>steepest descent</em>, as depicted in Fig.&nbsp;<a href="#fig:steepest-descent" data-reference-type="ref" data-reference="fig:steepest-descent">5</a>.</p>
<div id="fig:steepest-descent" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/steepest-descent.pdf.svg" class="img-fluid figure-img" style="width:70.0%"></p>
<figcaption>A <em>steepest-descent algorithm</em> minimizes a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math> by taking successive “downhill” steps in the direction <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>−</mi><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">-\nabla f</annotation></semantics></math>. (In the example shown here, we are minimizing a quadratic function in two dimensions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi>ℝ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">x \in \mathbb{R}^2</annotation></semantics></math>, performing an exact 1d minimization in the downhill direction for each step.) Steepest-descent algorithms can sometimes “zig-zag” along narrow valleys, slowing convergence (which can be counteracted in more sophisticated algorithms by “momentum” terms, second-derivative information, and so on).</figcaption>
</figure>
</div>
<p>Then, even if we have a million parameters, we can evolve all of them simultaneously in the downhill direction. It turns out that calculating all million derivatives costs about the same as evaluating the function at a point once (using reverse-mode/adjoint/left-to-right/backpropagation methods). Ultimately, this makes large-scale optimization practical for training neural nets, optimizing shapes of airplane wings, optimizing portfolios, etc.</p>
<p>Of course, there are many practical complications that make nonlinear optimization tricky (far more than can be covered in a single lecture, or even in a whole course!), but we give some examples here.</p>
<ul>
<li><p>For instance, even though we can compute the “downhill direction”, how far do we need to step in that direction? (In machine learning, this is sometimes called the “learning rate.”) Often, you want to take “as big of a step as you can” to speed convergence, but you don’t want the step to be too big because <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> only tells you a <em>local</em> approximation of&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>. There are many different ideas of how to determine this:</p>
<ul>
<li><p>Line search: using a 1D minimization to determine how far to step.</p></li>
<li><p>A “trust region” bounding the step size (where we trust the derivative-based approximation of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>). There are many techniques to evolve the size of the trust region as optimization progresses.</p></li>
</ul></li>
<li><p>We may also need to consider constraints, for instance minimizing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math> subject to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mi>k</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>≤</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">g_k(x) \leq 0</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>k</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">h_k(x)=0</annotation></semantics></math>, known as inequality/equality constraints. Points <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> satisfying the constraints are called “feasible”. One typically uses a combination of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><msub><mi>g</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\nabla g_k</annotation></semantics></math> to approximate (e.g.&nbsp;linearize) the problem and make progress towards the best feasible point.</p></li>
<li><p>If you just go straight downhill, you might “zig-zag” along narrow valleys, making convergence very slow. There are a few options to combat this, such as “momentum” terms and conjugate gradients. Even fancier than these techniques, one might estimate second-derivative “Hessian matrices” from a sequence of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> values—a famous version of this is known as the BFGS algorithm—and use the Hessian to take approximate Newton steps (for the root <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\nabla f = 0</annotation></semantics></math>). (We’ll return to Hessians in a later lecture.)</p></li>
<li><p>Ultimately, there are a lot of techniques and a zoo of competing algorithms that you might need to experiment with to find the best approach for a given problem. (There are many books on optimization algorithms, and even a whole book can only cover a small slice of what is out there!)</p></li>
</ul>
<p>Some parting advice: Often the main trick is less about the choice of algorithms than it is about finding the right mathematical formulation of your <em>problem</em>—e.g.&nbsp;what function, what constraints, and what parameters should you be considering—to match your problem to a good algorithm. However, if you have <em>many</em> (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≫</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">\gg 10</annotation></semantics></math>) parameters, <em>try hard</em> to use an analytical gradient (not finite differences), computed efficiently in reverse mode.</p>
</section>
<section id="engineeringphysical-optimization" class="level3">
<h3 class="anchored" data-anchor-id="engineeringphysical-optimization">Engineering/Physical Optimization</h3>
<p>There are many, many applications of optimization besides machine learning (fitting models to data). It is interesting to also consider engineering/physical optimization. (For instance, suppose you want to make an airplane wing that is as strong as possible.) The general outline of such problems is typically:</p>
<ol type="1">
<li><p>You start with some design parameters <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐩</mi><annotation encoding="application/x-tex">\mathbf{p}</annotation></semantics></math>, e.g.&nbsp;describing the geometry, materials, forces, or other degrees of freedom.</p></li>
<li><p>These <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐩</mi><annotation encoding="application/x-tex">\mathbf{p}</annotation></semantics></math> are then used in some physical model(s), such as solid mechanics, chemical reactions, heat transport, electromagnetism, acoustics, etc. For example, you might have a linear model of the form <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐩</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>x</mi><mo>=</mo><mi>b</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐩</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">A(\mathbf{p}) x = b(\mathbf{p})</annotation></semantics></math> for some matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> (typically very large and sparse).</p></li>
<li><p>The solution of the physical model is a solution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐩</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">x(\mathbf{p})</annotation></semantics></math>. For example, this could be the mechanical stresses, chemical concentrations, temperatures, electromagnetic fields, etc.</p></li>
<li><p>The physical solution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐩</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">x(\mathbf{p})</annotation></semantics></math> is the input into some design objective <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐩</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x(\mathbf{p}))</annotation></semantics></math> that you want to improve/optimize. For instance, strength, speed power, efficiency, etc.</p></li>
<li><p>To maximize/minimize <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>𝐩</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x(\mathbf{p}))</annotation></semantics></math>, one uses the gradient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><mi>𝐩</mi></msub><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla_{\mathbf{p}}f</annotation></semantics></math>, computed using reverse-mode/“adjoint” methods, to update the parameters <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐩</mi><annotation encoding="application/x-tex">\mathbf{p}</annotation></semantics></math> and improve the design.</p></li>
</ol>
<p>As a fun example, researchers have even applied “topology optimization” to design a chair, optimizing every voxel of the design—the parameters <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>𝐩</mi><annotation encoding="application/x-tex">\mathbf{p}</annotation></semantics></math> represent the material present (or not) in every voxel, so that the optimization discovers not just an optimal shape but an optimal <em>topology</em> (how materials are connected in space, how many holes there are, and so forth)—to support a given weight with <em>minimal material</em>. To see it in action, watch this <a href="https://www.youtube.com/watch?v=bJ_nSSBl040&amp;embeds_referring_euri=https%3A%2F%2Fdocs.google.com%2F&amp;embeds_referring_origin=https%3A%2F%2Fdocs.google.com&amp;source_ve_path=Mjg2NjY&amp;feature=emb_logo">chair-optimization video</a>. (People have applied such techniques to much more practical problems as well, from airplane wings to optical communications.)</p>
</section>
</section>
<section id="sec:adjoint-method" class="level2">
<h2 class="anchored" data-anchor-id="sec:adjoint-method">Reverse-mode “Adjoint” Differentiation</h2>
<p>But what is adjoint differentiation—the method of differentiating that makes these applications actually feasible to solve? Ultimately, it is yet another example of left-to-right/reverse-mode differentiation, essentially applying the chain rule from outputs to inputs. Consider, for example, trying to compute the gradient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>g</mi></mrow><annotation encoding="application/x-tex">\nabla g</annotation></semantics></math> of the scalar-valued function <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><munder><munder><mrow><mi>A</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>b</mi></mrow><mo accent="true">⏟</mo></munder><mi>x</mi></munder><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">g(p) = f(\underbrace{A(p)^{-1} b}_x) \, .</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> solves <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>x</mi><mo>=</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">A(p) x  = b</annotation></semantics></math> (e.g.&nbsp;a parameterized physical model as in the previous section) and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math> is a scalar-valued function of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> (e.g.&nbsp;an optimization objective depending on our physics solution). For example, this could arise in an optimization problem <span class="math display">$$\min_p g(p) \Longleftrightarrow \substack{  \text{ \normalsize  $\displaystyle \min_p f(x)$} \\ \text{ subject to } A(p)x = b }\; ,$$</span> for which the gradient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>g</mi></mrow><annotation encoding="application/x-tex">\nabla g</annotation></semantics></math> would be helpful to search for a local minimum. The chain rule for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math> corresponds to the following conceptual chain of dependencies: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mrow><mtext mathvariant="normal">change </mtext><mspace width="0.333em"></mspace></mrow><mrow><mi>d</mi><mi>g</mi></mrow><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> in </mtext><mspace width="0.333em"></mspace></mrow><mi>g</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>←</mo><mrow><mrow><mtext mathvariant="normal">change </mtext><mspace width="0.333em"></mspace></mrow><mrow><mi>d</mi><mi>x</mi></mrow><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> in </mtext><mspace width="0.333em"></mspace></mrow><mrow><mi>x</mi><mo>=</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>b</mi></mrow></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>←</mo><mrow><mrow><mtext mathvariant="normal">change </mtext><mspace width="0.333em"></mspace></mrow><mrow><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> in </mtext><mspace width="0.333em"></mspace></mrow><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>←</mo><mrow><mrow><mtext mathvariant="normal">change </mtext><mspace width="0.333em"></mspace></mrow><mrow><mi>d</mi><mi>A</mi></mrow><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> in </mtext><mspace width="0.333em"></mspace></mrow><mrow><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>←</mo><mrow><mrow><mtext mathvariant="normal">change </mtext><mspace width="0.333em"></mspace></mrow><mrow><mi>d</mi><mi>p</mi></mrow><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> in </mtext><mspace width="0.333em"></mspace></mrow><mi>p</mi></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
\text{change $dg$ in $g$} &amp;\longleftarrow 
\text{change $dx$ in $x = A^{-1} b$}  \\
&amp;\longleftarrow \text{change $d(A^{-1})$ in $A^{-1}$} \\
&amp;\longleftarrow 
\text{change $dA$ in $A(p)$}  \\
&amp;\longleftarrow \text{change $dp$ in $p$}
\end{aligned}</annotation></semantics></math> which is expressed by the equations: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>d</mi><mi>g</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow></mtd><mtd columnalign="right" style="text-align: right"><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> </mtext><mspace width="0.333em"></mspace></mrow></mtd><mtd columnalign="left" style="text-align: left"><mi>d</mi><mi>g</mi><mo>←</mo><mi>d</mi><mi>x</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>b</mi><mo stretchy="true" form="postfix">]</mo></mrow></mtd><mtd columnalign="right" style="text-align: right"><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> </mtext><mspace width="0.333em"></mspace></mrow></mtd><mtd columnalign="left" style="text-align: left"><mi>d</mi><mi>x</mi><mo>←</mo><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>−</mi><munder><munder><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><mo accent="true">⏟</mo></munder><msup><mi>v</mi><mi>T</mi></msup></munder><mi>d</mi><mi>A</mi><mspace width="0.167em"></mspace><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>b</mi></mtd><mtd columnalign="right" style="text-align: right"><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> </mtext><mspace width="0.333em"></mspace></mrow></mtd><mtd columnalign="left" style="text-align: left"><mi>d</mi><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo>←</mo><mi>d</mi><mi>A</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>−</mi><msup><mi>v</mi><mi>T</mi></msup><munder><munder><mrow><mi>A</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>p</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><mo accent="true">⏟</mo></munder><mrow><mi>d</mi><mi>A</mi></mrow></munder><mspace width="0.167em"></mspace><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>b</mi></mtd><mtd columnalign="right" style="text-align: right"><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> </mtext><mspace width="0.333em"></mspace></mrow></mtd><mtd columnalign="left" style="text-align: left"><mi>d</mi><mi>A</mi><mo>←</mo><mi>d</mi><mi>p</mi><mspace width="0.167em"></mspace><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
dg &amp;= f'(x) [dx] &amp;\text{ } &amp; dg \longleftarrow dx    \\
&amp; = f'(x) [d(A^{-1}) b] &amp;\text{ } &amp; dx \longleftarrow d(A^{-1})   \\
&amp; = - \underbrace{f'(x) A^{-1}}_{v^T} dA \, A^{-1} b &amp;\text{ } &amp; dA^{-1} \longleftarrow dA \\
&amp; = - v^T \underbrace{A'(p)[dp]}_{dA} \, A^{-1} b &amp;\text{ } &amp;dA \longleftarrow dp \, .
\end{aligned}</annotation></semantics></math> Here, we are defining the row vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>v</mi><mi>T</mi></msup><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">v^T = f'(x) A^{-1}</annotation></semantics></math>, and we have used the differential of a matrix inverse <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mspace width="0.167em"></mspace><mi>d</mi><mi>A</mi><mspace width="0.167em"></mspace><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">d(A^{-1})=-A^{-1}\,dA\,A^{-1}</annotation></semantics></math> from Sec.&nbsp;<a href="#sec:jacobian-inverse" data-reference-type="ref" data-reference="sec:jacobian-inverse">7.3</a>.</p>
<p>Grouping the terms left-to-right, we first solve the “adjoint” (transposed) equation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>A</mi><mi>T</mi></msup><mi>v</mi><mo>=</mo><mi>f</mi><mi>′</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mo>=</mo><msub><mi>∇</mi><mi>x</mi></msub><mi>f</mi></mrow><annotation encoding="application/x-tex">A^T v = f'(x)^T = \nabla_x f</annotation></semantics></math> for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math>, and then we obtain <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>g</mi><mo>=</mo><mi>−</mi><msup><mi>v</mi><mi>T</mi></msup><mi>d</mi><mi>A</mi><mspace width="0.167em"></mspace><mi>x</mi></mrow><annotation encoding="application/x-tex">dg = - v^T dA \, x</annotation></semantics></math>. Because the derivative <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">A'(p)</annotation></semantics></math> of a matrix with respect to a vector is awkward to write explicitly, it is convenient to examine this object one parameter at a time. For any given parameter <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>p</mi><mi>k</mi></msub><annotation encoding="application/x-tex">p_k</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∂</mi><mi>g</mi><mi>/</mi><mi>∂</mi><msub><mi>p</mi><mi>k</mi></msub><mo>=</mo><mi>−</mi><msup><mi>v</mi><mi>T</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>∂</mi><mi>A</mi><mi>/</mi><mi>∂</mi><msub><mi>p</mi><mi>k</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">\partial g/\partial p_k = -v^T (\partial A/\partial p_k) x</annotation></semantics></math> (and in many applications <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∂</mi><mi>A</mi><mi>/</mi><mi>∂</mi><msub><mi>p</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\partial A /\partial p_k</annotation></semantics></math> is very sparse); here, “dividing by” <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∂</mi><msub><mi>p</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\partial p_k</annotation></semantics></math> works because this is a scalar factor that commutes with the other linear operations. That is, it takes only <em>two solves</em> to get both <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>g</mi></mrow><annotation encoding="application/x-tex">\nabla g</annotation></semantics></math>: one for solving <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>x</mi><mo>=</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">A x = b</annotation></semantics></math> to find <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(p)=f(x)</annotation></semantics></math>, and another with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>A</mi><mi>T</mi></msup><annotation encoding="application/x-tex">A^T</annotation></semantics></math> for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math>, after which all of the derivatives <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∂</mi><mi>g</mi><mi>/</mi><mi>∂</mi><msub><mi>p</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\partial g/\partial p_k</annotation></semantics></math> are just some cheap dot products.</p>
<p>Note that you should <strong><em>not</em></strong> use right-to-left “forward-mode” derivatives with lots of parameters, because <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>∂</mi><mi>g</mi></mrow><mrow><mi>∂</mi><msub><mi>p</mi><mi>k</mi></msub></mrow></mfrac><mo>=</mo><mi>−</mi><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mfrac><mrow><mi>∂</mi><mi>A</mi></mrow><mrow><mi>∂</mi><msub><mi>p</mi><mi>k</mi></msub></mrow></mfrac><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\frac{\partial g}{\partial p_k} = - f'(x) \left(A^{-1} \frac{\partial A}{\partial p_k}x\right)</annotation></semantics></math> represents one solve per parameter <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>p</mi><mi>k</mi></msub><annotation encoding="application/x-tex">p_k</annotation></semantics></math>! As discussed in Sec.&nbsp;<a href="#sec:forward-vs-reverse" data-reference-type="ref" data-reference="sec:forward-vs-reverse">8.4</a>, right-to-left (a.k.a.&nbsp;forward mode) is better when there is one (or few) input parameters <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>p</mi><mi>k</mi></msub><annotation encoding="application/x-tex">p_k</annotation></semantics></math> and many outputs, while left-to-right “adjoint” differentiation (a.k.a.&nbsp;reverse mode) is better when there is one (or few) output values and many input parameters. (In Sec.&nbsp;<a href="#sec:dual-AD" data-reference-type="ref" data-reference="sec:dual-AD">8.1</a>, we will discuss using <a href="https://en.wikipedia.org/wiki/Dual_number">dual numbers</a> for differentiation, and this also corresponds to forward mode.)</p>
<p>Another possibility that might come to mind is to use finite differences (as in Sec.&nbsp;<a href="#sec:finitedifference" data-reference-type="ref" data-reference="sec:finitedifference">4</a>), but you should not use this if you have lots of parameters! Finite differences would involve a calculation of something like <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>∂</mi><mi>g</mi></mrow><mrow><mi>∂</mi><msub><mi>p</mi><mi>k</mi></msub></mrow></mfrac><mo>≈</mo><mrow><mo stretchy="true" form="prefix">[</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>+</mo><mi>ϵ</mi><msub><mi>e</mi><mi>k</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mi>/</mi><mi>ϵ</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">\frac{\partial g}{\partial p_k} \approx [g(p + \epsilon e_k) - g(p)]/\epsilon,</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>e</mi><mi>k</mi></msub><annotation encoding="application/x-tex">e_k</annotation></semantics></math> is a unit vector in the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>-th direction and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math> is a small number. This, however, requires one solve for each parameter <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>p</mi><mi>k</mi></msub><annotation encoding="application/x-tex">p_k</annotation></semantics></math>, just like forward-mode differentiation. (It becomes even more expensive if you use fancier higher-order finite-difference approximations in order to obtain higher accuracy.)</p>
<section id="nonlinear-equations" class="level3">
<h3 class="anchored" data-anchor-id="nonlinear-equations">Nonlinear equations</h3>
<p>You can also apply adjoint/reverse differentiation to nonlinear equations. For instance, consider the gradient of the scalar function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(p) = f(x(p))</annotation></semantics></math>, where <span class="math inline">$x(p)\in \R^n$</span> solves some system of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> equations <span class="math inline">$h(p,x) = 0 \in \R^n$</span>. By the chain rule, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn><mo>⟹</mo><mfrac><mrow><mi>∂</mi><mi>h</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mi>d</mi><mi>p</mi><mo>+</mo><mfrac><mrow><mi>∂</mi><mi>h</mi></mrow><mrow><mi>∂</mi><mi>x</mi></mrow></mfrac><mi>d</mi><mi>x</mi><mo>=</mo><mn>0</mn><mo>⟹</mo><mi>d</mi><mi>x</mi><mo>=</mo><mi>−</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>h</mi></mrow><mrow><mi>∂</mi><mi>x</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><mfrac><mrow><mi>∂</mi><mi>h</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mi>d</mi><mi>p</mi><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">h(p,x) = 0 \implies \frac{\partial h}{\partial p} dp + \frac{\partial  h}{\partial x} dx = 0 \implies dx = -\left(\frac{\partial  h}{\partial x}\right)^{-1}  \frac{\partial h}{\partial p}   dp \,.</annotation></semantics></math> (This is an instance of the <a href="https://en.wikipedia.org/wiki/Implicit_function_theorem">Implicit Function Theorem</a>: as long as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><mi>h</mi></mrow><mrow><mi>∂</mi><mi>x</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial  h}{\partial x}</annotation></semantics></math> is nonsingular, we can locally define a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">x(p)</annotation></semantics></math> from an implicit equation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">h=0</annotation></semantics></math>, here by linearization.) Hence, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>g</mi><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>x</mi><mo>=</mo><mi>−</mi><munder><munder><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>h</mi></mrow><mrow><mi>∂</mi><mi>x</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><mo accent="true">⏟</mo></munder><msup><mi>v</mi><mi>T</mi></msup></munder><mfrac><mrow><mi>∂</mi><mi>h</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mi>d</mi><mi>p</mi><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">dg = f'(x) dx = - \underbrace{ f'(x) \left(\frac{\partial  h}{\partial x}\right)^{-1} }_{v^T} \frac{\partial h}{\partial p} dp \, .</annotation></semantics></math> Associating left-to-right again leads to a single “adjoint” equation: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>∂</mi><mi>h</mi><mi>/</mi><mi>∂</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mi>v</mi><mo>=</mo><mi>f</mi><mi>′</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mo>=</mo><msub><mi>∇</mi><mi>x</mi></msub><mi>f</mi></mrow><annotation encoding="application/x-tex">(\partial h/\partial x)^T v = f'(x)^T = \nabla_x f</annotation></semantics></math>. In other words, it again only takes two solves to get both <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>g</mi></mrow><annotation encoding="application/x-tex">\nabla g</annotation></semantics></math>—one nonlinear “forward” solve for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> and one linear “adjoint” solve for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math>! Thereafter, all derivatives <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∂</mi><mi>g</mi><mi>/</mi><mi>∂</mi><msub><mi>p</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\partial g/\partial p_k</annotation></semantics></math> are cheap dot products. (Note that the linear “adjoint” solve involves the transposed Jacobian <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∂</mi><mi>h</mi><mi>/</mi><mi>∂</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\partial h/\partial x</annotation></semantics></math>. Except for the transpose, this is very similar to the cost of a single Newton step to solve <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">h=0</annotation></semantics></math> for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>. So the adjoint problem should be cheaper than the forward problem.)</p>
</section>
<section id="adjoint-methods-and-ad" class="level3">
<h3 class="anchored" data-anchor-id="adjoint-methods-and-ad">Adjoint methods and AD</h3>
<p>If you use automatic differentiation (AD) systems, why do you need to learn this stuff? Doesn’t the AD do everything for you? In practice, however, it is often helpful to understand adjoint methods even if you use automatic differentiation. Firstly, it helps you understand when to use forward- vs.&nbsp;reverse-mode automatic differentiation. Secondly, many physical models call large software packages written over the decades in various languages that <em>cannot be differentiated automatically</em> by AD. You can typically correct this by just supplying a “vector–Jacobian product” <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>y</mi><mi>T</mi></msup><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">y^T dx</annotation></semantics></math> for this physics, or even just <em>part</em> of the physics, and then AD will differentiate the rest and apply the chain rule for you. Lastly, often models involve approximate calculations (e.g.&nbsp;for the iterative solution of linear or nonlinear equations, numerical integration, and so forth), but AD tools often don’t “know” this and spend extra effort trying to differentiate the error in your approximation; in such cases, manually written derivative rules can sometimes be much more efficient. (For example, suppose your model involves solving a nonlinear system <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">h(x,p) = 0</annotation></semantics></math> by an iterative approach like Newton’s method. Naive AD will be very inefficient because it will attempt to differentiate through all your Newton steps. Assuming that you converge your Newton solver to enough accuracy that the error is negligible, it is much more efficient to perform differentiation via the implicit-function theorem as described above, leading to a single linear adjoint solve.)</p>
</section>
<section id="adjoint-method-example" class="level3">
<h3 class="anchored" data-anchor-id="adjoint-method-example">Adjoint-method example</h3>
<p>To finish off this section of the notes, we conclude with an example of how to use this “adjoint method” to compute a derivative efficiently. Before working through the example, we first state the problem and highly recommend trying it out before reading the solution.</p>
<div class="problem">
<p><span id="PSETexample" data-label="PSETexample"></span> Suppose that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">A(p)</annotation></semantics></math> takes a vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">p\in\mathbb{R}^{n-1}</annotation></semantics></math> and returns the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n\times n</annotation></semantics></math> tridiagonal real-symmetric matrix <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>a</mi><mn>1</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>p</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>p</mi><mn>1</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>a</mi><mn>2</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>p</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>p</mi><mn>2</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><mo>⋱</mo></mtd><mtd columnalign="center" style="text-align: center"><mo>⋱</mo></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"><mo>⋱</mo></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>a</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>p</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>p</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>a</mi><mi>n</mi></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">A(p)=\left(\begin{array}{ccccc}
a_{1} &amp; p_{1}\\
p_{1} &amp; a_{2} &amp; p_{2}\\
 &amp; p_{2} &amp; \ddots &amp; \ddots\\
 &amp;  &amp; \ddots &amp; a_{n-1} &amp; p_{n-1}\\
 &amp;  &amp;  &amp; p_{n-1} &amp; a_{n}
\end{array}\right),</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">a\in\mathbb{R}^{n}</annotation></semantics></math> is some constant vector. Now, define a scalar-valued function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(p)</annotation></semantics></math> by <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>c</mi><mi>T</mi></msup><mi>A</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>b</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">g(p)=\left(c^{T}A(p)^{-1}b\right)^{2}</annotation></semantics></math> for some constant vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mo>,</mo><mi>c</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">b,c\in\mathbb{R}^{n}</annotation></semantics></math> (assuming we choose <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math> so that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is invertible). Note that, in practice, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>b</mi></mrow><annotation encoding="application/x-tex">A(p)^{-1}b</annotation></semantics></math> is <em>not</em> computed by explicitly inverting the matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>—instead, it can be computed in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Theta(n)</annotation></semantics></math> (i.e., roughly proportional to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>) arithmetic operations using Gaussian elimination that takes advantage of the “sparsity” of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> (the pattern of zero entries), a “tridiagonal solve.”</p>
<ol type="1">
<li><p>Write down a formula for computing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∂</mi><mi>g</mi><mi>/</mi><mi>∂</mi><msub><mi>p</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\partial g/\partial p_{1}</annotation></semantics></math> (in terms of matrix–vector products and matrix inverses). (Hint: once you know <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>g</mi></mrow><annotation encoding="application/x-tex">dg</annotation></semantics></math> in terms of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">dA</annotation></semantics></math>, you can get <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∂</mi><mi>g</mi><mi>/</mi><mi>∂</mi><msub><mi>p</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\partial g/\partial p_{1}</annotation></semantics></math> by “dividing” both sides by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∂</mi><msub><mi>p</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\partial p_{1}</annotation></semantics></math>, so that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">dA</annotation></semantics></math> becomes <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∂</mi><mi>A</mi><mi>/</mi><mi>∂</mi><msub><mi>p</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\partial A/\partial p_{1}</annotation></semantics></math>.)</p></li>
<li><p>Outline a sequence of steps to compute both <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>g</mi></mrow><annotation encoding="application/x-tex">\nabla g</annotation></semantics></math> (with respect to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>) using only <em>two</em> tridiagonal solves <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>b</mi></mrow><annotation encoding="application/x-tex">x=A^{-1}b</annotation></semantics></math> and an “adjoint” solve <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>=</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mtext mathvariant="normal">(something)</mtext></mrow><annotation encoding="application/x-tex">v=A^{-1}\text{(something)}</annotation></semantics></math>, plus <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Theta(n)</annotation></semantics></math> (i.e., roughly proportional to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>) additional arithmetic operations.</p></li>
<li><p>Write a program implementing your <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>g</mi></mrow><annotation encoding="application/x-tex">\nabla g</annotation></semantics></math> procedure (in Julia, Python, Matlab, or any language you want) from the previous part. (You don’t need to use a fancy tridiagonal solve if you don’t know how to do this in your language; you can solve <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mtext mathvariant="normal">(vector)</mtext></mrow><annotation encoding="application/x-tex">A^{-1}\text{(vector)}</annotation></semantics></math> inefficiently if needed using your favorite matrix libraries.) Implement a finite-difference test: Choose <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>c</mi><mo>,</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">a,b,c,p</annotation></semantics></math> at random, and check that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>g</mi><mo>⋅</mo><mi>δ</mi><mi>p</mi><mo>≈</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>+</mo><mi>δ</mi><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\nabla g\cdot\delta p\approx g(p+\delta p)-g(p)</annotation></semantics></math> (to a few digits) for a randomly chosen small <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">\delta p</annotation></semantics></math>.</p></li>
</ol>
</div>
<p><strong><a href="#PSETexample" data-reference-type="ref+label" data-reference="PSETexample">[PSETexample]</a>(a) Solution:</strong> From the chain rule and the formula for the differential of a matrix inverse, we have <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>g</mi><mo>=</mo><mi>−</mi><mn>2</mn><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>c</mi><mi>T</mi></msup><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>b</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>c</mi><mi>T</mi></msup><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>d</mi><mi>A</mi><mspace width="0.167em"></mspace><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>b</mi></mrow><annotation encoding="application/x-tex">dg = -2(c^T A^{-1} b) c^T A^{-1} dA\,A^{-1} b</annotation></semantics></math> (noting that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>c</mi><mi>T</mi></msup><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>b</mi></mrow><annotation encoding="application/x-tex">c^T A^{-1} b</annotation></semantics></math> is a scalar so we can commute it as needed). Hence <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mfrac><mrow><mi>∂</mi><mi>g</mi></mrow><mrow><mi>∂</mi><msub><mi>p</mi><mn>1</mn></msub></mrow></mfrac></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><munder><munder><mrow><mi>−</mi><mn>2</mn><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>c</mi><mi>T</mi></msup><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>b</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>c</mi><mi>T</mi></msup><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><mo accent="true">⏟</mo></munder><msup><mi>v</mi><mi>T</mi></msup></munder><mfrac><mrow><mi>∂</mi><mi>A</mi></mrow><mrow><mi>∂</mi><msub><mi>p</mi><mn>1</mn></msub></mrow></mfrac><munder><munder><mrow><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>b</mi></mrow><mo accent="true">⏟</mo></munder><mi>x</mi></munder></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msup><mi>v</mi><mi>T</mi></msup><munder><munder><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mo>⋱</mo></mtd><mtd columnalign="center" style="text-align: center"><mo>⋱</mo></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"><mo>⋱</mo></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo accent="true">⏟</mo></munder><mfrac><mrow><mi>∂</mi><mi>A</mi></mrow><mrow><mi>∂</mi><msub><mi>p</mi><mn>1</mn></msub></mrow></mfrac></munder><mi>x</mi><mo>=</mo><menclose notation="box"><mrow><msub><mi>v</mi><mn>1</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><msub><mi>v</mi><mn>2</mn></msub><msub><mi>x</mi><mn>1</mn></msub></mrow></menclose><mspace width="0.167em"></mspace><mo>,</mo></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
\frac{\partial g}{\partial p_1} &amp;= \underbrace{-2(c^T A^{-1} b) c^T A^{-1}}_{v^T} \frac{\partial A}{\partial p_1} \underbrace{A^{-1} b}_x \\
&amp;=  v^T \underbrace{\left(\begin{array}{ccccc}
0 &amp; 1\\
1 &amp; 0 &amp; 0\\
 &amp; 0 &amp; \ddots &amp; \ddots\\
 &amp;  &amp; \ddots &amp; 0 &amp; 0\\
 &amp;  &amp;  &amp; 0 &amp; 0
\end{array}\right)}_{\frac{\partial A}{\partial p_1}} x = \boxed{v_1 x_2 + v_2 x_1} \, ,
\end{aligned}</annotation></semantics></math> where we have simplified the result in terms of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> for the next part.</p>
<p><strong><a href="#PSETexample" data-reference-type="ref+label" data-reference="PSETexample">[PSETexample]</a>(b) Solution:</strong> Using the notation from the previous part, exploiting the fact that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>A</mi><mi>T</mi></msup><mo>=</mo><mi>A</mi></mrow><annotation encoding="application/x-tex">A^T = A</annotation></semantics></math>, we can choose <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><menclose notation="box"><mrow><mi>v</mi><mo>=</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">[</mo><mi>−</mi><mn>2</mn><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>c</mi><mi>T</mi></msup><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>c</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow></menclose><annotation encoding="application/x-tex">\boxed{v = A^{-1} [-2(c^T x) c]}</annotation></semantics></math>, which is a single tridiagonal solve. Given <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math>, the results of our two <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Theta(n)</annotation></semantics></math> tridiagonal solves, we can compute each component of the gradient similar to above by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><menclose notation="box"><mrow><mi>∂</mi><mi>g</mi><mi>/</mi><mi>∂</mi><msub><mi>p</mi><mi>k</mi></msub><mo>=</mo><msub><mi>v</mi><mi>k</mi></msub><msub><mi>x</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>v</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub><msub><mi>x</mi><mi>k</mi></msub></mrow></menclose><annotation encoding="application/x-tex">\boxed{\partial g/\partial p_k = v_k x_{k+1} + v_{k+1} x_k}</annotation></semantics></math> for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>…</mi><mo>,</mo><mi>n</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k=1,\ldots,n-1</annotation></semantics></math>, which costs <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Theta(1)</annotation></semantics></math> arithmetic per <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> and hence <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Θ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Theta(n)</annotation></semantics></math> arithmetic to obtain all of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>g</mi></mrow><annotation encoding="application/x-tex">\nabla g</annotation></semantics></math>.</p>
<p><strong><a href="#PSETexample" data-reference-type="ref+label" data-reference="PSETexample">[PSETexample]</a>(c) Solution:</strong> See the <a href="https://nbviewer.org/github/mitmath/matrixcalc/blob/iap2023/psets/pset2sol.ipynb">Julia solution notebook (Problem&nbsp;1)</a> from our IAP 2023 course (which calls the function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> rather than <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>).</p>
</section>
</section>
</section>
<section id="derivative-of-matrix-determinant-and-inverse" class="level1">
<h1>Derivative of Matrix Determinant and Inverse</h1>
<section id="two-derivations" class="level2">
<h2 class="anchored" data-anchor-id="two-derivations">Two Derivations</h2>
<p>This section of notes follows <a href="https://rawcdn.githack.com/mitmath/matrixcalc/b08435612045b17745707f03900e4e4187a6f489/notes/determinant_and_inverse.html">this</a> Julia notebook. This notebook is a little bit short, but is an important and useful calculation.</p>
<div class="theorem">
<p>Given <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is a square matrix, we have <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>det</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mi mathvariant="normal">c</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mo>det</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>A</mi><mrow><mi>−</mi><mi>T</mi></mrow></msup><mo>:=</mo><mo>adj</mo><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>A</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>adj</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\nabla (\det A) = \mathrm{cofactor}(A) = (\det A)A^{-T} := \operatorname{adj}(A^T) = \operatorname{adj}(A)^T</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>adj</mo><annotation encoding="application/x-tex">\operatorname{adj}</annotation></semantics></math> is the “adjugate”. (You may not have heard of the matrix adjugate, but this formula tells us that it is simply <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>adj</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>det</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\operatorname{adj}(A) = \det(A) A^{-1}</annotation></semantics></math>, or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">c</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>adj</mo><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>A</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathrm{cofactor}(A) = \operatorname{adj}(A^T)</annotation></semantics></math>.) Furthermore, <span class="math display">$$d(\det A) = \tr(\det(A) A^{-1} dA) = \tr (\operatorname{adj}(A) dA) = \tr(\mathrm{cofactor}(A)^T dA).$$</span></p>
</div>
<p>You may remember that each entry <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math> of the cofactor matrix is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>−</mi><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>i</mi><mo>+</mo><mi>j</mi></mrow></msup><annotation encoding="application/x-tex">(-1)^{i + j}</annotation></semantics></math> times the determinant obtained by deleting row <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> and column <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>. Here are some <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2 \times 2</annotation></semantics></math> calculuations to obtain some intuition about these functions: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>M</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>a</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>c</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>b</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>d</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mo>⟹</mo><mrow><mi mathvariant="normal">c</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>M</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>d</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>−</mi><mi>c</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>−</mi><mi>b</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>a</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mo>adj</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>M</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>d</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>−</mi><mi>b</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>−</mi><mi>c</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>a</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>M</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mn>1</mn><mrow><mi>a</mi><mi>d</mi><mo>−</mo><mi>b</mi><mi>c</mi></mrow></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>d</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>−</mi><mi>b</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>−</mi><mi>c</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>a</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    M &amp;= \begin{pmatrix}
        a &amp; c \\ b &amp; d    
    \end{pmatrix} \\
    \implies \mathrm{cofactor}(M) &amp;= \begin{pmatrix}
         d &amp; -c \\ -b &amp; a
    \end{pmatrix}  \\
    \operatorname{adj}(M) &amp;= \begin{pmatrix}
d &amp; -b \\ -c &amp; a
\end{pmatrix} \\
(M)^{-1}  &amp;= \frac{1}{ad-bc} \begin{pmatrix}
    d &amp; -b \\ -c &amp; a
\end{pmatrix}.
\end{aligned}</annotation></semantics></math></p>
<p>Numerically, as is done in the notebook, you can construct a random <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \times n</annotation></semantics></math> matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> (say, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>9</mn><mo>×</mo><mn>9</mn></mrow><annotation encoding="application/x-tex">9 \times 9</annotation></semantics></math>), consider e.g.&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>A</mi><mo>=</mo><mn>.00001</mn><mi>A</mi></mrow><annotation encoding="application/x-tex">dA = .00001 A</annotation></semantics></math>, and see numerically that <span class="math display">$$\det(A + dA) - \det (A) \approx \tr(\operatorname{adj}(A) dA),$$</span> which numerically supports our claim for the theorem.</p>
<p>We now prove the theorem in two ways. Firstly, there is a direct proof where you just differentiate the scalar with respect to every input using the <a href="https://en.wikipedia.org/wiki/Laplace_expansion">cofactor expansion</a> of the determinant based on the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>-th row. Recall that <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>det</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>A</mi><mrow><mi>i</mi><mn>1</mn></mrow></msub><msub><mi>C</mi><mrow><mi>i</mi><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>A</mi><mrow><mi>i</mi><mn>2</mn></mrow></msub><msub><mi>C</mi><mrow><mi>i</mi><mn>2</mn></mrow></msub><mo>+</mo><mi>…</mi><mo>+</mo><msub><mi>A</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><msub><mi>C</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mi>.</mi></mrow><annotation encoding="application/x-tex">\det (A) = A_{i1} C_{i1} +A_{i2} C_{i2} + \dots + A_{in} C_{in}.</annotation></semantics></math> Thus, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>∂</mi><mo>det</mo><mi>A</mi></mrow><mrow><mi>∂</mi><msub><mi>A</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfrac><mo>=</mo><msub><mi>C</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>⟹</mo><mi>∇</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>det</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>C</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">\frac{\partial \det A}{ \partial A_{ij}} = C_{ij} \implies \nabla (\det A) = C,</annotation></semantics></math> the cofactor matrix. (In computing these partial derivatives, it’s important to remember that the cofactor <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>C</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">C_{ij}</annotation></semantics></math> contains no elements of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> from row&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> or column&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>. So, for example, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>A</mi><mrow><mi>i</mi><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">A_{i1}</annotation></semantics></math> only appears explicitly in the first term, and not hidden in any of the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math> terms in this expansion.)</p>
<p>There is also a fancier proof of the theorem using linearization near the identity. Firstly, note that it is easy to see from the properties of determinants that <span class="math display">$$\det(I + dA) - 1 = \tr(dA),$$</span> and thus <span class="math display">$$\begin{aligned}
    \det(A + A(A^{-1} dA)) - \det (A) &amp;= \det(A) (\det (I + A^{-1} dA) - 1) \\
    &amp;= \det(A) \tr(A^{-1} dA) = \tr(\det (A) A^{-1} dA) \\
    &amp;= \tr(\operatorname{adj}(A) dA).
\end{aligned}$$</span> This also implies the theorem.</p>
</section>
<section id="applications-1" class="level2">
<h2 class="anchored" data-anchor-id="applications-1">Applications</h2>
<section id="characteristic-polynomial" class="level3">
<h3 class="anchored" data-anchor-id="characteristic-polynomial">Characteristic Polynomial</h3>
<p>We now use this as an application to find the derivative of a characteristic polynomial evaluated at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>. Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>det</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mi>I</mi><mo>−</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(x) = \det(xI -A)</annotation></semantics></math>, a scalar function of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>. Recall that through factorization, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(x)</annotation></semantics></math> may be written in terms of eigenvalues <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>λ</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\lambda_i</annotation></semantics></math>. So we may ask: what is the derivative of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(x)</annotation></semantics></math>, the characteristic polynomial at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>? Using freshman calculus, we could simply compute <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>d</mi><mrow><mi>d</mi><mi>x</mi></mrow></mfrac><munder><mo>∏</mo><mi>i</mi></munder><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>−</mo><msub><mi>λ</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><munder><mo>∏</mo><mrow><mi>j</mi><mo>≠</mo><mi>i</mi></mrow></munder><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>−</mo><msub><mi>λ</mi><mi>j</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>∏</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>−</mo><msub><mi>λ</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="false" form="prefix">{</mo><munder><mo>∑</mo><mi>i</mi></munder><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>−</mo><msub><mi>λ</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="false" form="postfix">}</mo><mo>,</mo></mrow><annotation encoding="application/x-tex">\frac{d}{dx} \prod_i (x-\lambda_i) = \sum_i \prod_{j\neq i} (x-\lambda_j)  = \prod (x-\lambda_i) \{\sum_i (x- \lambda_i)^{-1}\},</annotation></semantics></math> as long as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>≠</mo><msub><mi>λ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x \neq \lambda_i</annotation></semantics></math>.</p>
<p>This is a perfectly good simply proof, but with our new technology we have a new proof: <span class="math display">$$\begin{aligned}
    d(\det (x I - A)) &amp;= \det(x I - A) \tr((xI - A)^{-1} d(x I - A)) \\
    &amp;= \det(xI - A) \tr(x I - A)^{-1} dx.
\end{aligned}$$</span> Note that here we used that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mi>I</mi><mo>−</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>d</mi><mi>x</mi><mspace width="0.167em"></mspace><mi>I</mi></mrow><annotation encoding="application/x-tex">d(x I - A) = dx \, I</annotation></semantics></math> when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is constant and <span class="math inline">$\tr (A dx) = \tr(A) dx$</span> since <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> is a scalar.</p>
<p>We may again check this computationally as we do in the notebook.</p>
</section>
<section id="the-logarithmic-derivative" class="level3">
<h3 class="anchored" data-anchor-id="the-logarithmic-derivative">The Logarithmic Derivative</h3>
<p>We can similarly compute using the chain rule that <span class="math display">$$d(\log (\det (A))) = \frac{d(\det A)}{ \det A} = \det (A^{-1}) d(\det (A)) = \tr(A^{-1} dA).$$</span> The logarithmic derivative shows up a lot in applied mathematics. Note that here we use that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mrow><mo>det</mo><mi>A</mi></mrow></mfrac><mo>=</mo><mo>det</mo><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\frac{1}{\det A} = \det(A^{-1})</annotation></semantics></math> as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>=</mo><mo>det</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>I</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>det</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>det</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>det</mo><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">1 = \det(I) = \det(AA^{-1}) = \det (A) \det(A^{-1}).</annotation></semantics></math></p>
<p>For instance, recall Newton’s method to find roots <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">f(x)=0</annotation></semantics></math> of single-variable real-valued functions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math> by taking a sequence of steps <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>→</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">x \to x + \delta x</annotation></semantics></math>. The key formula in Newton’s method is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi><mo>=</mo><mi>f</mi><mi>′</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\delta x = f'(x)^{-1}f(x)</annotation></semantics></math>, but this is the same as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mn>1</mn><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mo>log</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{1}{(\log f(x))'}</annotation></semantics></math>. So, derivatives of log determinants show up in finding <em>roots of determinants</em>, i.e.&nbsp;for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>det</mo><mi>M</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x) = \det M(x)</annotation></semantics></math>. When <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>A</mi><mo>−</mo><mi>x</mi><mi>I</mi></mrow><annotation encoding="application/x-tex">M(x) = A - x I</annotation></semantics></math>, roots of the determinant are eigenvalues of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>. For more general functions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">M(x)</annotation></semantics></math>, solving <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>det</mo><mi>M</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\det M(x) = 0</annotation></semantics></math> is therefore called a <em>nonlinear eigenproblem</em>.</p>
</section>
</section>
<section id="sec:jacobian-inverse" class="level2">
<h2 class="anchored" data-anchor-id="sec:jacobian-inverse">Jacobian of the Inverse</h2>
<p>Lastly, we compute the derivative (as both a linear operator and an explicit Jacobian matrix) of the inverse of a matrix. There is a neat trick to obtain this derivative, simply from the property <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>A</mi><mo>=</mo><mi>I</mi></mrow><annotation encoding="application/x-tex">A^{-1}A = I</annotation></semantics></math> of the inverse. By the product rule, this implies that <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>I</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn><mo>=</mo><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>A</mi><mo>+</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>d</mi><mi>A</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>⟹</mo><menclose notation="box"><mrow><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>A</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>−</mi><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mspace width="0.167em"></mspace><mi>d</mi><mi>A</mi><mspace width="0.167em"></mspace><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow></menclose><mspace width="0.167em"></mspace><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
d(A^{-1} A) &amp;= d(I) = 0 = d(A^{-1}) A + A^{-1} dA \\
&amp; \implies \boxed{d(A^{-1}) = (A^{-1})'[dA] = - A^{-1} \, dA \, A^{-1} }\, .
\end{aligned}</annotation></semantics></math> Here, the second line defines a perfectly good linear operator for the derivative <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi></mrow><annotation encoding="application/x-tex">(A^{-1})'</annotation></semantics></math>, but if we want we can rewrite this as an explicit Jacobian matrix by using Kronecker products acting on the “vectorized” matrices as we did in Sec.&nbsp;<a href="#sec:kronecker" data-reference-type="ref" data-reference="sec:kronecker">3</a>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>vec</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>vec</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>−</mi><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munder><munder><mrow><mi>−</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>A</mi><mrow><mi>−</mi><mi>T</mi></mrow></msup><mo>⊗</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mo accent="true">⏟</mo></munder><mrow><mi mathvariant="normal">J</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">n</mi></mrow></munder><mo>vec</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">\operatorname{vec}\left(d(A^{-1})\right) = \operatorname{vec}\left(-A^{-1} (dA) A^{-1}\right) = \underbrace{- (A^{-T} \otimes A^{-1})}_\mathrm{Jacobian} \operatorname{vec}(dA) \, ,</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>A</mi><mrow><mi>−</mi><mi>T</mi></mrow></msup><annotation encoding="application/x-tex">A^{-T}</annotation></semantics></math> denotes <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>A</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">(A^{-1})^T = (A^T)^{-1}</annotation></semantics></math>. One can check this formula numerically, as is done in the notebook.</p>
<p>In practice, however, you will probably find that the operator expression <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>−</mi><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mspace width="0.167em"></mspace><mi>d</mi><mi>A</mi><mspace width="0.167em"></mspace><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">- A^{-1} \, dA \, A^{-1}</annotation></semantics></math> is more useful than explicit Jacobian matrix for taking derivatives involving matrix inverses. For example, if you have a matrix-valued function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">A(t)</annotation></semantics></math> of a scalar parameter <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">t \in \mathbb{R}</annotation></semantics></math>, you immediately obtain <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>d</mi><mi>t</mi></mrow></mfrac><mo>=</mo><mi>−</mi><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mfrac><mrow><mi>d</mi><mi>A</mi></mrow><mrow><mi>d</mi><mi>t</mi></mrow></mfrac><msup><mi>A</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\frac{d(A^{-1})}{dt} = -A^{-1} \frac{dA}{dt} A^{-1}</annotation></semantics></math>. A more sophisticated application is discussed in Sec.&nbsp;<a href="#sec:adjoint-method" data-reference-type="ref" data-reference="sec:adjoint-method">6.3</a>.</p>
</section>
</section>
<section id="sec:AD" class="level1">
<h1>Forward and Reverse-Mode Automatic Differentiation</h1>
<p>The first time that Professor Edelman had heard about automatic differentiation (AD), it was easy for him to imagine what it was …but what he imagined was wrong! In his head, he thought it was straightforward symbolic differentiation applied to code—sort of like executing Mathematica or Maple, or even just automatically doing what he learned to do in his calculus class. For instance, just plugging in functions and their domains from something like the following first-year calculus table:</p>
<div class="center">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">Derivative</th>
<th style="text-align: left;">Domain</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mo>sin</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mo>=</mo><mo>cos</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">(\sin x)' = \cos x</annotation></semantics></math></td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>−</mi><mi>∞</mi><mo>&lt;</mo><mi>x</mi><mo>&lt;</mo><mi>∞</mi></mrow><annotation encoding="application/x-tex">- \infty&lt; x &lt; \infty</annotation></semantics></math></td>
</tr>
<tr class="even">
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mo>cos</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mo>=</mo><mi>−</mi><mo>sin</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">(\cos x)' = - \sin x</annotation></semantics></math></td>
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>−</mi><mi>∞</mi><mo>&lt;</mo><mi>x</mi><mo>&lt;</mo><mi>∞</mi></mrow><annotation encoding="application/x-tex">-\infty&lt; x &lt; \infty</annotation></semantics></math></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mo>tan</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mo>=</mo><msup><mo>sec</mo><mn>2</mn></msup><mi>x</mi></mrow><annotation encoding="application/x-tex">(\tan x)' = \sec^2 x</annotation></semantics></math></td>
<td style="text-align: left;"><span class="math inline">$x\neq \frac{\pi}{2} + \pi n, n \in \ZZ$</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mo>cot</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mo>=</mo><mi>−</mi><msup><mo>csc</mo><mn>2</mn></msup><mi>x</mi></mrow><annotation encoding="application/x-tex">(\cot x)' = - \csc^2 x</annotation></semantics></math></td>
<td style="text-align: left;"><span class="math inline">$x\neq \pi n, n \in \ZZ$</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mo>sec</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mo>=</mo><mo>tan</mo><mi>x</mi><mo>sec</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">(\sec x)' = \tan x \sec x</annotation></semantics></math></td>
<td style="text-align: left;"><span class="math inline">$x \neq \frac{\pi}{2} + \pi n, n\in \ZZ$</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mo>csc</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mo>=</mo><mi>−</mi><mo>cot</mo><mi>x</mi><mo>csc</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">(\csc x)' = - \cot x \csc x</annotation></semantics></math></td>
<td style="text-align: left;"><span class="math inline">$x\neq \pi n, n \in \ZZ$</span></td>
</tr>
</tbody>
</table>
</div>
<p>And in any case, if it wasn’t just like executing Mathematica or Maple, then it must be finite differences, like one learns in a numerical computing class (or as we did in Sec.&nbsp;<a href="#sec:finitedifference" data-reference-type="ref" data-reference="sec:finitedifference">4</a>).</p>
<p>It turns out that it is definitely <em>not</em> finite differences—AD algorithms are generally exact (in exact arithmetic, neglecting roundoff errors), not approximate. But it also doesn’t look much like conventional symbolic algebra: the computer doesn’t really construct a big “unrolled” symbolic expression and then differentiate it, the way you might imagine doing by hand or via computer-algebra software. For example, imagine a computer program that computes <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>det</mo><mi>A</mi></mrow><annotation encoding="application/x-tex">\det A</annotation></semantics></math> for an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \times n</annotation></semantics></math> matrix—writing down the “whole” symbolic expression isn’t possible until the program runs and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> is known (e.g.&nbsp;input by the user), and in any case a naive symbolic expression would require <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mi>!</mi></mrow><annotation encoding="application/x-tex">n!</annotation></semantics></math> terms. Thus, AD systems have to deal with computer-programming constructs like loops, recursion, and problem sizes <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> that are unknown until the program runs, while at the same time avoiding constructing symbolic expressions whose size becomes prohibitively large. (See Sec.&nbsp;<a href="#sec:babylonian" data-reference-type="ref" data-reference="sec:babylonian">8.1.1</a> for an example that looks very different from the formulas you differentiate in first-year calculus.) Design of AD systems often ends up being more about compilers than about calculus!</p>
<section id="sec:dual-AD" class="level2">
<h2 class="anchored" data-anchor-id="sec:dual-AD">Automatic Differentiation via Dual Numbers</h2>
<p><em>(This lecture is accompanied by a Julia “notebook” showing the results of various computational experiments, which can be found on the course web page. Excerpts from those experiments are included below.)</em></p>
<p>One AD approach that can be explained relatively simply is “forward-mode” AD, which is implemented by carrying out the computation of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math> in <em>tandem</em> with the computation of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>. One augments every intermediate value&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math> in the computer program with another value&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math> that represents its derivative, along with chain rules to propagate these derivatives through computations on values in the program. It turns out that this can be thought of as replacing real numbers (values&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>) with a new kind of “dual number” <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">D(a,b)</annotation></semantics></math> (values &amp; derivatives) and corresponding arithmetic rules, as explained below.</p>
<section id="sec:babylonian" class="level3">
<h3 class="anchored" data-anchor-id="sec:babylonian">Example: Babylonian square root</h3>
<p>We start with a simple example, an algorithm for the square-root function, where a practical method of automatic differentiation came as both a mathematical surprise and a computing wonder for Professor Edelman. In particular, we consider the “Babylonian” algorithm to compute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msqrt><mi>x</mi></msqrt><annotation encoding="application/x-tex">\sqrt{x}</annotation></semantics></math>, known for millennia (and later revealed as a special case of Newton’s method applied to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>t</mi><mn>2</mn></msup><mo>−</mo><mi>x</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">t^2 - x = 0</annotation></semantics></math>): simply repeat <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>←</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo>+</mo><mi>x</mi><mi>/</mi><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">t \leftarrow (t + x/t)/2</annotation></semantics></math> until <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> converges to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msqrt><mi>x</mi></msqrt><annotation encoding="application/x-tex">\sqrt{x}</annotation></semantics></math> to any desired accuracy. Each iteration has one addition and two divisions. For illustration purposes, 10 iterations suffice. Here is a short program in Julia that implements this algorithm, starting with a guess of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math> and then performing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> steps (defaulting to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">N=10</annotation></semantics></math>):</p>
<pre class="jlcon"><code>julia&gt; function Babylonian(x; N = 10) 
           t = (1+x)/2   # one step from t=1
           for i = 2:N   # remaining N-1 steps
               t = (t + x/t) / 2
           end    
           return t
       end</code></pre>
<p>If we run this function to compute the square root of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">x=4</annotation></semantics></math>, we will see that it converges very quickly: for only <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">N=3</annotation></semantics></math> steps, it obtains the correct answer&nbsp;(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>2</mn><annotation encoding="application/x-tex">2</annotation></semantics></math>) to nearly&nbsp;3 decimal places, and well before <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">N=10</annotation></semantics></math> steps it has converged to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>2</mn><annotation encoding="application/x-tex">2</annotation></semantics></math> within the limits of the accuracy of computer arithmetic (about 16 digits). In fact, it roughly doubles the number of correct digits on every step:</p>
<pre class="jlcon"><code>julia&gt; Babylonian(4, N=1)
2.5

julia&gt; Babylonian(4, N=2)
2.05

julia&gt; Babylonian(4, N=3)
2.000609756097561

julia&gt; Babylonian(4, N=4)
2.0000000929222947

julia&gt; Babylonian(4, N=10)
2.0</code></pre>
<p>Of course, any first-year calculus student knows the derivative of the square root, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><msqrt><mi>x</mi></msqrt><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mo>=</mo><mn>0.5</mn><mi>/</mi><msqrt><mi>x</mi></msqrt></mrow><annotation encoding="application/x-tex">(\sqrt{x})' = 0.5/\sqrt{x}</annotation></semantics></math>, which we could compute here via <code>0.5 / Babylonian(x)</code>, but we want to know how we can obtain this derivative <em>automatically</em>, directly from the Babylonian algorithm itself. If we can figure out how to easily and efficiently pass the chain rule through this algorithm, then we will begin to understand how AD can also differentiate much more complicated computer programs for which no simple derivative formula is known.</p>
</section>
<section id="easy-forward-mode-ad" class="level3">
<h3 class="anchored" data-anchor-id="easy-forward-mode-ad">Easy forward-mode AD</h3>
<p>The basic idea of carrying the chain rule through a computer program is very simple: replace every number with <em>two</em> numbers, one which keeps track of the <em>value</em> and one which tracks the <em>derivative</em> of that value. The values are computed the same way as before, and the derivatives are computed by carrying out the chain rule for elementary operations like <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>+</mi><annotation encoding="application/x-tex">+</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>/</mi><annotation encoding="application/x-tex">/</annotation></semantics></math>.</p>
<p>In Julia, we can implement this idea by defining a new type of number, which we’ll call <code>D</code>, that encapsulates a value <code>val</code> and a derivative <code>deriv</code>.</p>
<pre class="jlcon"><code>julia&gt; struct D &lt;: Number
           val::Float64
           deriv::Float64
       end</code></pre>
<p>(A detailed explanation of Julia syntax can be found <a href="https://julialang.org/learning/">elsewhere</a>, but hopefully you can follow the basic ideas even if you don’t understand every punctuation mark.) A quantity <code>x = D(a,b)</code> of this new type has two components <code>x.val = a</code> and <code>x.deriv = b</code>, which we will use to represent values and derivatives, respectively. The <code>Babylonian</code> code only uses two arithmetic operations, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>+</mi><annotation encoding="application/x-tex">+</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>/</mi><annotation encoding="application/x-tex">/</annotation></semantics></math>, so we just need to overload the built-in (“<code>Base</code>”) definitions of these in Julia to include new rules for our <code>D</code> type:</p>
<pre class="jlcon"><code>julia&gt; Base.:+(x::D, y::D) = D(x.val+y.val, x.deriv+y.deriv)
       Base.:/(x::D, y::D) = D(x.val/y.val, (y.val*x.deriv - x.val*y.deriv)/y.val^2)</code></pre>
<p>If you look closely, you’ll see that the values are just added and divided in the ordinary way, while the derivatives are computed using the sum rule (adding the derivatives of the inputs) and the quotient rule, respectively. We also need one other technical trick: we need to define <a href="https://docs.julialang.org/en/v1/manual/conversion-and-promotion/">“conversion” and “promotion” rules</a> that tell Julia how to combine <code>D</code> values with ordinary real numbers, as in expressions like <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">x+1</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">x/2</annotation></semantics></math>:</p>
<pre class="jlcon"><code>julia&gt; Base.convert(::Type{D}, r::Real) = D(r,0)
       Base.promote_rule(::Type{D}, ::Type{&lt;:Real}) = D</code></pre>
<p>This just says that an ordinary real number <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math> is combined with a <code>D</code> value by first converting <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math> to <code>D(r,0)</code>: the value is&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math> and the derivative is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0</mn><annotation encoding="application/x-tex">0</annotation></semantics></math> (the derivative of any constant is zero).</p>
<p>Given these definitions, we can now plug a <code>D</code> value into our <em>unmodified</em> <code>Babylonian</code> function, and it will “magically” compute the derivative of the square root. Let’s try it for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mn>49</mn><mo>=</mo><msup><mn>7</mn><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">x = 49 = 7^2</annotation></semantics></math>:</p>
<pre class="jlcon"><code>julia&gt; x = 49
49

julia&gt; Babylonian(D(x,1))
D(7.0, 0.07142857142857142)</code></pre>
<p>We can see that it correctly returned a value of <code>7.0</code> and a derivative of <code>0.07142857142857142</code>, which indeed matches the square root <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msqrt><mn>49</mn></msqrt><annotation encoding="application/x-tex">\sqrt{49}</annotation></semantics></math> and its derivative <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.5</mn><mi>/</mi><msqrt><mn>49</mn></msqrt></mrow><annotation encoding="application/x-tex">0.5/\sqrt{49}</annotation></semantics></math>:</p>
<pre class="jlcon"><code>julia&gt; (√x, 0.5/√x)
(7.0, 0.07142857142857142)</code></pre>
<p>Why did we input <code>D(x,1)</code>? Where did the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math> come from? That’s simply the fact that the derivative of the input <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> with respect to <em>itself</em> is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">(x)' = 1</annotation></semantics></math>, so this is the starting point for the chain rule.</p>
<p>In practice, all this (and more) has already been implemented in the <a href="https://github.com/JuliaDiff/ForwardDiff.jl">ForwardDiff.jl</a> package in Julia (and in many similar software packages in a variety of languages). That package hides the implementation details under the hood and explicitly provides a function to compute the derivative. For example:</p>
<pre class="jlcon"><code>julia&gt; using ForwardDiff

julia&gt; ForwardDiff.derivative(Babylonian, 49)
0.07142857142857142</code></pre>
<p>Essentially, however, this is the same as our little <code>D</code> implementation, but implemented with greater generality and sophistication (e.g.&nbsp;chain rules for more operations, support for more numeric types, partial derivatives with respect to multiple variables, etc.): just as we did, ForwardDiff augments every value with a second number that tracks the derivative, and propagates both quantities through the calculation.</p>
<p>We could have also implemented the same idea specifically for the Bablylonian algorithm, by writing a new function <code>dBabylonian</code> that tracks both the variable <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> and its derivative <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>′</mi><mo>=</mo><mi>d</mi><mi>t</mi><mi>/</mi><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">t' = dt/dx</annotation></semantics></math> through the course of the calculation:</p>
<pre class="jlcon"><code>julia&gt; function dBabylonian(x; N = 10) 
           t = (1+x)/2
           t′ = 1/2
           for i = 1:N
               t = (t+x/t)/2
               t′= (t′+(t-x*t′)/t^2)/2
           end    
           return t′
       end

julia&gt; dBabylonian(49)
0.07142857142857142</code></pre>
<p>This is doing <em>exactly</em> the same calculations as calling <code>Babylonian(D(x,1))</code> or <code>ForwardDiff.derivative(Babylonian, 49)</code>, but needs a lot more human effort—we’d have to do this for every computer program we write, rather than implementing a new number type <em>once</em>.</p>
</section>
<section id="dual-numbers" class="level3">
<h3 class="anchored" data-anchor-id="dual-numbers">Dual numbers</h3>
<p>There is a pleasing algebraic way to think about our new number type <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">D(a,b)</annotation></semantics></math> instead of the “value &amp; derivative” viewpoint above. Remember how a complex number <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>+</mo><mi>b</mi><mi>i</mi></mrow><annotation encoding="application/x-tex">a + bi</annotation></semantics></math> is formed from two real numbers <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(a,b)</annotation></semantics></math> by defining a special new quantity <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> (the imaginary unit) that satisfies <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>i</mi><mn>2</mn></msup><mo>=</mo><mi>−</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">i^2 = -1</annotation></semantics></math>, and all the other complex-arithmetic rules follow from this? Similarly, we can think of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">D(a,b)</annotation></semantics></math> as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>+</mo><mi>b</mi><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">a + b \epsilon</annotation></semantics></math>, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math> is a new “infinitesimal unit” quantity that satisfies <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ϵ</mi><mn>2</mn></msup><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\epsilon^2 = 0</annotation></semantics></math>. This viewpoint is called a <strong>dual number</strong>.</p>
<p>Given the elementary rule <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ϵ</mi><mn>2</mn></msup><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\epsilon^2 = 0</annotation></semantics></math>, the other algebraic rules for dual numbers immediately follow: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mrow><mo stretchy="true" form="prefix">(</mo><mi>a</mi><mo>+</mo><mi>b</mi><mi>ϵ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>±</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>c</mi><mo>+</mo><mi>d</mi><mi>ϵ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>a</mi><mo>±</mo><mi>c</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>b</mi><mo>±</mo><mi>d</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>ϵ</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mrow><mo stretchy="true" form="prefix">(</mo><mi>a</mi><mo>+</mo><mi>b</mi><mi>ϵ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>⋅</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>c</mi><mo>+</mo><mi>d</mi><mi>ϵ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>a</mi><mi>c</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>b</mi><mi>c</mi><mo>+</mo><mi>a</mi><mi>d</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>ϵ</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mfrac><mrow><mi>a</mi><mo>+</mo><mi>b</mi><mi>ϵ</mi></mrow><mrow><mi>c</mi><mo>+</mo><mi>d</mi><mi>ϵ</mi></mrow></mfrac></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mrow><mi>a</mi><mo>+</mo><mi>b</mi><mi>ϵ</mi></mrow><mrow><mi>c</mi><mo>+</mo><mi>d</mi><mi>ϵ</mi></mrow></mfrac><mo>⋅</mo><mfrac><mrow><mi>c</mi><mo>−</mo><mi>d</mi><mi>ϵ</mi></mrow><mrow><mi>c</mi><mo>−</mo><mi>d</mi><mi>ϵ</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>a</mi><mo>+</mo><mi>b</mi><mi>ϵ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>c</mi><mo>−</mo><mi>d</mi><mi>ϵ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><msup><mi>c</mi><mn>2</mn></msup></mfrac><mo>=</mo><mfrac><mi>a</mi><mi>c</mi></mfrac><mo>+</mo><mfrac><mrow><mi>b</mi><mi>c</mi><mo>−</mo><mi>a</mi><mi>d</mi></mrow><msup><mi>c</mi><mn>2</mn></msup></mfrac><mi>ϵ</mi><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    (a + b \epsilon) \pm (c + d \epsilon) &amp;= (a \pm c) + (b \pm d) \epsilon \\
    (a + b \epsilon) \cdot (c + d\epsilon) &amp;= (ac) + (bc + ad) \epsilon \\
    \frac{a + b \epsilon}{c + d \epsilon} &amp;= \frac{a + b \epsilon}{c + d \epsilon} \cdot \frac{c - d \epsilon}{c - d \epsilon} = \frac{(a + b \epsilon)(c - d \epsilon)}{c^2}  =     
    \frac{a}{c} + \frac{bc - ad}{c^2}\epsilon.
\end{aligned}</annotation></semantics></math> The <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math> coefficients of these rules correspond to the sum/difference, product, and quotient rules of differential calculus!</p>
<p>In fact, these are <em>exactly</em> the rules we implemented above for our <code>D</code> type. We were only missing the rules for subtraction and multiplication, which we can now include:</p>
<pre class="jlcon"><code>julia&gt; Base.:-(x::D, y::D) = D(x.val - y.val, x.deriv - y.deriv)
       Base.:*(x::D, y::D) = D(x.val*y.val, x.deriv*y.val + x.val*y.deriv)</code></pre>
<p>It’s also nice to add a <a href="https://docs.julialang.org/en/v1/manual/types/#man-custom-pretty-printing">“pretty printing”</a> rule to make Julia display dual numbers as <code>a + bϵ</code> rather than as <code>D(a,b)</code>:</p>
<pre class="jlcon"><code>julia&gt; Base.show(io::IO, x::D) = print(io, x.val, " + ", x.deriv, "ϵ")</code></pre>
<p>Once we implement the multiplication rule for dual numbers in Julia, then <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ϵ</mi><mn>2</mn></msup><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\epsilon^2 = 0</annotation></semantics></math> follows from the special case <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>=</mo><mi>c</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">a = c=0</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mo>=</mo><mi>d</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">b=d=1</annotation></semantics></math>:</p>
<pre class="jlcon"><code>julia&gt; ϵ = D(0,1)
0.0 + 1.0ϵ

julia&gt; ϵ * ϵ 
0.0 + 0.0ϵ

julia&gt; ϵ^2
0.0 + 0.0ϵ</code></pre>
<p>(We didn’t define a rule for powers <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">D(a,b)^n</annotation></semantics></math>, so how did it compute <code>ϵ</code><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi></mi><mn>2</mn></msup><annotation encoding="application/x-tex">^2</annotation></semantics></math>? The answer is that Julia implements <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>x</mi><mi>n</mi></msup><annotation encoding="application/x-tex">x^n</annotation></semantics></math> via repeated multiplication by default, so it sufficed to define the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>*</mi><annotation encoding="application/x-tex">*</annotation></semantics></math> rule.) Now, we can compute the derivative of the Babylonian algorithm at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mn>49</mn></mrow><annotation encoding="application/x-tex">x = 49</annotation></semantics></math> as above by:</p>
<pre class="jlcon"><code>julia&gt; Babylonian(x + ϵ)
7.0 + 0.07142857142857142ϵ</code></pre>
<p>with the “infinitesimal part” being the derivative <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.5</mn><mi>/</mi><msqrt><mn>49</mn></msqrt><mo>=</mo><mn>0.0714</mn><mi>⋯</mi></mrow><annotation encoding="application/x-tex">0.5/\sqrt{49} = 0.0714\cdots</annotation></semantics></math>.</p>
<p>A nice thing about this dual-number viewpoint is that it corresponds directly to our notion of a derivative as linearization: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>ϵ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>ϵ</mi><mo>+</mo><mtext mathvariant="normal">(higher-order terms)</mtext><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">f(x + \epsilon) = f(x) + f'(x) \epsilon + \mbox{(higher-order terms)} \, ,</annotation></semantics></math> with the dual-number rule <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ϵ</mi><mn>2</mn></msup><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\epsilon^2 = 0</annotation></semantics></math> corresponding to dropping the higher-order terms.</p>
</section>
</section>
<section id="naive-symbolic-differentiation" class="level2">
<h2 class="anchored" data-anchor-id="naive-symbolic-differentiation">Naive symbolic differentiation</h2>
<p>Forward-mode AD implements the exact analytical derivative by propagating chain rules, but it is completely different from what many people <em>imagine</em> AD might be: evaluating a program <em>symbolically</em> to obtain a giant symbolic expression, and <em>then</em> differentiating this giant expression to obtain the derivative. A basic issue with this approach is that the size of these symbolic expressions can quickly explode as the program runs. Let’s see what it would look like for the Babylonian algorithm.</p>
<p>Imagine inputting a “symbolic variable” <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> into our <code>Babylonian</code> code, running the algorithm, and writing a big algebraic expression for the result. After only one step, for example, we would get <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">(x + 1)/2</annotation></semantics></math>. After two steps, we would get <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mn>2</mn><mo>+</mo><mn>2</mn><mi>x</mi><mi>/</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">((x+1)/2 + 2x/(x+1))/2</annotation></semantics></math>, which simplifies to a ratio of two polynomials (a “rational function”): <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mn>6</mn><mi>x</mi><mo>+</mo><mn>1</mn></mrow><mrow><mn>4</mn><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">\frac{x^2 + 6x + 1}{4(x+1)} \, .</annotation></semantics></math> Continuing this process by hand is quite tedious, but fortunately the computer can do it for us (as shown in the accompanying Julia notebook). Three Babylonian iterations yields: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><msup><mi>x</mi><mn>4</mn></msup><mo>+</mo><mn>28</mn><msup><mi>x</mi><mn>3</mn></msup><mo>+</mo><mn>70</mn><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mn>28</mn><mi>x</mi><mo>+</mo><mn>1</mn></mrow><mrow><mn>8</mn><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>x</mi><mn>3</mn></msup><mo>+</mo><mn>7</mn><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mn>7</mn><mi>x</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">\frac{x^{4} + 28 x^{3} + 70 x^{2} + 28 x + 1}{8 \left(x^{3} + 7 x^{2} + 7 x + 1\right)} \, ,</annotation></semantics></math> four iterations gives <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><msup><mi>x</mi><mn>8</mn></msup><mo>+</mo><mn>120</mn><msup><mi>x</mi><mn>7</mn></msup><mo>+</mo><mn>1820</mn><msup><mi>x</mi><mn>6</mn></msup><mo>+</mo><mn>8008</mn><msup><mi>x</mi><mn>5</mn></msup><mo>+</mo><mn>12870</mn><msup><mi>x</mi><mn>4</mn></msup><mo>+</mo><mn>8008</mn><msup><mi>x</mi><mn>3</mn></msup><mo>+</mo><mn>1820</mn><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mn>120</mn><mi>x</mi><mo>+</mo><mn>1</mn></mrow><mrow><mn>16</mn><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>x</mi><mn>7</mn></msup><mo>+</mo><mn>35</mn><msup><mi>x</mi><mn>6</mn></msup><mo>+</mo><mn>273</mn><msup><mi>x</mi><mn>5</mn></msup><mo>+</mo><mn>715</mn><msup><mi>x</mi><mn>4</mn></msup><mo>+</mo><mn>715</mn><msup><mi>x</mi><mn>3</mn></msup><mo>+</mo><mn>273</mn><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mn>35</mn><mi>x</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">\frac{x^{8} + 120 x^{7} + 1820 x^{6} + 8008 x^{5} + 12870 x^{4} + 8008 x^{3} + 1820 x^{2} + 120 x + 1}{16 \left(x^{7} + 35 x^{6} + 273 x^{5} + 715 x^{4} + 715 x^{3} + 273 x^{2} + 35 x + 1\right)} \, ,</annotation></semantics></math> and five iterations produces the enormous expression: <span class="math display">$$\resizebox{1.0\hsize}{!}{$\frac{x^{16} + 496 x^{15} + 35960 x^{14} + 906192 x^{13} + 10518300 x^{12} + 64512240 x^{11} + 225792840 x^{10} + 471435600 x^{9} + 601080390 x^{8} + 471435600 x^{7} + 225792840 x^{6} + 64512240 x^{5} + 10518300 x^{4} + 906192 x^{3} + 35960 x^{2} + 496 x + 1}{32 \left(x^{15} + 155 x^{14} + 6293 x^{13} + 105183 x^{12} + 876525 x^{11} + 4032015 x^{10} + 10855425 x^{9} + 17678835 x^{8} + 17678835 x^{7} + 10855425 x^{6} + 4032015 x^{5} + 876525 x^{4} + 105183 x^{3} + 6293 x^{2} + 155 x + 1\right)}$} \, .$$</span> Notice how quickly these grow—in fact, the degree of the polynomials doubles on every iteration! Now, if we take the symbolic derivatives of these functions using our ordinary calculus rules, and simplify (with the help of the computer), the derivative of one iteration is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mn>1</mn><mn>2</mn></mfrac><annotation encoding="application/x-tex">\frac{1}{2}</annotation></semantics></math>, of two iterations is <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mn>2</mn><mi>x</mi><mo>+</mo><mn>5</mn></mrow><mrow><mn>4</mn><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mn>2</mn><mi>x</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">\frac{x^{2} + 2 x + 5}{4 \left(x^{2} + 2 x + 1\right)} \, ,</annotation></semantics></math> of three iterations is <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><msup><mi>x</mi><mn>6</mn></msup><mo>+</mo><mn>14</mn><msup><mi>x</mi><mn>5</mn></msup><mo>+</mo><mn>147</mn><msup><mi>x</mi><mn>4</mn></msup><mo>+</mo><mn>340</mn><msup><mi>x</mi><mn>3</mn></msup><mo>+</mo><mn>375</mn><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mn>126</mn><mi>x</mi><mo>+</mo><mn>21</mn></mrow><mrow><mn>8</mn><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>x</mi><mn>6</mn></msup><mo>+</mo><mn>14</mn><msup><mi>x</mi><mn>5</mn></msup><mo>+</mo><mn>63</mn><msup><mi>x</mi><mn>4</mn></msup><mo>+</mo><mn>100</mn><msup><mi>x</mi><mn>3</mn></msup><mo>+</mo><mn>63</mn><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mn>14</mn><mi>x</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">\frac{x^{6} + 14 x^{5} + 147 x^{4} + 340 x^{3} + 375 x^{2} + 126 x + 21}{8 \left(x^{6} + 14 x^{5} + 63 x^{4} + 100 x^{3} + 63 x^{2} + 14 x + 1\right)} \, ,</annotation></semantics></math> of four iterations is <span class="math display">$$\resizebox{1.0\hsize}{!}{$\frac{x^{14} + 70 x^{13} + 3199 x^{12} + 52364 x^{11} + 438945 x^{10} + 2014506 x^{9} + 5430215 x^{8} + 8836200 x^{7} + 8842635 x^{6} + 5425210 x^{5} + 2017509 x^{4} + 437580 x^{3} + 52819 x^{2} + 3094 x + 85}{16 \left(x^{14} + 70 x^{13} + 1771 x^{12} + 20540 x^{11} + 126009 x^{10} + 440986 x^{9} + 920795 x^{8} + 1173960 x^{7} + 920795 x^{6} + 440986 x^{5} + 126009 x^{4} + 20540 x^{3} + 1771 x^{2} + 70 x + 1\right)}$} \, ,$$</span> and of five iterations is a monstrosity you can only read by zooming in: <span class="math display">$$\resizebox{1.0\hsize}{!}{$\frac{x^{30} + 310 x^{29} + 59799 x^{28} + 4851004 x^{27} + 215176549 x^{26} + 5809257090 x^{25} + 102632077611 x^{24} + 1246240871640 x^{23} + 10776333438765 x^{22} + 68124037776390 x^{21} + 321156247784955 x^{20} + 1146261110726340 x^{19} + 3133113888931089 x^{18} + 6614351291211874 x^{17} + 10850143060249839 x^{16} + 13883516068991952 x^{15} + 13883516369532147 x^{14} + 10850142795067314 x^{13} + 6614351497464949 x^{12} + 3133113747810564 x^{11} + 1146261195398655 x^{10} + 321156203432790 x^{9} + 68124057936465 x^{8} + 10776325550040 x^{7} + 1246243501215 x^{6} + 102631341330 x^{5} + 5809427001 x^{4} + 215145084 x^{3} + 4855499 x^{2} + 59334 x + 341}{32 \left(x^{30} + 310 x^{29} + 36611 x^{28} + 2161196 x^{27} + 73961629 x^{26} + 1603620018 x^{25} + 23367042639 x^{24} + 238538538360 x^{23} + 1758637118685 x^{22} + 9579944198310 x^{21} + 39232152623175 x^{20} + 122387258419860 x^{19} + 293729420641881 x^{18} + 546274556891506 x^{17} + 791156255418003 x^{16} + 894836006026128 x^{15} + 791156255418003 x^{14} + 546274556891506 x^{13} + 293729420641881 x^{12} + 122387258419860 x^{11} + 39232152623175 x^{10} + 9579944198310 x^{9} + 1758637118685 x^{8} + 238538538360 x^{7} + 23367042639 x^{6} + 1603620018 x^{5} + 73961629 x^{4} + 2161196 x^{3} + 36611 x^{2} + 310 x + 1\right)}$} \, .$$</span> This is a terrible way to compute derivatives! (However, more sophisticated approaches to efficient symbolic differentiation exist, such as the <a href="https://www.microsoft.com/en-us/research/publication/the-d-symbolic-differentiation-algorithm/">“<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>D</mi><mo>*</mo></msup><annotation encoding="application/x-tex">D^*</annotation></semantics></math>” algorithm</a>, that avoid explicit giant formulas by exploiting repeated subexpressions.)</p>
<p>To be clear, the dual number approach (absent rounding errors) computes an answer exactly as if it evaluated these crazy expressions at some particular <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>, but the words “as if” are very important here. As you can see, we do not form these expressions, let alone evaluate them. We merely compute results that are equal to the values we would have gotten if we had.</p>
</section>
<section id="automatic-differentiation-via-computational-graphs" class="level2">
<h2 class="anchored" data-anchor-id="automatic-differentiation-via-computational-graphs">Automatic Differentiation via Computational Graphs</h2>
<p>Let’s now get into automatic differentiation via computational graphs. For this section, we consider the following simple motivating example.</p>
<div class="example">
<p><span id="ex:compute-graph" data-label="ex:compute-graph"></span> Define the following functions: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left" style="text-align: left"><mi>a</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>sin</mo><mi>x</mi></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><mi>b</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mi>y</mi></mfrac><mo>⋅</mo><mi>a</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><mi>z</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>b</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>x</mi><mi>.</mi></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">\begin{cases}
        a(x,y) = \sin x \\
        b(x,y) = \frac{1}{y}\cdot a(x,y) \\
        z(x,y) = b(x,y) + x.
    \end{cases}</annotation></semantics></math> Compute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><mi>z</mi></mrow><mrow><mi>∂</mi><mi>x</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial z}{\partial x}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>∂</mi><mi>z</mi></mrow><mrow><mi>∂</mi><mi>y</mi></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">\frac{\partial z}{\partial y}.</annotation></semantics></math></p>
</div>
<p>There are a few ways to solve this problem. Firstly, of course, one can compute this symbolically, noting that <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>b</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>x</mi><mo>=</mo><mfrac><mn>1</mn><mi>y</mi></mfrac><mi>a</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>x</mi><mo>=</mo><mfrac><mrow><mo>sin</mo><mi>x</mi></mrow><mi>y</mi></mfrac><mo>+</mo><mi>x</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">z(x,y) = b(x,y) + x = \frac{1}{y} a(x,y) + x = \frac{\sin x}{y} + x,</annotation></semantics></math> which implies <span class="math display">$$\frac{\partial z}{\partial x} = \frac{\cos x}{y} + 1 \hspace{.25cm} \text{and} \hspace{.25cm} \frac{\partial z}{\partial y} = -\frac{\sin x}{y^2}.$$</span></p>
<p>However, one can also use a Computational Graph (see Figure of Computational Graph below) where the edge from node <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> to node <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> is labelled with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><mi>B</mi></mrow><mrow><mi>∂</mi><mi>A</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial B}{\partial A}</annotation></semantics></math>.</p>
<figure class="figure">
<div class="center">

</div>
<figcaption>
A computational graph corresponding to example&nbsp;<a href="#ex:compute-graph" data-reference-type="ref" data-reference="ex:compute-graph">[ex:compute-graph]</a>, representing the computation of an <span style="color: blue">output</span> <span class="math inline"><em>z</em>(<em>x</em>, <em>y</em>)</span> from two <span style="color: red">inputs</span> <span class="math inline"><em>x</em>, <em>y</em></span>, with intermediate quantities <span class="math inline"><em>a</em>(<em>x</em>, <em>y</em>)</span> and <span class="math inline"><em>b</em>(<em>x</em>, <em>y</em>)</span>. The nodes are labelled by <em>values</em>, and edges are labelled with the <em>derivatives</em> of the values with respect to the preceding values.
</figcaption>
</figure>
<p>Now how do we use this directed acyclic graph (DAG) to find the derivatives? Well one view (called the “forward view”) is given by following the paths from the inputs to the outputs and (left) multiplying as you go, adding together multiple paths. For instance, following this procedure for paths from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">z(x,y)</annotation></semantics></math>, we have <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>∂</mi><mi>z</mi></mrow><mrow><mi>∂</mi><mi>x</mi></mrow></mfrac><mo>=</mo><mn>1</mn><mo>⋅</mo><mfrac><mn>1</mn><mi>y</mi></mfrac><mo>⋅</mo><mo>cos</mo><mi>x</mi><mo>+</mo><mn>1</mn><mo>=</mo><mfrac><mrow><mo>cos</mo><mi>x</mi></mrow><mi>y</mi></mfrac><mo>+</mo><mn>1</mn><mi>.</mi></mrow><annotation encoding="application/x-tex">\frac{\partial z}{\partial x} = 1 \cdot \frac{1}{y} \cdot \cos x + 1 = \frac{\cos x}{y} + 1.</annotation></semantics></math> Similarly, for paths from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math> to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">z(x,y)</annotation></semantics></math>, we have <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>∂</mi><mi>z</mi></mrow><mrow><mi>∂</mi><mi>y</mi></mrow></mfrac><mo>=</mo><mn>1</mn><mo>⋅</mo><mfrac><mrow><mi>−</mi><mi>a</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><msup><mi>y</mi><mn>2</mn></msup></mfrac><mo>=</mo><mfrac><mrow><mi>−</mi><mo>sin</mo><mi>x</mi></mrow><msup><mi>y</mi><mn>2</mn></msup></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">\frac{\partial z}{\partial y} = 1 \cdot \frac{-a(x,y)}{ y^2} = \frac{- \sin x}{y^2},</annotation></semantics></math> and if you have numerical derivatives on the edges, this algorithm works. Alternatively, you could follow a reverse view and follow the paths backwards (multiplying right to left), and obtain the same result. Note that there is nothing magic about these being scalar here– you could imagine these functions are the type that we are seeing in this class and do the same computations! The only thing that matters here fundamentally is the associativity. However, when considering vector-valued functions, the order in which you multiply the edge weights is vitally important (as vector/matrix valued functions are not generally commutative).</p>
<p>The graph-theoretic way of thinking about this is to consider “path products”. A path product is the product of edge weights as you traverse a path. In this way, we are interested in the sum of path products from inputs to outputs to compute derivatives using computational graphs. Clearly, we don’t particularly care which order we traverse the paths as long as the <em>order</em> we take the product in is correct. In this way, forward and reverse-mode automatic differentiation is not so mysterious.</p>
<p>Let’s take a closer view of the implementation of forward-mode automatic differentiation. Suppose we are at a node <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> during the process of computing the derivative of a computational graph, as shown in the figure below:</p>
<figure class="figure">
<div class="center">

</div>
</figure>
<p>Suppose we know the path product <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> of all the edges up to and including the one from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>B</mi><mn>2</mn></msub><annotation encoding="application/x-tex">B_2</annotation></semantics></math>? to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>. Then what is the new path product as we move to the right from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>? It is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>⋅</mo><mi>P</mi></mrow><annotation encoding="application/x-tex">f'(A)\cdot P</annotation></semantics></math>! So we need a data structure that maps in the following way: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">value</mtext><mo>,</mo><mtext mathvariant="normal">path product</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>↦</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">value</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>f</mi><mi>′</mi><mo>⋅</mo><mtext mathvariant="normal">path product</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">(\text{value}, \text{path product}) \mapsto (f(\text{value}), f'\cdot \text{path product}).</annotation></semantics></math> In some sense, this is another way to look at the Dual Numbers– taking in our path products and spitting out values. In any case, we overload our program which can easily calculate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">value</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(\text{value})</annotation></semantics></math> and tack-on <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mo>⋅</mo><mtext mathvariant="normal">(path product)</mtext></mrow><annotation encoding="application/x-tex">f'\cdot \text{(path product)}</annotation></semantics></math>.</p>
<p>One might ask how our program starts– this is how the program works in the “middle”, but what should our starting value be? Well the only thing it can be for this method to work is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(x, 1)</annotation></semantics></math>. Then, at every step you do the following map listed above: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">value</mtext><mo>,</mo><mtext mathvariant="normal">path product</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>↦</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">value</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>f</mi><mi>′</mi><mo>⋅</mo><mtext mathvariant="normal">path product</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">(\text{value}, \text{path product}) \mapsto (f(\text{value}), f'\cdot \text{path product}),</annotation></semantics></math> and at the end we obtain our derivatives.</p>
<p>Now how do we combine arrows? In other words, suppose at the two notes on the LHS we have the values <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>a</mi><mo>,</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(a,p)</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>b</mi><mo>,</mo><mi>q</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(b,q)</annotation></semantics></math>, as seen in the diagram below:</p>
<figure class="figure">
<div class="center">

</div>
</figure>
<p>So here, we aren’t thinking of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>,</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">a,b</annotation></semantics></math> as numbers, but as variables. What should the new output value be? We want to add the two path products together, obtaining <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mfrac><mrow><mi>∂</mi><mi>z</mi></mrow><mrow><mi>∂</mi><mi>a</mi></mrow></mfrac><mi>p</mi><mo>+</mo><mfrac><mrow><mi>∂</mi><mi>z</mi></mrow><mrow><mi>∂</mi><mi>b</mi></mrow></mfrac><mi>q</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\left(f(a,b), \frac{\partial z}{\partial a} p + \frac{\partial z}{\partial b} q\right).</annotation></semantics></math> So really, our overloaded data structure looks like this:</p>
<figure class="figure">
<div class="center">

</div>
</figure>
<p>This diagram of course generalizes if we may many different nodes on the left side of the graph.</p>
<p>If we come up with such a data structure for all of the simple computations (addition/subtraction, multiplication, and division), and if this is all we need for our computer program, then we are set! Here is how we define the structure for addition/subtraction, multiplication, and division.</p>
<p><strong>Addition/Subtraction:</strong> See figure.</p>
<figure class="figure">
<div class="center">

</div>
<figcaption>
Figure of Addition/Subtraction Computational Graph
</figcaption>
</figure>
<p><strong>Multiplication:</strong> See figure.</p>
<figure class="figure">
<div class="center">

</div>
<figcaption>
Figure of Multiplication Computational Graph
</figcaption>
</figure>
<p><strong>Division:</strong> See figure.</p>
<figure class="figure">
<div class="center">

</div>
<figcaption>
Figure of Division Computational Graph
</figcaption>
</figure>
<p>In theory, these three graphs are all we need, and we can use Taylor series expansions for more complicated functions. But in practice, we throw in what the derivatives of more complicated functions are so that we don’t waste our time trying to compute something we already know, like the derivative of sine or of a logarithm.</p>
<section id="reverse-mode-automatic-differentiation-on-graphs" class="level3">
<h3 class="anchored" data-anchor-id="reverse-mode-automatic-differentiation-on-graphs">Reverse Mode Automatic Differentiation on Graphs</h3>
<p>When we do reverse mode, we have arrows going the other direction, which we will understand in this section of the notes. In forward mode it was all about “what do we depend on,” i.e.&nbsp;computing the derivative on the right hand side of the above diagram using the functions in the nodes on the left. In reverse mode, the question is really “what are we influenced by?” or “what do we influence later?”</p>
<p>When going “backwards,” we need know what nodes a given node influences. For instance, given a node A, we want to know the nodes <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>B</mi><mi>i</mi></msub><annotation encoding="application/x-tex">B_i</annotation></semantics></math> that is influenced by, or depends on, node <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>. So now our diagram looks like this:</p>
<figure class="figure">
<div class="center">

</div>
</figure>
<p>So now, we eventually have a final node <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(z,1)</annotation></semantics></math> (far on the right hand side) where everything starts. This time, all of our multiplications take place from right to left as we are in reverse mode. Our goal is to be able to calculate the node <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>∂</mi><mi>z</mi><mi>/</mi><mi>∂</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(x,\partial z/\partial x)</annotation></semantics></math>. So if we know how to fill in the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><mi>z</mi></mrow><mrow><mi>∂</mi><mi>a</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial z}{\partial a}</annotation></semantics></math> term, we will be able to go from right to left in these computational graphs (i.e., in reverse mode). In fact, the formula for getting <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><mi>z</mi></mrow><mrow><mi>∂</mi><mi>a</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial z}{\partial a}</annotation></semantics></math> is given by <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>∂</mi><mi>z</mi></mrow><mrow><mi>∂</mi><mi>a</mi></mrow></mfrac><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>s</mi></munderover><mfrac><mrow><mi>∂</mi><msub><mi>b</mi><mi>i</mi></msub></mrow><mrow><mi>∂</mi><mi>a</mi></mrow></mfrac><mfrac><mrow><mi>∂</mi><mi>z</mi></mrow><mrow><mi>∂</mi><msub><mi>b</mi><mi>i</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial z}{\partial a} = \sum_{i=1}^s \frac{\partial b_i}{\partial a} \frac{\partial z}{\partial b_i}</annotation></semantics></math> where the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>b</mi><mi>i</mi></msub><annotation encoding="application/x-tex">b_i</annotation></semantics></math>s come from the nodes that are influenced by the node <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>. This is again just another chain rule like from calculus, but you can also view this as multiplying the sums of all the weights in the graph influenced by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>.</p>
<figure class="figure">
<div class="center">

</div>
</figure>
<p>Why can reverse mode be more efficient than forward mode? One reason it because it can save data and use it later. Take, for instance, the following sink/source computational graph.</p>
<p>If <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">x,y</annotation></semantics></math> here are our sources, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math> is our sink, we want to compute the sum of products of weights on paths from sources to sinks. If we were using forward mode, we would need to compute the paths <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>c</mi><mi>a</mi></mrow><annotation encoding="application/x-tex">dca</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>c</mi><mi>b</mi></mrow><annotation encoding="application/x-tex">dcb</annotation></semantics></math>, which requires four multiplications (and then you would add them together). If we were using reverse mode, we would only need compute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><munder><mrow><mi>c</mi><mi>d</mi></mrow><mo accent="true">_</mo></munder></mrow><annotation encoding="application/x-tex">a\underline{cd}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><munder><mrow><mi>c</mi><mi>d</mi></mrow><mo accent="true">_</mo></munder></mrow><annotation encoding="application/x-tex">b\underline{cd}</annotation></semantics></math> and sum them; notice reverse mode (since we need only compute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mi>d</mi></mrow><annotation encoding="application/x-tex">cd</annotation></semantics></math> once), only takes 3 multiplications. In general, this can more efficiently resolve certain types of problems, such as the source/sink one.</p>
</section>
</section>
<section id="sec:forward-vs-reverse" class="level2">
<h2 class="anchored" data-anchor-id="sec:forward-vs-reverse">Forward- vs.&nbsp;Reverse-mode Differentiation</h2>
<p>In this section, we briefly summarize the relative benefits and drawbacks of these two approaches to computation of derivatives (whether worked out by hand or using AD software). From a mathematical point of view, the two approaches are mirror images, but from a computational point of view they are quite different, because computer programs normally proceed “forwards” in time from inputs to outputs.</p>
<p>Suppose we are differentiating a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>:</mo><msup><mi>ℝ</mi><mi>n</mi></msup><mo>↦</mo><msup><mi>ℝ</mi><mi>m</mi></msup></mrow><annotation encoding="application/x-tex">f: \mathbb{R}^n \mapsto \mathbb{R}^m</annotation></semantics></math>, mapping <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> scalar inputs (an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-dimensional input) to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> scalar outputs (an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>-dimensional output). The first key distinction of forward- vs.&nbsp;reverse-mode is how the computational cost scales with the number/dimension of inputs and outputs:</p>
<ul>
<li><p>The cost of forward-mode differentiation (inputs-to-outputs) scales proportional to&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>, the number of <em>inputs</em>. This is ideal for functions where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>≪</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">n \ll m</annotation></semantics></math> (few inputs, many outputs).</p></li>
<li><p>The cost of reverse-mode differentiation (outputs-to-inputs) scales proportional to&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>, the number of <em>outputs</em>. This is ideal for functions where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>≪</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m \ll n</annotation></semantics></math> (few outputs, many inputs).</p></li>
</ul>
<p>Before this chapter, we first saw these scalings in Sec.&nbsp;<a href="#sec:cost-matrix-mult" data-reference-type="ref" data-reference="sec:cost-matrix-mult">2.5.1</a>, and again in Sec.&nbsp;<a href="#sec:adjoint-method" data-reference-type="ref" data-reference="sec:adjoint-method">6.3</a>; in a future lecture, we’ll see it yet again in Sec.&nbsp;<a href="#sec:ODE-sensitivity" data-reference-type="ref" data-reference="sec:ODE-sensitivity">9.2</a>. The case of few outputs is extremely common in large-scale optimization (whether for machine learning, engineering design, or other applications), because then one has many optimization parameters (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>≫</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n\gg 1</annotation></semantics></math>) but only a single output (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">m=1</annotation></semantics></math>) corresponding to the objective (or “loss”) function, or sometimes a few outputs corresponding to objective and constraint functions. Hence, reverse-mode differentiation (“backpropagation”) is the dominant approach for large-scale optimization and applications such as training neural networks.</p>
<p>There are other practical issues worth considering, however:</p>
<ul>
<li><p>Forward-mode differentiation proceeds in the same order as the computation of the function itself, from inputs to outputs. This seems to make forward-mode AD easier to implement (e.g.&nbsp;our sample implementation in Sec.&nbsp;<a href="#sec:dual-AD" data-reference-type="ref" data-reference="sec:dual-AD">8.1</a>) and efficient.</p></li>
<li><p>Reverse-mode differentiation proceeds in the <em>opposite</em> direction to ordinary computation. This makes reverse-mode AD much more complicated to implement, and adds a lot of <em>storage overhead</em> to the function computation. First you evaluate the function from inputs to outputs, but you (or the AD system) keep a <em>record</em> (a “tape”) of all the <em>intermediate steps</em> of the computation; then, you run the computation in <em>reverse</em> (“play the tape backwards”) to backpropagate the derivatives.</p></li>
</ul>
<p>As a result of these practical advantages, even for the case of many (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n &gt;1</annotation></semantics></math>) inputs and a single (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">m = 1</annotation></semantics></math>) output, practitioners tell us that they’ve found forward mode to be more efficient until <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> becomes sufficiently large (perhaps even until&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>&gt;</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">n &gt; 100</annotation></semantics></math>, depending on the function being differentiated and the AD implementation). (You may also be interested in the blog post <a href="https://www.stochasticlifestyle.com/engineering-trade-offs-in-automatic-differentiation-from-tensorflow-and-pytorch-to-jax-and-julia/">Engineering Trade-offs in AD</a> by Chris&nbsp;Rackauckas, which is mainly about reverse-mode implementations.)</p>
<p>If <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mi>m</mi></mrow><annotation encoding="application/x-tex">n = m</annotation></semantics></math>, where neither approach has a scaling advantage, one typically prefers the lower overhead and simplicity of forward-mode differentiation. This case arises in computing explicit Jacobian matrices for nonlinear root-finding (Sec.&nbsp;<a href="#sec:newton-roots" data-reference-type="ref" data-reference="sec:newton-roots">6.1</a>), or Hessian matrices of second derivatives (Sec.&nbsp;<a href="#sec:hessians" data-reference-type="ref" data-reference="sec:hessians">12</a>), for which one often uses forward mode…or even a <em>combination</em> of forward and reverse modes, as discussed below.</p>
<p>Of course, forward and reverse are not the only options. The chain rule is associative, so there are many possible orderings (e.g.&nbsp;starting from both ends and meeting in the middle, or vice versa). A difficult<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> problem that may often require hybrid schemes is to compute Jacobians (or Hessians) in a minimal number of operations, exploiting any problem-specific structure (e.g.&nbsp;sparsity: many entries may be zero). Discussion of this and other AD topics can be found, in vastly greater detail than in these notes, in the book <em>Evaluating Derivatives</em> (2nd ed.) by Griewank and Walther&nbsp;(2008).</p>
<section id="sec:forward-over-reverse" class="level3">
<h3 class="anchored" data-anchor-id="sec:forward-over-reverse">Forward-over-reverse mode: Second derivatives</h3>
<p>Often, a <em>combination</em> of forward- and reverse-mode differentiation is advantageous when computing <em>second</em> derivatives, which arise in many practical applications.</p>
<p><strong>Hessian computation:</strong> For example, let us consider a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>:</mo><msup><mi>ℝ</mi><mi>n</mi></msup><mo>→</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">f(x): \mathbb{R}^n \to \mathbb{R}</annotation></semantics></math> mapping <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> inputs <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> to a single scalar. The first derivative <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">f'(x) = (\nabla f)^T</annotation></semantics></math> is best computed by reverse mode if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>≫</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n \gg 1</annotation></semantics></math> (many inputs). Now, however, consider the <em>second</em> derivative, which is the derivative of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">g(x) = \nabla f</annotation></semantics></math>, mapping <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> inputs <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> outputs <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math>. It should be clear that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g'(x)</annotation></semantics></math> is therefore an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \times n</annotation></semantics></math> Jacobian matrix, called the <strong>Hessian</strong> of&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>, which we will discuss much more generally in Sec.&nbsp;<a href="#sec:hessians" data-reference-type="ref" data-reference="sec:hessians">12</a>. Since <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(x)</annotation></semantics></math> has the same number of inputs and outputs, neither forward nor reverse mode has an inherent scaling advantage, so typically forward mode is chosen for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">g'</annotation></semantics></math> thanks to its practical simplicity, while still computing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> in reverse-mode. That is, we compute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> by reverse mode, but then compute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mi>′</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi></mrow><annotation encoding="application/x-tex">g' = (\nabla f)'</annotation></semantics></math> by applying forward-mode differentiation to the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> algorithm. This is called a <strong>forward-over-reverse</strong> algorithm.</p>
<p>An even more clear-cut application of forward-over-reverse differentiation is to <strong>Hessian–vector products</strong>. In many applications, it turns out that what is required is only the <em>product</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mi>v</mi></mrow><annotation encoding="application/x-tex">(\nabla f)' v</annotation></semantics></math> of the Hessian <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi></mrow><annotation encoding="application/x-tex">(\nabla f)'</annotation></semantics></math> with an arbitrary vector&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math>. In this case, one can completely avoid computing (or storing) the Hessian matrix explicitly, and incur computational cost proportional only to that of a single function evaluation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math>. The trick is to recall (from Sec.&nbsp;<a href="#sec:directional" data-reference-type="ref" data-reference="sec:directional">2.2.1</a>) that, for <em>any</em> function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>, the linear operation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>v</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">g'(x)[v]</annotation></semantics></math> is a <em>directional derivative</em>, equivalent to a <em>single-variable</em> derivative <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>∂</mi><mrow><mi>∂</mi><mi>α</mi></mrow></mfrac><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>α</mi><mi>v</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\frac{\partial}{\partial\alpha} g(x+\alpha v)</annotation></semantics></math> evaluated at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\alpha = 0</annotation></semantics></math>. Here, we simply apply that rule to the function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">g(x) = \nabla f</annotation></semantics></math>, and obtain the following formula for a Hessian–vector product: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mi>v</mi><mo>=</mo><msub><mrow><mfrac><mi>∂</mi><mrow><mi>∂</mi><mi>α</mi></mrow></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><msub><mrow><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mi>x</mi><mo>+</mo><mi>α</mi><mi>v</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mi>α</mi><mo>=</mo><mn>0</mn></mrow></msub><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">(\nabla f)' v = \left. \frac{\partial}{\partial\alpha} \left( \left. \nabla f \right|_{x + \alpha v} \right) \right|_{\alpha=0} \, .</annotation></semantics></math> Computationally, the inner evaluation of the gradient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> at an arbitrary point <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>+</mo><mi>α</mi><mi>v</mi></mrow><annotation encoding="application/x-tex">x + \alpha v</annotation></semantics></math> can be accomplished efficiently by a reverse/adjoint/backpropagation algorithm. In contrast, the <em>outer</em> derivative with respect to a <em>single</em> input <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> is best performed by forward-mode differentiation.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> Since the Hessian matrix is symmetric (as discussed in great generality by Sec.&nbsp;<a href="#sec:hessians" data-reference-type="ref" data-reference="sec:hessians">12</a>), the same algorithm works for <strong>vector–Hessian products</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>v</mi><mi>T</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">[</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mi>v</mi><mo stretchy="true" form="postfix">]</mo></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">v^T (\nabla f)' = [(\nabla f)' v]^T</annotation></semantics></math>, a fact that we employ in the next example.</p>
<p><strong>Scalar-valued functions of gradients:</strong> There is another common circumstance in which one often combines forward and reverse differentiation, but which can appear somewhat more subtle, and that is in differentiating a scalar-valued function of a gradient of another scalar-valued function. Consider the following example:</p>
<div class="example">
<p>Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>:</mo><msup><mi>ℝ</mi><mi>n</mi></msup><mo>↦</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">f(x): \mathbb{R}^n \mapsto \mathbb{R}</annotation></semantics></math> be a scalar-valued function of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>≫</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n \gg 1</annotation></semantics></math> inputs with gradient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>x</mi></msub><mo>=</mo><mi>f</mi><mi>′</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\left. \nabla f \right|_{x} = f'(x)^T</annotation></semantics></math>, and let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>:</mo><msup><mi>ℝ</mi><mi>n</mi></msup><mo>↦</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">g(z): \mathbb{R}^n \mapsto \mathbb{R}</annotation></semantics></math> be another such function with gradient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi>∇</mi><mi>g</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>z</mi></msub><mo>=</mo><mi>g</mi><mi>′</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\left. \nabla g \right|_{z} = g'(z)^T</annotation></semantics></math>. Now, consider the scalar-valued function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mrow><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>x</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>:</mo><msup><mi>ℝ</mi><mi>n</mi></msup><mo>↦</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">h(x) = g(\left. \nabla f \right|_{x}): \mathbb{R}^n \mapsto \mathbb{R}</annotation></semantics></math> and compute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi>∇</mi><mi>h</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>x</mi></msub><mo>=</mo><mi>h</mi><mi>′</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\left. \nabla h \right|_{x} = h'(x)^T</annotation></semantics></math>.</p>
<p>Denote <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>=</mo><msub><mrow><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>x</mi></msub></mrow><annotation encoding="application/x-tex">z = \left. \nabla f \right|_{x}</annotation></semantics></math>. By the chain rule, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>g</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">h'(x) = g'(z)(\nabla f)'(x)</annotation></semantics></math>, but we want to avoid explicitly computing the large <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \times n</annotation></semantics></math> Hessian matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi></mrow><annotation encoding="application/x-tex">(\nabla f)'</annotation></semantics></math>. Instead, as discussed above, we use the fact that such a vector–Hessian product is equivalent (by symmetry of the Hessian) to the transpose of a Hessian–vector product multiplying the Hessian <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi></mrow><annotation encoding="application/x-tex">(\nabla f)'</annotation></semantics></math> with the vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>g</mi><mo>=</mo><mi>g</mi><mi>′</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\nabla g = g'(z)^T</annotation></semantics></math>, which is equivalent to a directional derivative: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi>∇</mi><mi>h</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>x</mi></msub><mo>=</mo><mi>h</mi><mi>′</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mo>=</mo><msub><mrow><mfrac><mi>∂</mi><mrow><mi>∂</mi><mi>α</mi></mrow></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><msub><mrow><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mi>x</mi><mo>+</mo><mi>α</mi><msub><mrow><mi>∇</mi><mi>g</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>z</mi></msub></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mi>α</mi><mo>=</mo><mn>0</mn></mrow></msub><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">\left. \nabla h \right|_{x} = h'(x)^T =
\left. \frac{\partial}{\partial\alpha} \left( \left. \nabla f \right|_{x + \alpha \left. \nabla g \right|_{z}}\right) \right|_{\alpha = 0} \, ,</annotation></semantics></math> involving differentiation with respect to a single scalar <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">\alpha \in \mathbb{R}</annotation></semantics></math>. As for any Hessian–vector product, therefore, we can evaluate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">\nabla h</annotation></semantics></math> by:</p>
<ol type="1">
<li><p>Evaluate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">h(x)</annotation></semantics></math>: evaluate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>=</mo><msub><mrow><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>x</mi></msub></mrow><annotation encoding="application/x-tex">z = \left. \nabla f \right|_{x}</annotation></semantics></math> by reverse mode, and plug it into <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(z)</annotation></semantics></math>.</p></li>
<li><p>Evaluate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">\nabla h</annotation></semantics></math>:</p>
<ol type="1">
<li><p>Evaluate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mrow><mi>∇</mi><mi>g</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>z</mi></msub><annotation encoding="application/x-tex">\left. \nabla g \right|_{z}</annotation></semantics></math> by reverse mode.</p></li>
<li><p>Implement <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mrow><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mi>x</mi><mo>+</mo><mi>α</mi><msub><mrow><mi>∇</mi><mi>g</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>z</mi></msub></mrow></msub><annotation encoding="application/x-tex">\left. \nabla f \right|_{x + \alpha \left. \nabla g \right|_{z} }</annotation></semantics></math> by reverse mode, and then differentiate with respect to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> by <em>forward</em> mode, evaluated at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\alpha = 0</annotation></semantics></math>.</p></li>
</ol></li>
</ol>
<p>This is a “forward-over-reverse” algorithm, where forward mode is used efficiently for the single-input derivative with respect to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">\alpha \in \mathbb{R}</annotation></semantics></math>, combined with reverse mode to differentate with respect to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>,</mo><mi>z</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">x,z \in \mathbb{R}^n</annotation></semantics></math>.</p>
</div>
<p>Example Julia code implementing the above “forward-over-reverse” process for just such a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">h(x)=g(\nabla f)</annotation></semantics></math> function is given below. Here, the forward-mode differentiation with respect to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> is implemented by the ForwardDiff.jl package discussed in Sec.&nbsp;<a href="#sec:dual-AD" data-reference-type="ref" data-reference="sec:dual-AD">8.1</a>, while the reverse-mode differentiation with respect to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math> is performed by the <a href="https://fluxml.ai/Zygote.jl/stable/">Zygote.jl</a> package. First, let’s import the packages and define simple example functions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mi>/</mi><mo stretchy="false" form="postfix">‖</mo><mi>x</mi><mo stretchy="false" form="postfix">‖</mo></mrow><annotation encoding="application/x-tex">f(x) = 1/\Vert x \Vert</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mo>∑</mo><mi>k</mi></msub><msub><mi>z</mi><mi>k</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">g(z) = (\sum_k z_k)^3</annotation></semantics></math>, along with the computation of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>h</mi><annotation encoding="application/x-tex">h</annotation></semantics></math> via Zygote:</p>
<pre class="jlcon"><code>julia&gt; using ForwardDiff, Zygote, LinearAlgebra
julia&gt; f(x) = 1/norm(x)
julia&gt; g(z) = sum(z)^3
julia&gt; h(x) = g(Zygote.gradient(f, x)[1])</code></pre>
<p>Now, we’ll compute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">\nabla h</annotation></semantics></math> by forward-over-reverse:</p>
<pre class="jlcon"><code>julia&gt; function ∇h(x)
           ∇f(y) = Zygote.gradient(f, y)[1]
           ∇g = Zygote.gradient(g, ∇f(x))[1]
           return ForwardDiff.derivative(α -&gt; ∇f(x + α*∇g), 0)
       end</code></pre>
<p>We can now plug in some random numbers and compare to a finite-difference check:</p>
<pre class="jlcon"><code>julia&gt; x = randn(5); δx = randn(5) * 1e-8;

julia&gt; h(x)
-0.005284687528953334

julia&gt; ∇h(x)
5-element Vector{Float64}:
 -0.006779692698531759
  0.007176439898271982
 -0.006610264199241697
 -0.0012162087082746558
  0.007663756720005014

julia&gt; ∇h(x)' * δx    # directional derivative
-3.0273434457397667e-10

julia&gt; h(x+δx) - h(x)  # finite-difference check
-3.0273433933303284e-10</code></pre>
<p>The finite-difference check matches to about 7&nbsp;significant digits, which is as much as we can hope for—the forward-over-reverse code works!</p>
<div class="problem">
<p>A common variation on the above procedure, which often appears in machine learning, involves a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">f(x,p) \in \mathbb{R}</annotation></semantics></math> that maps input “data” <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">x \in \mathbb{R}^n</annotation></semantics></math> and “parameters” <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>N</mi></msup></mrow><annotation encoding="application/x-tex">p \in \mathbb{R}^N</annotation></semantics></math> to a scalar. Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><mi>x</mi></msub><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla_x f</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><mi>p</mi></msub><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla_p f</annotation></semantics></math> denote the gradients with respect to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>.</p>
<p>Now, suppose we have a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>:</mo><msup><mi>ℝ</mi><mi>n</mi></msup><mo>↦</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">g(z): \mathbb{R}^n \mapsto \mathbb{R}</annotation></semantics></math> as before, and define <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mrow><msub><mi>∇</mi><mi>x</mi></msub><mi>f</mi><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mi>x</mi><mo>,</mo><mi>p</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">h(x,p) = g(\left. \nabla_x f \right|_{x,p})</annotation></semantics></math>. We want to compute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><mi>p</mi></msub><mi>h</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>∂</mi><mi>h</mi><mi>/</mi><mi>∂</mi><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\nabla_p h = (\partial h / \partial p)^T</annotation></semantics></math>, which will involve “mixed” derivatives of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> with respect to <em>both</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>.</p>
<p>Show that you can compute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><mi>p</mi></msub><mi>h</mi></mrow><annotation encoding="application/x-tex">\nabla_p h</annotation></semantics></math> by: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><msub><mi>∇</mi><mi>p</mi></msub><mi>h</mi><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mi>x</mi><mo>,</mo><mi>p</mi></mrow></msub><mo>=</mo><msub><mrow><mfrac><mi>∂</mi><mrow><mi>∂</mi><mi>α</mi></mrow></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><msub><mrow><msub><mi>∇</mi><mi>p</mi></msub><mi>f</mi><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mi>x</mi><mo>+</mo><mi>α</mi><msub><mrow><mi>∇</mi><mi>g</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>z</mi></msub><mo>,</mo><mi>p</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mi>α</mi><mo>=</mo><mn>0</mn></mrow></msub><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">\left. \nabla_p h \right|_{x,p} =
\left. \frac{\partial}{\partial\alpha} \left( \left. \nabla_p f \right|_{x + \alpha \left. \nabla g \right|_{z},p}\right) \right|_{\alpha = 0} \, ,</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>=</mo><msub><mrow><msub><mi>∇</mi><mi>x</mi></msub><mi>f</mi><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mi>x</mi><mo>,</mo><mi>p</mi></mrow></msub></mrow><annotation encoding="application/x-tex">z = \left. \nabla_x f \right|_{x,p}</annotation></semantics></math>. (Crucially, this avoids ever computing an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">n \times N</annotation></semantics></math> mixed-derivative matrix of&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>.)</p>
<p>Try coming up with simple example functions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>, implementing the above formula by forward-over-reverse in Julia similar to above (forward mode for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∂</mi><mi>/</mi><mi>∂</mi><mi>α</mi></mrow><annotation encoding="application/x-tex">\partial/\partial \alpha</annotation></semantics></math> and reverse mode for the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>∇</mi><annotation encoding="application/x-tex">\nabla</annotation></semantics></math>’s), and checking your result against a finite-difference approximation.</p>
</div>
</section>
</section>
</section>
<section id="differentiating-ode-solutions" class="level1">
<h1>Differentiating ODE solutions</h1>
<p>In this lecture, we will consider the problem of differentiating the <em>solution</em> of ordinary differential equations (ODEs) with respect to parameters that appear in the equations and/or initial conditions. This is as important topic in a surprising number of practical applications, such as evaluating the effect of uncertainties, fitting experimental data, or machine learning (which is increasingly combining ODE models with neural networks). As in previous lectures, we will find that there are crucial practical distinctions between “forward” and “reverse” (“adjoint”) techniques for computing these derivatives, depending upon the number of parameters and desired outputs.</p>
<p>Although a basic familiarity with the concept of an ODE will be helpful to readers of this lecture, we will begin with a short review in order to establish our notation and terminology.</p>
<p>The video lecture on this topic for IAP 2023 was given by Dr.&nbsp;Frank Schäfer (MIT). These notes follow the same basic approach, but differ in some minor notational details.</p>
<section id="ordinary-differential-equations-odes" class="level2">
<h2 class="anchored" data-anchor-id="ordinary-differential-equations-odes">Ordinary differential equations (ODEs)</h2>
<p>An <strong>ordinary differential equation</strong> (<strong>ODE</strong>) is an equation for a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(t)</annotation></semantics></math> of “time”<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">t\in\mathbb{R}</annotation></semantics></math> in terms of one or more derivatives, most commonly in the <strong>first-order</strong> form <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>d</mi><mi>u</mi></mrow><mrow><mi>d</mi><mi>t</mi></mrow></mfrac><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\frac{du}{dt}=f(u,t)</annotation></semantics></math> for some right-hand-side function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>. Note that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(t)</annotation></semantics></math> need not be a scalar function—it could be a column vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">u\in\mathbb{R}^{n}</annotation></semantics></math>, a matrix, or any other differentiable object. One could also write ODEs in terms of higher derivatives <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mn>2</mn></msup><mi>u</mi><mi>/</mi><mi>d</mi><msup><mi>t</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">d^{2}u/dt^{2}</annotation></semantics></math> and so on, but it turns out that one can write any ODE in terms of first derivatives alone, simply by making <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> a vector with more components.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> To uniquely determine a solution of a first-order ODE, we need some additional information, typically an <strong>initial value</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>u</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">u(0)=u_{0}</annotation></semantics></math> (the value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">t=0</annotation></semantics></math>), in which case it is called an <strong>initial-value problem</strong>. These facts, and many other properties of ODEs, are reviewed in detail by many textbooks on differential equations, as well as in classes like 18.03 at MIT.</p>
<p>ODEs are important for a huge variety of applications, because the behavior of many realistic systems is defined in terms of rates of change (derivatives). For example, you may recall Newton’s laws of mechanics, in which acceleration (the derivative of velocity) is related to force (which may be a function of time, position, and/or velocity), and the solution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtext mathvariant="normal">position</mtext><mo>,</mo><mtext mathvariant="normal">velocity</mtext><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">u=[\text{position},\text{velocity}]</annotation></semantics></math> of the corresponding ODE tells us the trajectory of the system. In chemistry, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> might represent the concentrations of one or more reactant molecules, with the right-hand side <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> providing reaction rates. In finance, there are ODE-like models of stock or option prices. <em>Partial</em> differential equations (PDEs) are more complicated versions of the same idea, for example in which <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(x,t)</annotation></semantics></math> is a function of space <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> as well as time <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> and one has <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>t</mi></mrow></mfrac><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>,</mo><mi>x</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\frac{\partial u}{\partial t}=f(u,x,t)</annotation></semantics></math> in which <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> may involve some spatial derivatives of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math>.</p>
<p>In linear algebra (e.g.&nbsp;18.06 at MIT), we often consider initial-value problems for <em>linear</em> ODEs of the form <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>u</mi><mi>/</mi><mi>d</mi><mi>t</mi><mo>=</mo><mi>A</mi><mi>u</mi></mrow><annotation encoding="application/x-tex">du/dt=Au</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> is a column vector and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is a square matrix; if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is a constant matrix (independent of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math>), then the solution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>e</mi><mrow><mi>A</mi><mi>t</mi></mrow></msup><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(t)=e^{At}u(0)</annotation></semantics></math> can be described in terms of a matrix exponential <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>e</mi><mrow><mi>A</mi><mi>t</mi></mrow></msup><annotation encoding="application/x-tex">e^{At}</annotation></semantics></math>. More generally, there are many tricks to find explicit solutions of various sorts of ODEs (various functions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>). However, just as one cannot find explicit formulas for the integrals of most functions, there is no explicit formula for the solution of <em>most</em> ODEs, and in many practical applications one must resort to approximate numerical solutions. Fortunately, if you supply a computer program that can compute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(u,t)</annotation></semantics></math>, there are mature and sophisticated software libraries<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> which can compute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(t)</annotation></semantics></math> from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(0)</annotation></semantics></math> for any desired set of times <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>, to any desired level of accuracy (for example, to 8 significant digits).</p>
<p>For example, the most basic numerical ODE method computes the solution at a sequence of times <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>n</mi></msub><mo>=</mo><mi>n</mi><mi>Δ</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">t_{n}=n\Delta t</annotation></semantics></math> for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi>…</mi></mrow><annotation encoding="application/x-tex">n=0,1,2,\ldots</annotation></semantics></math> simply by approximating <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>d</mi><mi>u</mi></mrow><mrow><mi>d</mi><mi>t</mi></mrow></mfrac><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\frac{du}{dt}=f(u,t)</annotation></semantics></math> using the finite difference <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>t</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>t</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>Δ</mi><mi>t</mi></mrow></mfrac><mo>≈</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>t</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><msub><mi>t</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\frac{u(t_{n+1})-u(t_{n})}{\Delta t}\approx f(u(t_{n}),t_{n})</annotation></semantics></math>, giving us the “explicit” timestep algorithm: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>t</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>≈</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>t</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>Δ</mi><mi>t</mi><mspace width="0.167em"></mspace><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>t</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><msub><mi>t</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">u(t_{n+1})\approx u(t_{n})+\Delta t\,f(u(t_{n}),t_{n}).</annotation></semantics></math> Using this technique, known as “Euler’s method,” we can march the solution forward in time: starting from our initial condition <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>u</mi><mn>0</mn></msub><annotation encoding="application/x-tex">u_{0}</annotation></semantics></math>, we compute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>t</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Δ</mi><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(t_1) = u(\Delta t)</annotation></semantics></math>, then <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>t</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mi>Δ</mi><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(t_2) = u(2\Delta t)</annotation></semantics></math> from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>Δ</mi><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(\Delta t)</annotation></semantics></math>, and so forth. Of course, this might be rather inaccurate unless we make <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Δ</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">\Delta t</annotation></semantics></math> very small, necessitating many timesteps to reach a given time <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>, and there can arise other subtleties like “instabilities” where the error may accumulate exponentially rapidly with each timestep. It turns out that Euler’s method is mostly obsolete: there are much more sophisticated algorithms that robustly produce accurate solutions with far less computational cost. However, they all resemble Euler’s method in the conceptual sense: they use evaluations of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> at a few nearby times <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> to “extrapolate” <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> at a subsequent time somehow, and thus march the solution forwards through time.</p>
<p>Relying on a computer to obtain numerical solutions to ODEs is practically essential, but it can also make ODEs a lot more fun to work with. If you ever took a class on ODEs, you may remember a lot of tedious labor (tricky integrals, polynomial roots, systems of equations, integrating factors, etc.) to obtain solutions by hand. Instead, we can focus here on simply setting up the correct ODEs and integrals and trust the computer to do the rest.</p>
</section>
<section id="sec:ODE-sensitivity" class="level2">
<h2 class="anchored" data-anchor-id="sec:ODE-sensitivity">Sensitivity analysis of ODE solutions</h2>
<div id="fig:ode" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/ode-fig.pdf.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>If we have an ordinary differential equation (ODE) <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>t</mi></mrow></mfrac><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>,</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\frac{\partial u}{\partial t} = f(u, p, t)</annotation></semantics></math> whose solution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(p, t)</annotation></semantics></math> depends on parameters <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>, we would like to know the change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>u</mi><mo>=</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>+</mo><mi>d</mi><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">d u = u(p + d p, t) - u(p, t)</annotation></semantics></math> in the solution due to changes in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>. Here, we show a simple example <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>t</mi></mrow></mfrac><mo>=</mo><mi>−</mi><mi>p</mi><mi>u</mi></mrow><annotation encoding="application/x-tex">\frac{\partial u}{\partial t} = -pu</annotation></semantics></math>, whose solution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>e</mi><mrow><mi>−</mi><mi>p</mi><mi>t</mi></mrow></msup><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(p,t) = e^{-p t} u(p,0)</annotation></semantics></math> is known analytically, and show the change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>u</mi></mrow><annotation encoding="application/x-tex">\delta u</annotation></semantics></math> from changing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">p=1</annotation></semantics></math> to by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>p</mi><mo>=</mo><mn>0.1</mn></mrow><annotation encoding="application/x-tex">\delta p = 0.1</annotation></semantics></math>.</figcaption>
</figure>
</div>
<p>Often, ODEs depend on some additional parameters <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>N</mi></msup></mrow><annotation encoding="application/x-tex">p\in\mathbb{R}^{N}</annotation></semantics></math> (or some other vector space). For example, these might be reaction-rate coefficients in a chemistry problem, the masses of particles in a mechanics problem, the entries of the matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> in a linear ODE, and so on. So, you really have a problem of the form <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>t</mi></mrow></mfrac><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>,</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\frac{\partial u}{\partial t}=f(u,p,t),</annotation></semantics></math> where the solution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(p,t)</annotation></semantics></math> depends both on time <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> and the parameters <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>, and in which the initial condition <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>u</mi><mn>0</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(p,0)=u_{0}(p)</annotation></semantics></math> may also depend on the parameters.</p>
<p>The question is, how can we compute the derivative <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∂</mi><mi>u</mi><mi>/</mi><mi>∂</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">\partial u/\partial p</annotation></semantics></math> of the solution with respect to the parameters of the ODE? By this, as usual, we mean the linear operator that gives the first-order change in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> for a change in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>, as depicted in Fig.&nbsp;<a href="#fig:ode" data-reference-type="ref" data-reference="fig:ode">6</a>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>+</mo><mi>d</mi><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>p</mi><mo stretchy="true" form="postfix">]</mo></mrow><mspace width="2.0em"></mspace><mrow><mtext mathvariant="normal">(an </mtext><mspace width="0.333em"></mspace></mrow><mi>n</mi><mtext mathvariant="normal">-component infinitesimal vector)</mtext><mo>,</mo></mrow><annotation encoding="application/x-tex">u(p+dp,t)-u(p,t)=\frac{\partial u}{\partial p}[dp] \qquad \mbox{(an }n\mbox{-component infinitesimal vector)},</annotation></semantics></math> where of course <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∂</mi><mi>u</mi><mi>/</mi><mi>∂</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">\partial u/\partial p</annotation></semantics></math> (which can be thought of as an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">n \times N</annotation></semantics></math> Jacobian matrix) depends on <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>. This kind of question is commonplace. For example, it is important in:</p>
<ul>
<li><p>Uncertainty quantification (UQ): if you have some uncertainty in the parameters of your ODE (for example, you have a chemical reaction in which the reaction rates are only known experimentally <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>±</mi><annotation encoding="application/x-tex">\pm</annotation></semantics></math> some measurement errors), the derivative <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∂</mi><mi>u</mi><mi>/</mi><mi>∂</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">\partial u/\partial p</annotation></semantics></math> tells you (to first order, at least) how sensitive your answer is to each of these uncertainties.</p></li>
<li><p>Optimization and fitting: often, you want to choose the parameters <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> to maximize or minimize some objective (or “loss” in machine learning). For example, if your ODE models some chemical reaction with unknown reaction rates or other parameters <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>, you might want to <em>fit</em> the parameters <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> to minimize the difference between <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(p,t)</annotation></semantics></math> and some experimentally observed concentrations.</p></li>
</ul>
<p>In the latter case of optimization, you have a <strong>scalar objective function</strong> of the solution, since to minimize or maximize something you need a real number (and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> might be a vector). For example, this could take on one of the following two forms:</p>
<ol type="1">
<li><p>A real-valued function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>T</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>T</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">g(u(p,T),T) \in \mathbb{R}</annotation></semantics></math> that depends on the solution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>T</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(p,T)</annotation></semantics></math> at a particular time <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>. For example, if you have an experimental solution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>u</mi><mo>*</mo></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u_{*}(t)</annotation></semantics></math> that you are are trying to match at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">t=T</annotation></semantics></math>, you might minimize <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>T</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>T</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo stretchy="false" form="postfix">‖</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>T</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msub><mi>u</mi><mo>*</mo></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>T</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">g(u(p,T),T)=\Vert u(p,T)-u_{*}(T)\Vert^{2}</annotation></semantics></math>.</p></li>
<li><p>A real-valued function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mi>T</mi></msubsup><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">G(p)=\int_{0}^{T}g(u(p,t),t)dt</annotation></semantics></math> that depends on an average (here scaled by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>) over many times <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>∈</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">t\in(0,T)</annotation></semantics></math> of our time-dependent <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>. In the example of fitting experimental data <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>u</mi><mo>*</mo></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u_{*}(t)</annotation></semantics></math>, minimizing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mi>T</mi></msubsup><mo stretchy="false" form="postfix">‖</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msub><mi>u</mi><mo>*</mo></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn></msup><mi>d</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">G(p)=\int_{0}^{T}\Vert u(p,t)-u_{*}(t)\Vert^{2}dt</annotation></semantics></math> corresponds to a least-square fit to minimize the error averaged over a time <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> (e.g.&nbsp;the duration of your experiment).</p></li>
</ol>
<p>More generally, you can give more weight to certain times than others by including a non-negative weight function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">w(t)</annotation></semantics></math> in the integral: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>G</mi><mi>w</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mi>∞</mi></msubsup><munder><munder><mrow><mo stretchy="false" form="postfix">∥</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msub><mi>u</mi><mo>*</mo></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mo stretchy="false" form="postfix">∥</mo><mn>2</mn></msup></mrow><mo accent="true">⏟</mo></munder><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></munder><mspace width="0.167em"></mspace><mi>w</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>d</mi><mi>t</mi><mo>,</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">G_w(p)=\int_0^\infty \underbrace{\|u(p,t)-u_*(t)\|^2}_{g(u(p,t),t)}  \, w(t) \, dt , .</annotation></semantics></math> The two cases above are simply the choices <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>δ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo>−</mo><mi>T</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">w(t) = \delta(t-T)</annotation></semantics></math> (a Dirac delta function) and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left" style="text-align: left"><mn>1</mn></mtd><mtd columnalign="left" style="text-align: left"><mi>t</mi><mo>≤</mo><mi>T</mi></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><mn>0</mn></mtd><mtd columnalign="left" style="text-align: left"><mtext mathvariant="normal">otherwise</mtext></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">w(t) = \begin{cases} 1 &amp; t \le T \\ 0 &amp; \text{otherwise} \end{cases}</annotation></semantics></math> (a&nbsp;step function), respectively. As discussed in Problem&nbsp;<a href="#prob:discrete-data" data-reference-type="ref" data-reference="prob:discrete-data">[prob:discrete-data]</a>, you can let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">w(t)</annotation></semantics></math> be a sum of delta functions to represent data at a sequence of discrete times.</p>
<p>In both cases, since these are scalar-valued functions, for optimization/fitting one would like to know the gradient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><mi>p</mi></msub><mi>g</mi></mrow><annotation encoding="application/x-tex">\nabla_{p}g</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><mi>p</mi></msub><mi>G</mi></mrow><annotation encoding="application/x-tex">\nabla_{p}G</annotation></semantics></math>, such that, as usual, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>+</mo><mi>d</mi><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>∇</mi><mi>p</mi></msub><mi>g</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mi>d</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">g(u(p+dp,t),t)-g(u(p,t),t)=\left(\nabla_{p}g\right)^{T}dp</annotation></semantics></math> so that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>±</mi><msub><mi>∇</mi><mi>p</mi></msub><mi>g</mi></mrow><annotation encoding="application/x-tex">\pm\nabla_{p}g</annotation></semantics></math> is the steepest ascent/descent direction for maximization/minimization of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math>, respectively. It is worth emphasizing that gradients (which we only define for scalar-valued functions) have the same shape as their inputs&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>, so <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><mi>p</mi></msub><mi>g</mi></mrow><annotation encoding="application/x-tex">\nabla_p g</annotation></semantics></math> is a vector of length <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> (the number of parameters) that depends on <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>.</p>
<p>These are “just derivatives,” but probably you can see the difficulty: if we don’t have a formula (explicit solution) for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(p,t)</annotation></semantics></math>, only some numerical software that can crank out numbers for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(p,t)</annotation></semantics></math> given any parameters&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> and&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>, how do we apply differentiation rules to find <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∂</mi><mi>u</mi><mi>/</mi><mi>∂</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">\partial u/\partial p</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><mi>p</mi></msub><mi>g</mi></mrow><annotation encoding="application/x-tex">\nabla_{p}g</annotation></semantics></math>? Of course, we could use finite differences as in Sec.&nbsp;<a href="#sec:finitedifference" data-reference-type="ref" data-reference="sec:finitedifference">4</a>—just crank through numerical solutions for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>+</mo><mi>δ</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">p+\delta p</annotation></semantics></math> and subtract them—but that will be quite slow if we want to differentiate with respect to many parameters (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>≫</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">N\gg1</annotation></semantics></math>), not to mention giving potentially poor accuracy. In fact, people often have <em>huge</em> numbers of parameters inside an ODE that they want to differentiate. Nowadays, our right-hand-side function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>,</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(u,p,t)</annotation></semantics></math> can even contain a <em>neural network</em> (this is called a “neural ODE”) with thousands or millions (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>) of parameters <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>, and we need all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> of these derivatives <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><mi>p</mi></msub><mi>g</mi></mrow><annotation encoding="application/x-tex">\nabla_{p}g</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><mi>p</mi></msub><mi>G</mi></mrow><annotation encoding="application/x-tex">\nabla_{p}G</annotation></semantics></math> to minimize the “loss” function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>. So, not only do we need to find a way to differentiate our ODE solutions (or scalar functions thereof), but these derivatives must be obtained <em>efficiently</em>. It turns out that there are two ways to do this, and both of them hinge on the fact that the derivative is obtained by <em>solving another ODE</em>:</p>
<ul>
<li><p><strong>Forward</strong> mode: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial u}{\partial p}</annotation></semantics></math> turns out to solve <em>another</em> ODE that we can integrate with the same numerical solvers for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math>. This gives us all of the derivatives we could want, but the drawback is that the ODE for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial u}{\partial p}</annotation></semantics></math> is larger by a factor of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> than the original ODE for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math>, so it is only practical for small <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> (few parameters).</p></li>
<li><p><strong>Reverse</strong> (“<strong>adjoint</strong>”) mode: for scalar objectives, it turns out that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><mi>p</mi></msub><mi>g</mi></mrow><annotation encoding="application/x-tex">\nabla_{p}g</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><mi>p</mi></msub><mi>G</mi></mrow><annotation encoding="application/x-tex">\nabla_{p}G</annotation></semantics></math> can be computed by solving a different ODE for an “adjoint” solution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">v(p,t)</annotation></semantics></math> of the <em>same size</em> as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math>, and then computing some simple integrals involving <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> (the “forward” solution) and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math>. This has the advantage of giving us all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> derivatives with only about <em>twice</em> the cost of solving for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math>, regardless of the number <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> of parameters. The disadvantage is that, since it turns out that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> must be integrated “backwards” in time (starting from an “initial” condition at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">t=T</annotation></semantics></math> and working back to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">t=0</annotation></semantics></math>) and depends on <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math>, it is necessary to store <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(p,t)</annotation></semantics></math> for all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>∈</mo><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">t\in[0,T]</annotation></semantics></math> (rather than marching <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> forwards in time and discarding values from previous times when they are no longer needed), which can require a vast amount of computer memory for large ODE systems integrated over long times.</p></li>
</ul>
<p>We will now consider each of these approaches in more detail.</p>
<section id="forward-sensitivity-analysis-of-odes" class="level3">
<h3 class="anchored" data-anchor-id="forward-sensitivity-analysis-of-odes">Forward sensitivity analysis of ODEs</h3>
<p>Let us start with our ODE <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>t</mi></mrow></mfrac><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>,</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\frac{\partial u}{\partial t}=f(u,p,t)</annotation></semantics></math>, and consider what happens to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> for a small change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">dp</annotation></semantics></math> in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>d</mi><munder><munder><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>t</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo accent="true">⏟</mo></munder><mrow><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>,</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></munder></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mi>∂</mi><mrow><mi>∂</mi><mi>t</mi></mrow></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mi>∂</mi><mrow><mi>∂</mi><mi>t</mi></mrow></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>p</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mi>∂</mi><mrow><mi>∂</mi><mi>t</mi></mrow></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>p</mi><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>,</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><mi>u</mi></mrow></mfrac><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo>+</mo><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>p</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>,</mo></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
d\underbrace{\left(\frac{\partial u}{\partial t}\right)}_{=f(u,p,t)} &amp;= \frac{\partial}{\partial t} (du)
=
\frac{\partial}{\partial t}
\left( \frac{\partial u}{\partial p} [ dp ]\right)=\frac{\partial}{\partial t}\left(\frac{\partial u}{\partial p}\right)[dp] \\
&amp;=d(f(u,p,t)) = \left(\frac{\partial f}{\partial u}\frac{\partial u}{\partial p}+\frac{\partial f}{\partial p}\right)[dp],
\end{aligned}</annotation></semantics></math> where we have used the familiar rule (from multivariable calculus) of interchanging the order of partial derivatives—a property that we will re-derive explicitly for our generalized linear-operator derivatives in our lecture on Hessians and second derivatives. Equating the right-hand sides of the two lines, we see that we have an ODE <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><menclose notation="box"><mrow><mfrac><mi>∂</mi><mrow><mi>∂</mi><mi>t</mi></mrow></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><mi>u</mi></mrow></mfrac><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo>+</mo><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac></mrow></menclose><annotation encoding="application/x-tex">\boxed{\frac{\partial}{\partial t}\left(\frac{\partial u}{\partial p}\right)=\frac{\partial f}{\partial u}\frac{\partial u}{\partial p}+\frac{\partial f}{\partial p}}</annotation></semantics></math> for the derivative <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial u}{\partial p}</annotation></semantics></math>, whose initial condition is obtained simply by differentiating the initial condition <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>u</mi><mn>0</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(p,0)=u_{0}(p)</annotation></semantics></math> for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow></msub><mo>=</mo><mfrac><mrow><mi>∂</mi><msub><mi>u</mi><mn>0</mn></msub></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">\left.\frac{\partial u}{\partial p}\right|_{t=0}=\frac{\partial u_{0}}{\partial p}.</annotation></semantics></math> We can therefore plug this into any ODE solver technique (usually numerical methods, unless we are extremely lucky and can solve this ODE analytically for a particular <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>) to find <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial u}{\partial p}</annotation></semantics></math> at any desired time <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>. Simple, right?</p>
<p>The only thing that might seem a little weird here is the <em>shape</em> of the solution: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial u}{\partial p}</annotation></semantics></math> is a linear operator, but how can the solution of an ODE be a linear operator? It turns out that there is nothing wrong with this, but it is helpful to think about a few examples:</p>
<ul>
<li><p>If <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>,</mo><mi>p</mi><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">u,p\in\mathbb{R}</annotation></semantics></math> are scalars (that is, we have a single scalar ODE with a single scalar parameter), then <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial u}{\partial p}</annotation></semantics></math> is just a (time-dependent) number, and our ODE for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial u}{\partial p}</annotation></semantics></math> is an ordinary scalar ODE with an ordinary scalar initial condition.</p></li>
<li><p>If <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">u\in\mathbb{R}^{n}</annotation></semantics></math> (a “system” of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> ODEs) and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">p\in\mathbb{R}</annotation></semantics></math> is a scalar, then <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">\frac{\partial u}{\partial p}\in\mathbb{R}^{n}</annotation></semantics></math> is another column vector and our ODE for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial u}{\partial p}</annotation></semantics></math> is another system of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> ODEs. So, we solve two ODEs of the same size <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> to obtain <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial u}{\partial p}</annotation></semantics></math>.</p></li>
<li><p>If <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">u\in\mathbb{R}^{n}</annotation></semantics></math> (a “system” of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> ODEs) and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>N</mi></msup></mrow><annotation encoding="application/x-tex">p\in\mathbb{R}^{N}</annotation></semantics></math> is a vector of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> parameters, then <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>n</mi><mo>×</mo><mi>N</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\frac{\partial u}{\partial p}\in\mathbb{R}^{n\times N}</annotation></semantics></math> is an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">n\times N</annotation></semantics></math> Jacobian <em>matrix.</em> Our ODE for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial u}{\partial p}</annotation></semantics></math> is effectivly system of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">nN</annotation></semantics></math> ODEs for all the components of this matrix, with a matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><msub><mi>u</mi><mn>0</mn></msub></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial u_{0}}{\partial p}</annotation></semantics></math> of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">nN</annotation></semantics></math> initial conditions! Solving this “matrix ODE” with numerical methods poses no conceptual difficulty, but will generally require about <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> times the computational work of solving for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math>, simply because there are <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> times as many unknowns. This could be expensive if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> is large!</p></li>
</ul>
<p>This reflects our general observation of forward-mode differentiation: it is expensive when the number <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> of “input” parameters being differentiated is large. However, forward mode is straightforward and, especially for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>≲</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">N\lesssim100</annotation></semantics></math> or so, is often the first method to try when differentiating ODE solutions. Given <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial u}{\partial p}</annotation></semantics></math> , one can then straightforwardly differentiate scalar objectives by the chain rule: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><msub><mrow><msub><mi>∇</mi><mi>p</mi></msub><mi>g</mi><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mi>t</mi><mo>=</mo><mi>T</mi></mrow></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msub><mrow><munder><munder><msup><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mi>T</mi></msup><mo accent="true">⏟</mo></munder><msup><mtext mathvariant="normal">Jacobian</mtext><mi>T</mi></msup></munder><munder><munder><msup><mfrac><mrow><mi>∂</mi><mi>g</mi></mrow><mrow><mi>∂</mi><mi>u</mi></mrow></mfrac><mi>T</mi></msup><mo accent="true">⏟</mo></munder><mtext mathvariant="normal">vector</mtext></munder><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mi>t</mi><mo>=</mo><mi>T</mi></mrow></msub><mo>,</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>∇</mi><mi>p</mi></msub><mi>G</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mi>T</mi></msubsup><msub><mi>∇</mi><mi>p</mi></msub><mi>g</mi><mspace width="0.167em"></mspace><mi>d</mi><mi>t</mi><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
\left.\nabla_{p}g\right|_{t=T} &amp; =\left.\underbrace{\frac{\partial u}{\partial p}^{T}}_{\text{Jacobian}^{T}}\underbrace{\frac{\partial g}{\partial u}^{T}}_{\text{vector}}\right|_{t=T},\\
\nabla_{p}G &amp; =\int_{0}^{T}\nabla_{p}g\,dt.
\end{aligned}</annotation></semantics></math> The left-hand side <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><mi>p</mi></msub><mi>G</mi></mrow><annotation encoding="application/x-tex">\nabla_p G</annotation></semantics></math> is gradient of a scalar function of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> parameters, and hence the gradient is a vector of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> components. Correspondingly, the right-hand side is an integral of an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>-component gradient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><mi>p</mi></msub><mi>g</mi></mrow><annotation encoding="application/x-tex">\nabla_p g</annotation></semantics></math> as well, and the integral of a vector-valued function can be viewed as simply the elementwise integral (the vector of integrals of each component).</p>
</section>
<section id="reverseadjoint-sensitivity-analysis-of-odes" class="level3">
<h3 class="anchored" data-anchor-id="reverseadjoint-sensitivity-analysis-of-odes">Reverse/adjoint sensitivity analysis of ODEs</h3>
<p>For large <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>≫</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">N\gg1</annotation></semantics></math> and scalar objectives <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>g</mi><annotation encoding="application/x-tex">g</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> (etc.), we can in principle compute derivatives <em>much</em> more efficiently, with about the same cost as computing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math>, by applying a “reverse-mode” or “adjoint” approach. In other lectures, we’ve obtained analogous reverse-mode methods simply by evaluating the chain rule left-to-right (outputs-to-inputs) instead of right-to-left. Conceptually, the process for ODEs is similar,<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> but algebraically the derivation is rather trickier and less direct. The key thing is that, if possible, we want to avoid computing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial u}{\partial p}</annotation></semantics></math> explicitly, since this could be a prohibitively large Jacobian matrix if we have many parameters (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> is large), especially if we have many equations (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> is large).</p>
<p>In particular, let’s start with our forward-mode sensitivity analysis, and consider the derivative <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mi>′</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>∇</mi><mi>p</mi></msub><mi>G</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">G'=(\nabla_{p}G)^{T}</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> is the integral of a time-varying objective <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>,</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(u,p,t)</annotation></semantics></math> (which we allow to depend explicitly on <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> for generality). By the chain rule, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mi>′</mi><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mi>T</mi></msubsup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>g</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo>+</mo><mfrac><mrow><mi>∂</mi><mi>g</mi></mrow><mrow><mi>∂</mi><mi>u</mi></mrow></mfrac><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>t</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">G'=\int_{0}^{T}\left(\frac{\partial g}{\partial p}+\frac{\partial g}{\partial u}\frac{\partial u}{\partial p}\right)dt,</annotation></semantics></math> which involves our unwanted factor <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial u}{\partial p}</annotation></semantics></math>. To get rid of this, we’re going to use a “weird trick”</p>
<p>(much like Lagrange multipliers) of adding <em>zero</em> to this equation: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mi>′</mi><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mi>T</mi></msubsup><mrow><mo stretchy="true" form="prefix">[</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>g</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo>+</mo><mfrac><mrow><mi>∂</mi><mi>g</mi></mrow><mrow><mi>∂</mi><mi>u</mi></mrow></mfrac><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msup><mi>v</mi><mi>T</mi></msup><munder><munder><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mi>∂</mi><mrow><mi>∂</mi><mi>t</mi></mrow></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><mi>u</mi></mrow></mfrac><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo>−</mo><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo accent="true">⏟</mo></munder><mrow><mo>=</mo><mn>0</mn></mrow></munder><mo stretchy="true" form="postfix">]</mo></mrow><mi>d</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">G'=\int_{0}^{T}\left[\left(\frac{\partial g}{\partial p}+\frac{\partial g}{\partial u}\frac{\partial u}{\partial p}\right)+v^{T}\underbrace{\left(\frac{\partial}{\partial t}\left(\frac{\partial u}{\partial p}\right)-\frac{\partial f}{\partial u}\frac{\partial u}{\partial p}-\frac{\partial f}{\partial p}\right)}_{=0}\right]dt</annotation></semantics></math> for some function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">v(t)</annotation></semantics></math> of the <em>same shape as u</em> that multiplies our “forward-mode” equation for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∂</mi><mi>u</mi><mi>/</mi><mi>∂</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">\partial u/ \partial p</annotation></semantics></math>. (If <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">u\in\mathbb{R}^{n}</annotation></semantics></math> then <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">v\in\mathbb{R}^{n}</annotation></semantics></math>; more generally, for other vector spaces, read <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>v</mi><mi>T</mi></msup><annotation encoding="application/x-tex">v^{T}</annotation></semantics></math> as an inner product with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math>.) The new term <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>v</mi><mi>T</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>⋯</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">v^T (\cdots)</annotation></semantics></math> is zero because the parenthesized expression is precisely the ODE satisfied by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial u}{\partial p}</annotation></semantics></math>, as obtained in our forward-mode analysis above, <em>regardless</em> of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">v(t)</annotation></semantics></math>. This is important because it allows us the freedom to <em>choose</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">v(t)</annotation></semantics></math> to <em>cancel</em> the unwanted <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial u}{\partial p}</annotation></semantics></math> term. In particular, if we first <em>integrate by parts</em> on the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>v</mi><mi>T</mi></msup><mfrac><mi>∂</mi><mrow><mi>∂</mi><mi>t</mi></mrow></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">v^{T}\frac{\partial}{\partial t}\left(\frac{\partial u}{\partial p}\right)</annotation></semantics></math> term to change it to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>−</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>v</mi></mrow><mrow><mi>∂</mi><mi>t</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">-\left(\frac{\partial v}{\partial t}\right)^{T}\frac{\partial u}{\partial p}</annotation></semantics></math> plus a boundary term, then re-group the terms, we find: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mi>′</mi><mo>=</mo><msubsup><mrow><msup><mi>v</mi><mi>T</mi></msup><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo stretchy="true" form="postfix">|</mo></mrow><mn>0</mn><mi>T</mi></msubsup><mo>+</mo><msubsup><mo>∫</mo><mn>0</mn><mi>T</mi></msubsup><mrow><mo stretchy="true" form="prefix">[</mo><mfrac><mrow><mi>∂</mi><mi>g</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo>−</mo><msup><mi>v</mi><mi>T</mi></msup><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo>+</mo><munder><munder><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>g</mi></mrow><mrow><mi>∂</mi><mi>u</mi></mrow></mfrac><mo>−</mo><msup><mi>v</mi><mi>T</mi></msup><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><mi>u</mi></mrow></mfrac><mo>−</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>v</mi></mrow><mrow><mi>∂</mi><mi>t</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo accent="true">⏟</mo></munder><mtext mathvariant="normal">want to be zero!</mtext></munder><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo stretchy="true" form="postfix">]</mo></mrow><mi>d</mi><mi>t</mi><mspace width="0.222em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">G'=\left.v^{T}\frac{\partial u}{\partial p}\right|_{0}^{T}+\int_{0}^{T}\left[\frac{\partial g}{\partial p}-v^{T}\frac{\partial f}{\partial p}+\underbrace{\left(\frac{\partial g}{\partial u}-v^{T}\frac{\partial f}{\partial u}-\left(\frac{\partial v}{\partial t}\right)^{T}\right)}_{\text{want to be zero!}}\frac{\partial u}{\partial p}\right]dt\:.</annotation></semantics></math> If we could now set the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>⋯</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(\cdots)</annotation></semantics></math> term to zero, then the unwanted <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial u}{\partial p}</annotation></semantics></math> would vanish from the integral calculation in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">G'</annotation></semantics></math>. We can accomplish this by <em>choosing</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">v(t)</annotation></semantics></math> (which could be <em>anything</em> up to now) to satisfy the <strong>“adjoint” ODE</strong>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><menclose notation="box"><mrow><mfrac><mrow><mi>∂</mi><mi>v</mi></mrow><mrow><mi>∂</mi><mi>t</mi></mrow></mfrac><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>g</mi></mrow><mrow><mi>∂</mi><mi>u</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mo>−</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><mi>u</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mi>v</mi></mrow></menclose><mi>.</mi></mrow><annotation encoding="application/x-tex">\boxed{\frac{\partial v}{\partial t}=\left(\frac{\partial g}{\partial u}\right)^{T} - \left(\frac{\partial f}{\partial u}\right)^{T}v}.</annotation></semantics></math> What initial condition should we choose for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">v(t)</annotation></semantics></math>? Well, we can use this choice to get rid of the boundary term we obtained above from integration by parts: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mrow><msup><mi>v</mi><mi>T</mi></msup><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo stretchy="true" form="postfix">|</mo></mrow><mn>0</mn><mi>T</mi></msubsup><mo>=</mo><mi>v</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>T</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><munder><munder><msub><mrow><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo stretchy="true" form="postfix">|</mo></mrow><mi>T</mi></msub><mo accent="true">⏟</mo></munder><mtext mathvariant="normal">unknown</mtext></munder><mo>−</mo><mi>v</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><munder><munder><mfrac><mrow><mi>∂</mi><msub><mi>u</mi><mn>0</mn></msub></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo accent="true">⏟</mo></munder><mtext mathvariant="normal">known</mtext></munder><mi>.</mi></mrow><annotation encoding="application/x-tex">\left.v^{T}\frac{\partial u}{\partial p}\right|_{0}^{T}=v(T)^{T}\underbrace{\left.\frac{\partial u}{\partial p}\right|_{T}}_{\text{unknown}}-v(0)^{T}\underbrace{\frac{\partial u_{0}}{\partial p}}_{\text{known}}.</annotation></semantics></math> Here, the unknown <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mrow><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo stretchy="true" form="postfix">|</mo></mrow><mi>T</mi></msub><annotation encoding="application/x-tex">\left.\frac{\partial u}{\partial p}\right|_{T}</annotation></semantics></math> term is a problem—to compute that, we would be forced to go back to integrating our big <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{\partial u}{\partial p}</annotation></semantics></math> ODE from forward mode. The other term is okay: since the initial condition <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>u</mi><mn>0</mn></msub><annotation encoding="application/x-tex">u_{0}</annotation></semantics></math> is always given, we should know its dependence on <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> explicitly (and we will simply have <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>∂</mi><msub><mi>u</mi><mn>0</mn></msub></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\frac{\partial u_{0}}{\partial p}=0</annotation></semantics></math> in the common case where the initial conditions don’t depend on <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>). To eliminate the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mrow><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo stretchy="true" form="postfix">|</mo></mrow><mi>T</mi></msub><annotation encoding="application/x-tex">\left.\frac{\partial u}{\partial p}\right|_{T}</annotation></semantics></math> term, therefore, we make the choice <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><menclose notation="box"><mrow><mi>v</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>T</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn></mrow></menclose><mi>.</mi></mrow><annotation encoding="application/x-tex">\boxed{v(T)=0}.</annotation></semantics></math> Instead of an <em>initial</em> condition, our adjoint ODE has a <strong>final condition</strong>. That’s no problem for a numerical solver: it just means that the <strong>adjoint ODE is integrated</strong> <strong><em>backwards</em></strong> <strong>in time</strong>, starting from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">t=T</annotation></semantics></math> and working down to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">t=0</annotation></semantics></math>. Once we have solved the adjoint ODE for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">v(t)</annotation></semantics></math>, we can plug it into our equation for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">G'</annotation></semantics></math> to obtain our gradient by a simple integral: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><mi>p</mi></msub><mi>G</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>G</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mo>=</mo><mi>−</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><msub><mi>u</mi><mn>0</mn></msub></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mi>v</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msubsup><mo>∫</mo><mn>0</mn><mi>T</mi></msubsup><mrow><mo stretchy="true" form="prefix">[</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>g</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mo>−</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mi>v</mi><mo stretchy="true" form="postfix">]</mo></mrow><mi>d</mi><mi>t</mi><mspace width="0.222em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">\nabla_{p}G=\left(G'\right)^{T}=-\left(\frac{\partial u_{0}}{\partial p}\right)^{T}v(0)+\int_{0}^{T}\left[\left(\frac{\partial g}{\partial p}\right)^{T}-\left(\frac{\partial f}{\partial p}\right)^{T}v\right]dt\:.</annotation></semantics></math> (If you want to be fancy, you can compute this <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mo>∫</mo><mn>0</mn><mi>T</mi></msubsup><annotation encoding="application/x-tex">\int_{0}^{T}</annotation></semantics></math> simultaneously with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> itself, by augmenting the adjoint ODE with an additional set of unknowns and equations representing the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">G'</annotation></semantics></math> integrand. But that’s mainly just a computational convenience and doesn’t change anything fundamental about the process.)</p>
<p>The only remaining annoyance is that the adjoint ODE depends on <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(p,t)</annotation></semantics></math> for all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>∈</mo><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">t\in[0,T]</annotation></semantics></math>. Normally, if we are solving the “forward” ODE for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(p,t)</annotation></semantics></math> numerically, we can “march” the solution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> forwards in time and only store the solution at a few of the most recent timesteps. Since the adjoint ODE starts at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">t=T</annotation></semantics></math>, however, we can only start integrating <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> after we have completed the calculation of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math>. This requires us to save essentially <em>all</em> of our previously computed <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(p,t)</annotation></semantics></math> values, so that we can evaluate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> at arbitrary times <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>∈</mo><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">t\in[0,T]</annotation></semantics></math> during the integration of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> (and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">G'</annotation></semantics></math>). This can require a lot of computer memory if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> is large (e.g.&nbsp;it could represent <em>millions</em> of grid points from a spatially discretized PDE, such as in a heat-diffusion problem) and many timesteps <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> were required. To ameliorate this challenge, a variety of strategies have been employed, typically centered around “checkpointing” techniques in which <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> is only saved at a subset of times <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>, and its value at other times is obtained during the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> integration by <em>re-computing</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> as needed (numerically integrating the ODE starting at the closest “checkpoint” time). A detailed discussion of such techniques lies outside the scope of these notes, however.</p>
</section>
</section>
<section id="example" class="level2">
<h2 class="anchored" data-anchor-id="example">Example</h2>
<p>Let us illustrate the above techniques with a simple example. Suppose that we are integrating the scalar ODE <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>t</mi></mrow></mfrac><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>,</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>p</mi><mn>1</mn></msub><mo>+</mo><msub><mi>p</mi><mn>2</mn></msub><mi>u</mi><mo>+</mo><msub><mi>p</mi><mn>3</mn></msub><msup><mi>u</mi><mn>2</mn></msup><mo>=</mo><msup><mi>p</mi><mi>T</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>u</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msup><mi>u</mi><mn>2</mn></msup></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\frac{\partial u}{\partial t}=f(u,p,t)=p_{1}+p_{2}u+p_{3}u^{2}=p^{T}\left(\begin{array}{c}
1\\
u\\
u^{2}
\end{array}\right)</annotation></semantics></math> for an initial condition <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>u</mi><mn>0</mn></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">u(p,0)=u_{0}=0</annotation></semantics></math> and three parameters <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>∈</mo><msup><mi>ℝ</mi><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">p \in \mathbb{R}^3</annotation></semantics></math>. (This is probably simple enough to solve in closed form, but we won’t bother with that here.) We will also consider the scalar function <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mi>T</mi></msubsup><munder><munder><msup><mrow><mo stretchy="true" form="prefix">[</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msub><mi>u</mi><mo>*</mo></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mn>2</mn></msup><mo accent="true">⏟</mo></munder><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>,</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></munder><mi>d</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">G(p)=\int_{0}^{T}\underbrace{\left[u(p,t)-u_{*}(t)\right]^{2}}_{g(u,p,t)}dt</annotation></semantics></math> that (for example) we may want to minimize for some given <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>u</mi><mo>*</mo></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u_{*}(t)</annotation></semantics></math> (e.g.&nbsp;experimental data or some given formula like <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>u</mi><mo>*</mo></msub><mo>=</mo><msup><mi>t</mi><mn>3</mn></msup></mrow><annotation encoding="application/x-tex">u_{*}=t^{3}</annotation></semantics></math>), so we are hoping to compute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><mi>p</mi></msub><mi>G</mi></mrow><annotation encoding="application/x-tex">\nabla_{p}G</annotation></semantics></math>.</p>
<section id="forward-mode" class="level3">
<h3 class="anchored" data-anchor-id="forward-mode">Forward mode</h3>
<p>The Jacobian matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><msub><mi>p</mi><mn>1</mn></msub></mrow></mfrac></mtd><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><msub><mi>p</mi><mn>2</mn></msub></mrow></mfrac></mtd><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><msub><mi>p</mi><mn>3</mn></msub></mrow></mfrac></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\frac{\partial u}{\partial p}=\left(\begin{array}{ccc}
\frac{\partial u}{\partial p_{1}} &amp; \frac{\partial u}{\partial p_{2}} &amp; \frac{\partial u}{\partial p_{3}}\end{array}\right)</annotation></semantics></math> is simply a row vector, and satisfies our “forward-mode” ODE: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>∂</mi><mrow><mi>∂</mi><mi>t</mi></mrow></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><mi>u</mi></mrow></mfrac><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo>+</mo><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>p</mi><mn>2</mn></msub><mo>+</mo><mn>2</mn><msub><mi>p</mi><mn>3</mn></msub><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd><mtd columnalign="center" style="text-align: center"><mi>u</mi></mtd><mtd columnalign="center" style="text-align: center"><msup><mi>u</mi><mn>2</mn></msup></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\frac{\partial}{\partial t}\left(\frac{\partial u}{\partial p}\right)=\frac{\partial f}{\partial u}\frac{\partial u}{\partial p}+\frac{\partial f}{\partial p}=\left(p_{2}+2p_{3}u\right)\frac{\partial u}{\partial p}+\left(\begin{array}{ccc}
1 &amp; u &amp; u^{2}\end{array}\right)</annotation></semantics></math> for the initial condition <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow></msub><mo>=</mo><mfrac><mrow><mi>∂</mi><msub><mi>u</mi><mn>0</mn></msub></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\left.\frac{\partial u}{\partial p}\right|_{t=0}=\frac{\partial u_{0}}{\partial p}=0</annotation></semantics></math>. This is an inhomogeneous system of three coupled <em>linear</em> ODEs, which might look more conventional if we simply transpose both sides: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>∂</mi><mrow><mi>∂</mi><mi>t</mi></mrow></mfrac><munder><munder><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><msub><mi>p</mi><mn>1</mn></msub></mrow></mfrac></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><msub><mi>p</mi><mn>2</mn></msub></mrow></mfrac></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><msub><mi>p</mi><mn>3</mn></msub></mrow></mfrac></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo accent="true">⏟</mo></munder><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>∂</mi><mi>u</mi><mi>/</mi><mi>∂</mi><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup></munder><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>p</mi><mn>2</mn></msub><mo>+</mo><mn>2</mn><msub><mi>p</mi><mn>3</mn></msub><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><msub><mi>p</mi><mn>1</mn></msub></mrow></mfrac></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><msub><mi>p</mi><mn>2</mn></msub></mrow></mfrac></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><msub><mi>p</mi><mn>3</mn></msub></mrow></mfrac></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>u</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msup><mi>u</mi><mn>2</mn></msup></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\frac{\partial}{\partial t}\underbrace{\left(\begin{array}{c}
\frac{\partial u}{\partial p_{1}}\\
\frac{\partial u}{\partial p_{2}}\\
\frac{\partial u}{\partial p_{3}}
\end{array}\right)}_{(\partial u/\partial p)^{T}}=\left(p_{2}+2p_{3}u\right)\left(\begin{array}{c}
\frac{\partial u}{\partial p_{1}}\\
\frac{\partial u}{\partial p_{2}}\\
\frac{\partial u}{\partial p_{3}}
\end{array}\right)+\left(\begin{array}{c}
1\\
u\\
u^{2}
\end{array}\right).</annotation></semantics></math> The fact that this depends on our “forward” solution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(p,t)</annotation></semantics></math> makes it not so easy to solve by hand, but a computer can solve it numerically with no difficulty. On a computer, we would probably solve for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∂</mi><mi>u</mi><mi>/</mi><mi>∂</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">\partial u/ \partial p</annotation></semantics></math><em>simultaneously</em> by combining the two ODEs into a single ODE with&nbsp;4 components: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>∂</mi><mrow><mi>∂</mi><mi>t</mi></mrow></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>u</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>∂</mi><mi>u</mi><mi>/</mi><mi>∂</mi><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>p</mi><mn>1</mn></msub><mo>+</mo><msub><mi>p</mi><mn>2</mn></msub><mi>u</mi><mo>+</mo><msub><mi>p</mi><mn>3</mn></msub><msup><mi>u</mi><mn>2</mn></msup></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>p</mi><mn>2</mn></msub><mo>+</mo><mn>2</mn><msub><mi>p</mi><mn>3</mn></msub><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>∂</mi><mi>u</mi><mi>/</mi><mi>∂</mi><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>u</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msup><mi>u</mi><mn>2</mn></msup></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\frac{\partial}{\partial t}\left(\begin{array}{c}
u \\
(\partial u/\partial p)^{T}
\end{array}\right) =
\left(\begin{array}{c}
p_{1}+p_{2}u+p_{3}u^{2} \\
\left(p_{2}+2p_{3}u\right) (\partial u/\partial p)^{T} +\left(\begin{array}{c}
1\\
u\\
u^{2}
\end{array}\right)
\end{array}\right).</annotation></semantics></math> Given <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∂</mi><mi>u</mi><mi>/</mi><mi>∂</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">\partial u/\partial p</annotation></semantics></math>, we can then plug this into the chain rule for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><mi>p</mi></msub><mi>G</mi><mo>=</mo><mn>2</mn><msubsup><mo>∫</mo><mn>0</mn><mi>T</mi></msubsup><mrow><mo stretchy="true" form="prefix">[</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msub><mi>u</mi><mo>*</mo></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><msup><mfrac><mrow><mi>∂</mi><mi>u</mi></mrow><mrow><mi>∂</mi><mi>p</mi></mrow></mfrac><mi>T</mi></msup><mspace width="0.167em"></mspace><mi>d</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">\nabla_{p}G=2\int_{0}^{T}\left[u(p,t)-u_{*}(t)\right]\frac{\partial u}{\partial p}^{T}\,dt</annotation></semantics></math> (again, an integral that a computer could evaluate numerically).</p>
</section>
<section id="reverse-mode" class="level3">
<h3 class="anchored" data-anchor-id="reverse-mode">Reverse mode</h3>
<p>In reverse mode, we have an adjoint solution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">v(t)\in\mathbb{R}</annotation></semantics></math> (the same shape as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math>) which solves our adjoint equation <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>∂</mi><mi>v</mi></mrow><mrow><mi>d</mi><mi>t</mi></mrow></mfrac><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>g</mi></mrow><mrow><mi>∂</mi><mi>u</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mo>−</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><mi>u</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mi>v</mi><mo>=</mo><mn>2</mn><mrow><mo stretchy="true" form="prefix">[</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msub><mi>u</mi><mo>*</mo></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>−</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>p</mi><mn>2</mn></msub><mo>+</mo><mn>2</mn><msub><mi>p</mi><mn>3</mn></msub><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">\frac{\partial v}{dt}=\left(\frac{\partial g}{\partial u}\right)^{T} -\left(\frac{\partial f}{\partial u}\right)^{T}v =2\left[u(p,t)-u_{*}(t)\right] - \left(p_{2}+2p_{3}u\right)v</annotation></semantics></math> with a <em>final</em> condition <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>T</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn><mi>.</mi></mrow><annotation encoding="application/x-tex">v(T)=0.</annotation></semantics></math> Again, a computer can solve this numerically without difficulty (given the numerical “forward” solution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math>) to find <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">v(t)</annotation></semantics></math> for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>∈</mo><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">t\in[0,T]</annotation></semantics></math>. Finally, our gradient is the integrated product: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><mi>p</mi></msub><mi>G</mi><mo>=</mo><mi>−</mi><msubsup><mo>∫</mo><mn>0</mn><mi>T</mi></msubsup><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>1</mn></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>u</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msup><mi>u</mi><mn>2</mn></msup></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mi>v</mi><mspace width="0.167em"></mspace><mi>d</mi><mi>t</mi><mspace width="0.222em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">\nabla_{p}G = -\int_{0}^{T}
\left(
\begin{array}{c} 1 \\ u \\ u^{2} \end{array}
\right)
v\,dt\:.</annotation></semantics></math></p>
<p>Another useful exercise is to consider a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> that takes the form of a summation:</p>
<div class="problem">
<p><span id="prob:discrete-data" data-label="prob:discrete-data"></span> Suppose that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">G(p)</annotation></semantics></math> takes the form of a sum of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math> terms: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>K</mi></munderover><msub><mi>g</mi><mi>k</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><msub><mi>t</mi><mi>k</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">G(p) = \sum_{k=1}^{K} g_k(p,u(p,t_k))</annotation></semantics></math> for times <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>t</mi><mi>k</mi></msub><mo>∈</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mi>T</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">t_k \in (0, T)</annotation></semantics></math> and functions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mi>k</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g_k(p,u)</annotation></semantics></math>. For example, this could arise in least-square fitting of experimental data <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>u</mi><mo>*</mo></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>t</mi><mi>k</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u_*(t_k)</annotation></semantics></math> at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math> discrete times, with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mi>k</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><msub><mi>t</mi><mi>k</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo stretchy="false" form="postfix">‖</mo><msub><mi>u</mi><mo>*</mo></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>t</mi><mi>k</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><msub><mi>t</mi><mi>k</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><msup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">g_k(u(p,t_k)) = \Vert u_*(t_k) -u(p,t_k)\Vert^2</annotation></semantics></math> measuring the squared difference between <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>,</mo><msub><mi>t</mi><mi>k</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(p,t_k)</annotation></semantics></math> and the measured data at time <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>t</mi><mi>k</mi></msub><annotation encoding="application/x-tex">t_k</annotation></semantics></math>.</p>
<ol type="1">
<li><p>Show that such a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">G(p)</annotation></semantics></math> can be expressed as a special case of our formulation in this chapter, by defining our function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(u,t)</annotation></semantics></math> using a sum of Dirac delta functions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo>−</mo><msub><mi>t</mi><mi>k</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\delta(t - t_k)</annotation></semantics></math>.</p></li>
<li><p>Explain how this affects the adjoint solution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">v(t)</annotation></semantics></math>: in particular, how the introduction of delta-function terms on the right-hand side of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>v</mi><mi>/</mi><mi>d</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">dv/dt</annotation></semantics></math> causes <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">v(t)</annotation></semantics></math> to have a sequence of discontinuous jumps. (In several popular numerical ODE solvers, such discontinuities can be incorporated via discrete-time “callbacks”.)</p></li>
<li><p>Explain how these delta functions may also introduce a summation into the computation of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><mi>p</mi></msub><mi>G</mi></mrow><annotation encoding="application/x-tex">\nabla_p G</annotation></semantics></math>, but only if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>g</mi><mi>k</mi></msub><annotation encoding="application/x-tex">g_k</annotation></semantics></math> depends explicitly on&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> (not just via&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math>).</p></li>
</ol>
</div>
</section>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further reading</h2>
<p>A classic reference on reverse/adjoint differentiation of ODEs (and generalizations thereof), using notation similar to that used today (except that the adjoint solution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> is denoted <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\lambda(t)</annotation></semantics></math>, in an homage to Lagrange multipliers), is Cao&nbsp;et&nbsp;al.&nbsp;(2003) (<a href="https://doi.org/10.1137/S1064827501380630" class="uri">https://doi.org/10.1137/S1064827501380630</a>), and a more recent review article is Sapienza&nbsp;et&nbsp;al.&nbsp;(2024) (<a href="https://arxiv.org/abs/2406.09699" class="uri">https://arxiv.org/abs/2406.09699</a>). See also the SciMLSensitivity.jl package (<a href="https://github.com/SciML/SciMLSensitivity.jl" class="uri">https://github.com/SciML/SciMLSensitivity.jl</a>) for sensitivity analysis with Chris Rackauckas’s amazing DifferentialEquations.jl software suite for numerical solution of ODEs in Julia. There is a nice 2021 YouTube lecture on adjoint sensitivity of ODEs (<a href="https://youtu.be/k6s2G5MZv-I" class="uri">https://youtu.be/k6s2G5MZv-I</a>), again using a similar notation. A discrete version of this process arises for recurrence relations, in which case one obtains a reverse-order “adjoint” recurrence relation as described in MIT course notes by S.&nbsp;G.&nbsp;Johnson (<a href="https://math.mit.edu/~stevenj/18.336/recurrence2.pdf" class="uri">https://math.mit.edu/~stevenj/18.336/recurrence2.pdf</a>).</p>
<p>The differentiation methods in this chapter (e.g.&nbsp;for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∂</mi><mi>u</mi><mi>/</mi><mi>∂</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">\partial u/\partial p</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><mi>p</mi></msub><mi>G</mi></mrow><annotation encoding="application/x-tex">\nabla_p G</annotation></semantics></math>) are derived assuming that the ODEs are solved exactly: given the exact ODE for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math>, we derived an exact ODE for the derivative. On a computer, you will solve these forward and adjoint ODEs approximately, and in consequence the resulting derivatives will only be approximately correct (to the tolerance specified by your ODE solver). This is known as a <strong>differentiate-then-discretize</strong> approach, which has the advantage of simplicity (it is independent of the numerical solution scheme) at the expense of slight inaccuracy (your approximate derivative will not exactly predict the first-order change in your approximate solution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math>). The alternative is a <strong>discretize-then-differentiate</strong> approach, in which you first approximate (“discretize”) your ODE into a discrete-time recurrence formula, and then <em>exactly</em> differentiate the recurrence. This has the advantage of exactly differentiating your approximate solution, at the expense of complexity (the derivation is specific to your discretization scheme). Various authors discuss these tradeoffs and their implications, e.g.&nbsp;in chapter&nbsp;4 of M.&nbsp;D.&nbsp;Gunzburger’s <a href="https://doi.org/10.1137/1.9780898718720.ch4"><em>Perspectives in Flow Control and Optimization</em></a> (2002) or in papers like <a href="https://dl.acm.org/doi/10.1007/s00158-013-1024-4">Jensen&nbsp;et&nbsp;al.&nbsp;(2014)</a>.</p>
</section>
</section>
<section id="calculus-of-variations" class="level1">
<h1>Calculus of Variations</h1>
<p>In this lecture, we will apply our derivative machinery to a new type of input: neither scalars, nor column vectors, nor matrices, but rather the <strong>inputs will be functions</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(x)</annotation></semantics></math>, which form a perfectly good vector space (and can even have norms and inner products).<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> It turns out that there are lots of amazing applications for differentiating with respect to <em>functions</em>, and the resulting techniques are sometimes called the “calculus of variations” and/or “Frechét” derivatives.</p>
<section id="functionals-mapping-functions-to-scalars" class="level2">
<h2 class="anchored" data-anchor-id="functionals-mapping-functions-to-scalars">Functionals: Mapping functions to scalars</h2>
<div class="example">
<p>For example, consider functions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(x)</annotation></semantics></math> that map <span class="math inline">$x\in [0,1] \to u(x) \in \R$</span>. We may then define the function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mn>1</mn></msubsup><mo>sin</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi mathvariant="normal">d</mi><mi>x</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">f(u) = \int_0^1 \sin (u(x)) \,\mathrm{d} x.</annotation></semantics></math> Such a function, mapping an input <em>function</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> to an output <em>number</em>, is sometimes called a “functional.” What is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> in this case?</p>
</div>
<p>Recall that, given any function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>, we always define the derivative as a linear operator <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(u)</annotation></semantics></math> via the equation: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>+</mo><mi>d</mi><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>u</mi><mo stretchy="true" form="postfix">]</mo></mrow><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">df = f (u + du ) - f(u) = f'(u) [du] \, ,</annotation></semantics></math> where now <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>u</mi></mrow><annotation encoding="application/x-tex">du</annotation></semantics></math> denotes an arbitrary “small-valued” <em>function</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">du(x)</annotation></semantics></math> that represents a small change in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(x)</annotation></semantics></math>, as depicted in Fig.&nbsp;<a href="#fig:du" data-reference-type="ref" data-reference="fig:du">7</a> for the analogous case of a non-infinitesimal <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\delta u(x)</annotation></semantics></math>. Here, we may compute this via linearization of the integrand: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>d</mi><mi>f</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>+</mo><mi>d</mi><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mn>1</mn></msubsup><mo>sin</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>d</mi><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mo>sin</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>d</mi><mi>x</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mn>1</mn></msubsup><mo>cos</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>d</mi><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>d</mi><mi>x</mi><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>u</mi><mo stretchy="true" form="postfix">]</mo></mrow><mspace width="0.167em"></mspace><mo>,</mo></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    df &amp;= f(u+ du) - f(u) \\ 
     &amp;=\int_0^1 \sin (u(x) + du(x)) - \sin (u(x)) \, dx \\
     &amp;= \int_0^1 \cos (u(x)) \, du(x) \, dx = f'(u) [du] \, ,
\end{aligned}</annotation></semantics></math> where in the last step we took <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">du(x)</annotation></semantics></math> to be arbitrarily small<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> so that we could linearize <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>sin</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>+</mo><mi>d</mi><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\sin(u + du)</annotation></semantics></math> to first-order in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">du(x)</annotation></semantics></math>. That’s it, we have our derivative <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(u)</annotation></semantics></math> as a perfectly good linear operation acting on <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>u</mi></mrow><annotation encoding="application/x-tex">du</annotation></semantics></math>!</p>
<div id="fig:du" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/du.pdf.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>If our <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(u)</annotation></semantics></math>’s inputs <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> are <em>functions</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(x)</annotation></semantics></math> (e.g., mapping <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">]</mo></mrow><mo>↦</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">[0,1] \mapsto \mathbb{R}</annotation></semantics></math>), then the essence of differentiation is linearizing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> for small perturbations <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\delta u(x)</annotation></semantics></math> that are themselves functions, in the limit where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\delta u(x)</annotation></semantics></math> becomes arbitrarily small. Here, we show an example of a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(x)</annotation></semantics></math> and a perturbation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>δ</mi><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(x)+\delta u(x)</annotation></semantics></math>.</figcaption>
</figure>
</div>
</section>
<section id="inner-products-of-functions" class="level2">
<h2 class="anchored" data-anchor-id="inner-products-of-functions">Inner products of functions</h2>
<p>In order to define a gradient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> when studying such “functionals” (maps from functions to <span class="math inline">$\R$</span>), it is natural to ask if there is an inner product on the input space. In fact, there are perfectly good ways to define inner products of functions! Given functions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>v</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(x), v(x)</annotation></semantics></math> defined on <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">x\in [0,1]</annotation></semantics></math>, we could define a “Euclidean” inner product: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy="false" form="postfix">⟩</mo><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mn>1</mn></msubsup><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>v</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi mathvariant="normal">d</mi><mi>x</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">\langle u, v \rangle = \int_0^1 u(x) v(x) \,\mathrm dx.</annotation></semantics></math> Notice that this implies <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">‖</mo><mi>u</mi><mo stretchy="true" form="postfix">‖</mo></mrow><mo>:=</mo><msqrt><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>u</mi><mo>,</mo><mi>u</mi><mo stretchy="false" form="postfix">⟩</mo></mrow></msqrt><mo>=</mo><msqrt><mrow><msubsup><mo>∫</mo><mn>0</mn><mn>1</mn></msubsup><mi>u</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mi>d</mi><mi>x</mi></mrow></msqrt><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">\lVert u \rVert := \sqrt{\langle u, u \rangle} = \sqrt{\int_0^1 u(x)^2 dx} \, .</annotation></semantics></math></p>
<p>Recall that the gradient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> is <em>defined</em> as whatever we take the inner product of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>u</mi></mrow><annotation encoding="application/x-tex">du</annotation></semantics></math> with to obtain <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">df</annotation></semantics></math>. Therefore, we obtain the gradient as follows: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>u</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mn>1</mn></msubsup><mo>cos</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>d</mi><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>d</mi><mi>x</mi><mo>=</mo><mo stretchy="false" form="prefix">⟨</mo><mi>∇</mi><mi>f</mi><mo>,</mo><mi>d</mi><mi>u</mi><mo stretchy="false" form="postfix">⟩</mo><mo>⟹</mo><mi>∇</mi><mi>f</mi><mo>=</mo><mo>cos</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">df = f'(u)[du]  = \int_0^1 \cos(u(x)) \,du(x)\, dx  = \langle \nabla f, du \rangle \implies \nabla f = \cos (u(x)) \, .</annotation></semantics></math> The two infinitesimals <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>u</mi></mrow><annotation encoding="application/x-tex">du</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> may seem a bit disconcerting, but if this is confusing you can just think of the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">du(x)</annotation></semantics></math> as a small non-infinitesimal function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\delta u(x)</annotation></semantics></math> (as in Fig.&nbsp;<a href="#fig:du" data-reference-type="ref" data-reference="fig:du">7</a>) for which we are dropping higher-order terms.</p>
<p>The gradient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> is just another function, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>cos</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\cos(u(x))</annotation></semantics></math>! As usual, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> has the same “shape” as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math>.</p>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>It might be instructive here to compare the gradient of an integral, above, with a discretized version where the integral is replaced by a sum. If we have <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mo>sin</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>u</mi><mi>k</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>Δ</mi><mi>x</mi><mspace width="0.167em"></mspace></mrow><annotation encoding="application/x-tex">f(u) = \sum_{k=1}^n \sin(u_k) \Delta x \,</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Δ</mi><mi>x</mi><mo>=</mo><mn>1</mn><mi>/</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">\Delta x = 1/n</annotation></semantics></math>, for a vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">u \in \mathbb{R}^n</annotation></semantics></math>, related to our previous <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(x)</annotation></semantics></math> by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>u</mi><mi>k</mi></msub><mo>=</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mi>Δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u_k = u(k\Delta x)</annotation></semantics></math>, which can be thought of as a “rectangle rule” (or Riemann sum, or Euler) approximation for the integral. Then, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>∇</mi><mi>u</mi></msub><mi>f</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mo>cos</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>u</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mo>cos</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>u</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mi>Δ</mi><mi>x</mi><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">\nabla_u f = \begin{pmatrix} \cos(u_1) \\ \cos(u_2) \\ \vdots \end{pmatrix} \Delta x  \, .</annotation></semantics></math> Why does this discrete version have a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\Delta x</annotation></semantics></math> multiplying the gradient, whereas our continuous version did not? The reason is that in the continuous version we effectively included the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> in the definition of the inner product <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy="false" form="postfix">⟩</mo></mrow><annotation encoding="application/x-tex">\langle u, v \rangle</annotation></semantics></math> (which was an integral). In discrete case, the ordinary inner product (used to define the conventional gradient) is just a sum without a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\Delta x</annotation></semantics></math>. However, if we define a <em>weighted</em> discrete inner product <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy="false" form="postfix">⟩</mo><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msub><mi>u</mi><mi>k</mi></msub><msub><mi>v</mi><mi>k</mi></msub><mi>Δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\langle u, v \rangle = \sum_{k=1}^n u_k v_k \Delta x</annotation></semantics></math>, then, according to Sec.&nbsp;<a href="#sec:generalvectorspaces" data-reference-type="ref" data-reference="sec:generalvectorspaces">5</a>, this changes the definition of the gradient, and in fact will remove the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\Delta x</annotation></semantics></math> term to correspond to the continuous version.</p>
</div>
</section>
<section id="example-minimizing-arc-length" class="level2">
<h2 class="anchored" data-anchor-id="example-minimizing-arc-length">Example: Minimizing arc length</h2>
<p>We now consider a more tricky example with an intuitive geometric interpretation.</p>
<div class="example">
<p>Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> be a differentiable function on <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[0,1]</annotation></semantics></math> and consider the functional <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mn>1</mn></msubsup><msqrt><mrow><mn>1</mn><mo>+</mo><mi>u</mi><mi>′</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow></msqrt><mspace width="0.167em"></mspace><mi>d</mi><mi>x</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">f(u) = \int_0^1 \sqrt{1+ u'(x)^2}\,dx.</annotation></semantics></math> Solve for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn><mi>.</mi></mrow><annotation encoding="application/x-tex">u(0) = u(1) = 0.</annotation></semantics></math></p>
</div>
<p>Geometrically, you learned in first-year calculus that this is simply the <strong>length of the curve</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(x)</annotation></semantics></math> from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">x=0</annotation></semantics></math> to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">x=1</annotation></semantics></math>. To differentiate this, first notice that ordinary single-variable calculus gives us the linearization <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><msqrt><mrow><mn>1</mn><mo>+</mo><msup><mi>v</mi><mn>2</mn></msup></mrow></msqrt><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msqrt><mrow><mn>1</mn><mo>+</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>v</mi><mo>+</mo><mi>d</mi><mi>v</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow></msqrt><mo>−</mo><msqrt><mrow><mn>1</mn><mo>+</mo><msup><mi>v</mi><mn>2</mn></msup></mrow></msqrt><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><msqrt><mrow><mn>1</mn><mo>+</mo><msup><mi>v</mi><mn>2</mn></msup></mrow></msqrt><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mi>d</mi><mi>v</mi><mo>=</mo><mfrac><mi>v</mi><msqrt><mrow><mn>1</mn><mo>+</mo><msup><mi>v</mi><mn>2</mn></msup></mrow></msqrt></mfrac><mi>d</mi><mi>v</mi><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">d\left( \sqrt{1 + v^2} \right) = \sqrt{1+ (v + dv)^2} - \sqrt{1 + v^2} = \left( \sqrt{1 + v^2} \right)' dv =  \frac{v}{\sqrt{1+ v^2}} dv \, .</annotation></semantics></math> Therefore, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>d</mi><mi>f</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>+</mo><mi>d</mi><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mn>1</mn></msubsup><mrow><mo stretchy="true" form="prefix">(</mo><msqrt><mrow><mn>1</mn><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>+</mo><mi>d</mi><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>′</mi><mn>2</mn></msup></mrow></msqrt><mo>−</mo><msqrt><mrow><mn>1</mn><mo>+</mo><mi>u</mi><msup><mi>′</mi><mn>2</mn></msup></mrow></msqrt><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>x</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mn>1</mn></msubsup><mfrac><mrow><mi>u</mi><mi>′</mi></mrow><msqrt><mrow><mn>1</mn><mo>+</mo><mi>u</mi><msup><mi>′</mi><mn>2</mn></msup></mrow></msqrt></mfrac><mspace width="0.167em"></mspace><mi>d</mi><mi>u</mi><mi>′</mi><mi>d</mi><mi>x</mi><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
   df &amp;= f(u+ du) - f(u) \\
   &amp;= \int_0^1 \left( \sqrt{1 + (u+du)'^2} - \sqrt{1 + u'^2} \right) dx \\
   &amp;= \int_0^1 \frac{u'}{\sqrt{1 + u'^2}} \, du'  dx.
\end{aligned}</annotation></semantics></math> However, this is a linear operator on <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>u</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">du'</annotation></semantics></math> and not (directly) on <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>u</mi></mrow><annotation encoding="application/x-tex">du</annotation></semantics></math>. Abstractly, this is fine, because <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>u</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">du'</annotation></semantics></math> is itself a linear operation on <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>u</mi></mrow><annotation encoding="application/x-tex">du</annotation></semantics></math>, so we have <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>u</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">f'(u)[du]</annotation></semantics></math> as the composition of two linear operations. However, it is more revealing to rewrite it explicitly in terms of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>u</mi></mrow><annotation encoding="application/x-tex">du</annotation></semantics></math>, for example in order to define <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math>. To accomplish this, we can apply <em>integration by parts</em> to obtain <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>u</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mn>1</mn></msubsup><mfrac><mrow><mi>u</mi><mi>′</mi></mrow><msqrt><mrow><mn>1</mn><mo>+</mo><mi>u</mi><msup><mi>′</mi><mn>2</mn></msup></mrow></msqrt></mfrac><mspace width="0.167em"></mspace><mi>d</mi><mi>u</mi><mi>′</mi><mi>d</mi><mi>x</mi><mo>=</mo><msubsup><mrow><mfrac><mrow><mi>u</mi><mi>′</mi></mrow><msqrt><mrow><mn>1</mn><mo>+</mo><mi>u</mi><msup><mi>′</mi><mn>2</mn></msup></mrow></msqrt></mfrac><mi>d</mi><mi>u</mi><mo stretchy="true" form="postfix">|</mo></mrow><mn>0</mn><mn>1</mn></msubsup><mo>−</mo><msubsup><mo>∫</mo><mn>0</mn><mn>1</mn></msubsup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>u</mi><mi>′</mi></mrow><msqrt><mrow><mn>1</mn><mo>+</mo><mi>u</mi><msup><mi>′</mi><mn>2</mn></msup></mrow></msqrt></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mspace width="0.167em"></mspace><mi>d</mi><mi>u</mi><mspace width="0.167em"></mspace><mi>d</mi><mi>x</mi><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">f'(u) [du] = \int_0^1 \frac{u'}{\sqrt{1 + u'^2}} \, du' dx = \left. \frac{u'}{\sqrt{1 + u'^2}} du \right|_0^1 - \int_0^1 \left(\frac{u'}{\sqrt{1 + u'^2}}\right)' \, du \, dx \, .</annotation></semantics></math></p>
<p>Notice that up until now we did not need utilize the “boundary conditions” <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">u(0) = u(1) = 0</annotation></semantics></math> for this calculation. However, if we want to restrict ourselves to such functions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(x)</annotation></semantics></math>, then our perturbation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>u</mi></mrow><annotation encoding="application/x-tex">du</annotation></semantics></math> cannot change the endpoint values, i.e.&nbsp;we must have <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>d</mi><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">du(0) = du(1) = 0</annotation></semantics></math>. (Geometrically, suppose that we want to find the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> that minimizes arc length between <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(0,0)</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(1,0)</annotation></semantics></math>, so that we need to fix the endpoints.) This implies that the boundary term in the above equation is zero. Hence, we have that <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>−</mi><msubsup><mo>∫</mo><mn>0</mn><mn>1</mn></msubsup><munder><munder><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>u</mi><mi>′</mi></mrow><msqrt><mrow><mn>1</mn><mo>+</mo><mi>u</mi><msup><mi>′</mi><mn>2</mn></msup></mrow></msqrt></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi></mrow><mo accent="true">⏟</mo></munder><mrow><mi>∇</mi><mi>f</mi></mrow></munder><mspace width="0.167em"></mspace><mi>d</mi><mi>u</mi><mspace width="0.167em"></mspace><mi>d</mi><mi>x</mi><mo>=</mo><mo stretchy="false" form="prefix">⟨</mo><mi>∇</mi><mi>f</mi><mo>,</mo><mi>d</mi><mi>u</mi><mo stretchy="false" form="postfix">⟩</mo><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">df = -\int_0^1 \underbrace{\left(\frac{u'}{\sqrt{1 + u'^2}}\right)'}_{\nabla f} \, du \, dx = \langle \nabla f , du \rangle \, .</annotation></semantics></math></p>
<p>Furthermore, note that the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> that minimizes the functional <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> has the property that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>u</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\left. \nabla f \right|_u = 0</annotation></semantics></math>. Therefore, for a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> that minimizes the functional <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> (the <em>shortest curve</em>), we must have the following result: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mn>0</mn><mo>=</mo><mi>∇</mi><mi>f</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>−</mi><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>u</mi><mi>′</mi></mrow><msqrt><mrow><mn>1</mn><mo>+</mo><mi>u</mi><msup><mi>′</mi><mn>2</mn></msup></mrow></msqrt></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>−</mi><mfrac><mrow><mi>u</mi><mi>″</mi><msqrt><mrow><mn>1</mn><mo>+</mo><mi>u</mi><msup><mi>′</mi><mn>2</mn></msup></mrow></msqrt><mo>−</mo><mi>u</mi><mi>′</mi><mfrac><mrow><mi>u</mi><mi>″</mi><mi>u</mi><mi>′</mi></mrow><msqrt><mrow><mn>1</mn><mo>+</mo><mi>u</mi><msup><mi>′</mi><mn>2</mn></msup></mrow></msqrt></mfrac></mrow><mrow><mn>1</mn><mo>+</mo><mi>u</mi><msup><mi>′</mi><mn>2</mn></msup></mrow></mfrac></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>−</mi><mfrac><mrow><mi>u</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>u</mi><msup><mi>′</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>u</mi><mi>″</mi><mi>u</mi><msup><mi>′</mi><mn>2</mn></msup></mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>u</mi><msup><mi>′</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mn>3</mn><mi>/</mi><mn>2</mn></mrow></msup></mfrac></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>−</mi><mfrac><mrow><mi>u</mi><mi>″</mi></mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>u</mi><msup><mi>′</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mn>3</mn><mi>/</mi><mn>2</mn></mrow></msup></mfrac><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    0 = \nabla f &amp;= -\left(\frac{u'}{\sqrt{1 + u'^2}}\right)' \\
    &amp;= -\frac{u'' \sqrt{1 + u'^2} - u' \frac{u'' u '}{\sqrt{1 + u'^2}}}{ 1 + u'^2} \\
    &amp;= -\frac{u'' ( 1 + u'^2) - u'' u'^2}{(1 + u'^2)^{3/2}} \\
    &amp;= -\frac{u''}{( 1+ u'^2)^{3/2}}.
\end{aligned}</annotation></semantics></math> Hence, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi><mo>=</mo><mn>0</mn><mo>⟹</mo><mi>u</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn><mo>⟹</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>a</mi><mi>x</mi><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">\nabla f = 0 \implies u''(x) = 0 \implies u(x) = ax + b</annotation></semantics></math> for constants <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>,</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">a,b</annotation></semantics></math>; and for these boundary conditions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo>=</mo><mi>b</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">a=b=0</annotation></semantics></math>. In other words, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> is the horizontal straight line segment!</p>
<p>Thus, we have recovered the familiar result that straight line segments in <span class="math inline">$\R^2$</span> are the shortest curves between two points!</p>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>Notice that the expression <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>u</mi><mi>″</mi></mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>+</mo><mi>u</mi><msup><mi>′</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mn>3</mn><mi>/</mi><mn>2</mn></mrow></msup></mfrac><annotation encoding="application/x-tex">\frac{u''}{(1+ u'^2)^{3/2}}</annotation></semantics></math> is the formula from multivariable calculus for the curvature of the curve defined by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">y= u(x)</annotation></semantics></math>. It is not a coincidence that the gradient of arc length is the (negative) curvature, and the minimum arc length occurs for zero gradient = zero curvature.</p>
</div>
</section>
<section id="eulerlagrange-equations" class="level2">
<h2 class="anchored" data-anchor-id="eulerlagrange-equations">Euler–Lagrange equations</h2>
<p>This style of calculation is part of the subject known as the <strong>calculus of variations</strong>. Of course, the final answer in the example above (a straight line) may have been obvious, but a similar approach can be applied to many more interesting problems. We can generalize the approach as follows:</p>
<div class="example">
<p>Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msubsup><mo>∫</mo><mi>a</mi><mi>b</mi></msubsup><mi>F</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>,</mo><mi>u</mi><mi>′</mi><mo>,</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">f(u) = \int_a^b F(u, u', x) \, dx</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> is a differentiable function on <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mi>a</mi><mo>,</mo><mi>b</mi><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[a,b]</annotation></semantics></math>. Suppose the endpoints of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> are fixed (i.e.&nbsp;its values at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mi>a</mi></mrow><annotation encoding="application/x-tex">x=a</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">x=b</annotation></semantics></math> are constants). Let us calculate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">df</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math>.</p>
</div>
<p>We find: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>d</mi><mi>f</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>+</mo><mi>d</mi><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msubsup><mo>∫</mo><mi>a</mi><mi>b</mi></msubsup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>F</mi></mrow><mrow><mi>∂</mi><mi>u</mi></mrow></mfrac><mi>d</mi><mi>u</mi><mo>+</mo><mfrac><mrow><mi>∂</mi><mi>F</mi></mrow><mrow><mi>∂</mi><mi>u</mi><mi>′</mi></mrow></mfrac><mi>d</mi><mi>u</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>x</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><munder><munder><mrow><mfrac><mrow><mi>∂</mi><mi>F</mi></mrow><mrow><mi>∂</mi><mi>u</mi><mi>′</mi></mrow></mfrac><mi>d</mi><mi>u</mi><msubsup><mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">|</mo><mi>a</mi><mi>b</mi></msubsup></mrow><mo accent="true">⏟</mo></munder><mrow><mo>=</mo><mn>0</mn></mrow></munder><mo>+</mo><msubsup><mo>∫</mo><mi>a</mi><mi>b</mi></msubsup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>F</mi></mrow><mrow><mi>∂</mi><mi>u</mi></mrow></mfrac><mo>−</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>F</mi></mrow><mrow><mi>∂</mi><mi>u</mi><mi>′</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>d</mi><mi>u</mi><mspace width="0.167em"></mspace><mi>d</mi><mi>x</mi><mspace width="0.167em"></mspace><mo>,</mo></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    df &amp;= f(u + du) - f(u) \\
    &amp;= \int_a^b \left(\frac{\partial F}{\partial u} du  + \frac{\partial F}{\partial u'} du' \right) dx \\
    &amp;= \underbrace{\frac{\partial F}{\partial u'} du \bigr|_a^b}_{= 0} + \int_a^b \left( \frac{\partial F}{\partial u} - \left(\frac{\partial F}{\partial u'}\right)'\right) \, du \, dx \, ,
\end{aligned}</annotation></semantics></math> where we used the fact that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>u</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">du = 0</annotation></semantics></math> at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math> or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math> if the endpoints <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>a</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(a)</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>b</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u(b)</annotation></semantics></math> are fixed. Hence, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi><mo>=</mo><mfrac><mrow><mi>∂</mi><mi>F</mi></mrow><mrow><mi>∂</mi><mi>u</mi></mrow></mfrac><mo>−</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>F</mi></mrow><mrow><mi>∂</mi><mi>u</mi><mi>′</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">\nabla f = \frac{\partial F}{\partial u} - \left(\frac{\partial F}{\partial u'}\right)',</annotation></semantics></math> which equals zero at extremum. Notice that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\nabla f = 0</annotation></semantics></math> yields a second-order differential equation in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math>, known as the <a href="https://en.wikipedia.org/wiki/Euler%E2%80%93Lagrange_equation">Euler–Lagrange equations</a>!</p>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>The notation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∂</mi><mi>F</mi><mi>/</mi><mi>∂</mi><mi>u</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">\partial F / \partial u'</annotation></semantics></math> is a notoriously confusing aspect of the calculus of variations—what does it mean to take the derivative “with respect to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">u'</annotation></semantics></math>” while holding <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>u</mi><annotation encoding="application/x-tex">u</annotation></semantics></math> fixed? A more explicit, albeit more verbose, way of expressing this is to think of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo>,</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">F(u,v,x)</annotation></semantics></math> as a function of three <em>unrelated</em> arguments, for which we only substitute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>=</mo><mi>u</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">v=u'</annotation></semantics></math> <em>after</em> differentiating with respect to the second argument <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>∂</mi><mi>F</mi></mrow><mrow><mi>∂</mi><mi>u</mi><mi>′</mi></mrow></mfrac><mo>=</mo><msub><mrow><mfrac><mrow><mi>∂</mi><mi>F</mi></mrow><mrow><mi>∂</mi><mi>v</mi></mrow></mfrac><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mi>v</mi><mo>=</mo><mi>u</mi><mi>′</mi></mrow></msub><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">\frac{\partial F}{\partial u'} = \left. \frac{\partial F}{\partial v} \right|_{v=u'} \, .</annotation></semantics></math></p>
</div>
<p>There are many wonderful applications of this idea. For example, search online for information about the “<a href="https://mathworld.wolfram.com/BrachistochroneProblem.html">brachistochrone problem</a>” (animated <a href="https://en.wikipedia.org/wiki/Brachistochrone_curve">here</a>) and/or the “<a href="https://en.wikipedia.org/wiki/Stationary-action_principle">principle of least action</a>”. Another example is a <a href="https://en.wikipedia.org/wiki/Catenary">catenary</a> curve, which minimizes the potential energy of a hanging cable. A classic textbook on the topic is <em>Calculus of Variations</em> by Gelfand and Fomin.</p>
</section>
</section>
<section id="derivatives-of-random-functions" class="level1">
<h1>Derivatives of Random Functions</h1>
<p>These notes are from a guest lecture by Gaurav Arya in IAP 2023.</p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In this class, we’ve learned how to take derivatives of all sorts of crazy functions. Recall one of our first examples: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>A</mi><mn>2</mn></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">f(A) = A^2,</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is a matrix. To differentiate this function, we had to go back to the drawing board, and ask:</p>
<div class="question">
<p>If we perturb the input slightly, how does the output change? <span id="question:perturb" data-label="question:perturb"></span></p>
</div>
<p>To this end, we wrote down something like: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>f</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>+</mo><mi>δ</mi><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>−</mo><msup><mi>A</mi><mn>2</mn></msup><mo>=</mo><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>δ</mi><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>δ</mi><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>A</mi><mo>+</mo><munder><munder><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>δ</mi><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo accent="true">⏟</mo></munder><mtext mathvariant="normal">neglected</mtext></munder><mi>.</mi></mrow><annotation encoding="application/x-tex">\delta f = (A+\delta A)^2 - A^2 = A (\delta A) + (\delta A) A + \underbrace{(\delta A)^2}_{\text{neglected}}.</annotation></semantics></math> We called <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\delta f</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">\delta A</annotation></semantics></math> <em>differentials</em> in the limit where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">\delta A</annotation></semantics></math> became arbitrarily small. We then had to ask:</p>
<div class="question">
<p>What terms in the differential can we neglect? <span id="question:neglect" data-label="question:neglect"></span></p>
</div>
<p>We decided that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>δ</mi><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><annotation encoding="application/x-tex">(\delta A)^2</annotation></semantics></math> should be neglected, justifying this by the fact that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>δ</mi><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><annotation encoding="application/x-tex">(\delta A)^2</annotation></semantics></math> is “higher-order”. We were left with the derivative operator <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>A</mi><mo>↦</mo><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>δ</mi><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>δ</mi><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">\delta A \mapsto
A(\delta A) + (\delta A) A</annotation></semantics></math>: the best possible <em>linear</em> approximation to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> in a neighbourhood of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>. At a high level, the main challenge here was dealing with complicated input and output spaces: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> was matrix-valued, and also matrix-accepting. We had to ask ourselves: in this case, what should the notion of a derivative even mean?</p>
<p>In this lecture, we will face a similar challenge, but with an even weirder type of function. This time, the output of our function will be <em>random</em>. Now, we need to revisit the same questions. If the output is random, how can we describe its response to a change in the input? And how can we form a useful notion of derivative?</p>
</section>
<section id="stochastic-programs" class="level2">
<h2 class="anchored" data-anchor-id="stochastic-programs">Stochastic programs</h2>
<p>More precisely, we will consider random, or <em>stochastic</em>, functions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> with real input <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">p \in \mathbb{R}</annotation></semantics></math> and real-valued random-variable output. As a map, we can write <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> as <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>↦</mo><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">p \mapsto X(p),</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)</annotation></semantics></math> is a random variable. (To keep things simple, we’ll take <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">p \in \mathbb{R}</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">X(p) \in \mathbb{R}</annotation></semantics></math> in this chapter, though of course they could be generalized to other vector spaces as in the other chapters. For now, the randomness is complicated enough to deal with.)</p>
<p>The idea is that we can only <em>sample</em> from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)</annotation></semantics></math>, according to some distribution of numbers with probabilities that depend upon <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>. One simple example would be sampling real numbers uniformly (equal probabilities) from the interval <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mi>p</mi><mo stretchy="true" form="postfix">]</mo></mrow><annotation encoding="application/x-tex">[0,p]</annotation></semantics></math>. As a more complicated example, suppose <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)</annotation></semantics></math> follows the <em>exponential distribution</em> with scale <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>, corresponding to randomly sampled real numbers <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">x \ge 0</annotation></semantics></math> whose probability decreases proportional to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>e</mi><mrow><mi>−</mi><mi>x</mi><mi>/</mi><mi>p</mi></mrow></msup><annotation encoding="application/x-tex">e^{-x/p}</annotation></semantics></math>. This can be denoted <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∼</mo><mo>Exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p) \sim \operatorname{Exp}(p)</annotation></semantics></math>, and implemented in Julia by:</p>
<pre class="jlcon"><code>julia&gt; using Distributions

julia&gt; sample_X(p) = rand(Exponential(p))
sample_X (generic function with 1 method)</code></pre>
<p>We can take a few samples:</p>
<pre class="jlcon"><code>julia&gt; sample_X(10.0)
1.7849785709142214

julia&gt; sample_X(10.0)
4.435847397169775

julia&gt; sample_X(10.0)
0.6823343897949835

julia&gt; mean(sample_X(10.0) for i = 1:10^9) # mean = p
9.999930348291866</code></pre>
<p>If our program gives a different output each time, what could a useful notion of derivative be? Before we try to answer this, let’s ask <em>why</em> we might want to take a derivative. The answer is that we may be very interested in <em>statistical properties</em> of random functions, i.e.&nbsp;values that can be expressed using <em>averages</em>. Even if a function is stochastic, its <em>average</em> (“expected value”), assuming the average exists, can be a deterministic function of its parameters that has a conventional derivative.</p>
<p>So, why not take the average <em>first</em>, and then take the ordinary derivative of this average? This simple approach works for very basic stochastic functions (e.g.&nbsp;the exponential distribution above has expected value <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>, with derivative <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math>), but runs into practical difficulties for more complicated distributions (as are commonly implemented by large computer programs working with random numbers).</p>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>It is often much easier to produce an “unbiased estimate” <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)</annotation></semantics></math> of a statistical quantity than to compute it exactly. (Here, an unbiased estimate means that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)</annotation></semantics></math> averages out to our statistical quantity of interest.)</p>
</div>
<p>For example, in deep learning, the “variational autoencoder” (VAE) is a very common architecture that is inherently stochastic. It is easy to get a stochastic <em>unbiased estimate</em> of the loss function by running a random simulation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)</annotation></semantics></math>: the loss function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(p)</annotation></semantics></math> is then the “average” value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)</annotation></semantics></math>, denoted by the <em>expected value</em> <span class="math inline">$\EE[X(p)]$</span>. However, computing the loss <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(p)</annotation></semantics></math> exactly would require integrating over all possible outcomes, which usually is impractical. Now, to train the VAE, we also need to differentiate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(p)</annotation></semantics></math>, i.e.&nbsp;differentiate <span class="math inline">$\EE[X(p)]$</span> with respect to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>!</p>
<p>Perhaps more intuitive examples can be found in the physical sciences, where randomness may be baked into your model of a physical process. In this case, it’s hard to get around the fact that you need to deal with stochasticity! For example, you may have two particles that interact with an <em>average</em> rate of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math>. But in reality, the times when these interactions actually occur follow a stochastic process. (In fact, the time until the first interaction might be exponentially distributed, with scale <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>/</mi><mi>r</mi></mrow><annotation encoding="application/x-tex">1/r</annotation></semantics></math>.) And if you want to (e.g.) fit the parameters of your stochastic model to real-world data, it’s once again very useful to have derivatives.</p>
<p>If we can’t compute our statistical quantity of interest exactly, it seems unreasonable to assume we can compute its derivative exactly. However, we could hope to stochastically <em>estimate</em> its derivative. That is, if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)</annotation></semantics></math> represents the full program that produces an unbiased estimate of our statistical quantity, here’s one property we’d definitely like our notion of derivative to have: we should be able to construct from it an unbiased gradient estimator<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X'(p)</annotation></semantics></math> satisfying <span class="math display">$$\EE[X'(p)] = \EE[X(p)]' = \frac{\partial \EE[X(p)]}{\partial p}.$$</span> Of course, there are infinitely many such estimators. For example, given any estimator <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X'(p)</annotation></semantics></math> we can add any other random variable that has zero average without changing the expectation value. But in practice there are two additional considerations: (1) we want <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X'(p)</annotation></semantics></math> to be easy to compute/sample (about as easy as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)</annotation></semantics></math>), and (2) we want the <em>variance</em> (the “spread”) of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X'(p)</annotation></semantics></math> to be small enough that we don’t need too many samples to estimate its average accurately (hopefully no worse than estimating <span class="math inline">$\EE[X(p)]$</span>).</p>
</section>
<section id="stochastic-differentials-and-the-reparameterization-trick" class="level2">
<h2 class="anchored" data-anchor-id="stochastic-differentials-and-the-reparameterization-trick">Stochastic differentials and the reparameterization trick</h2>
<p>Let’s begin by answering our first question (Question <a href="#question:perturb" data-reference-type="ref" data-reference="question:perturb">[question:perturb]</a>): how does <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)</annotation></semantics></math> respond to a change in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>? Let us consider a specific <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> and write down a <em>stochastic differential</em>, taking a small but non-infinitesimal <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">\delta p</annotation></semantics></math> to avoid thinking about infinitesimals for now: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>+</mo><mi>δ</mi><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\delta X(p) = X(p + \delta p) - X(p),</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">\delta p</annotation></semantics></math> represents an arbitrary small change in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>. What sort of object is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\delta X(p)</annotation></semantics></math>?</p>
<p>Since we’re subtracting two random variables, it ought to itself be a random variable. However, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\delta X(p)</annotation></semantics></math> is still not fully specified! We have only specified the marginal distributions of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>+</mo><mi>δ</mi><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p+\delta p)</annotation></semantics></math>: to be able to subtract the two, we need to know their <em>joint distribution</em>.</p>
<p>One possibility is to treat <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>+</mo><mi>δ</mi><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p + \delta p)</annotation></semantics></math> as independent. This means that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\delta X(p)</annotation></semantics></math> would be constructed as the difference of independent samples. Let’s see how samples from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\delta X(p)</annotation></semantics></math> would look like in this case!</p>
<pre class="jlcon"><code>julia&gt; sample_X(p) = rand(Exponential(p))
sample_X (generic function with 1 method)

julia&gt; sample_δX(p, δp) = sample_X(p + δp) - sample_X(p)
sample_δX (generic function with 1 method)

julia&gt; p = 10; δp = 1e-5;

julia&gt; sample_δX(p, δp)
-26.000938718875904

julia&gt; sample_δX(p, δp)
-2.6157162001718092

julia&gt; sample_δX(p, δp)
6.352622554495474

julia&gt; sample_δX(p, δp)
-9.53215951927184

julia&gt; sample_δX(p, δp)
1.2232268930932104</code></pre>
<p>We can observe something a bit worrying: even for a very tiny <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">\delta p</annotation></semantics></math> (we chose <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>p</mi><mo>=</mo><msup><mn>10</mn><mrow><mi>−</mi><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\delta p = 10^{-5}</annotation></semantics></math>), <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\delta X(p)</annotation></semantics></math> is still fairly large: essentially as large as the original random variables. This is not good news if we want to construct a derivative from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\delta X(p)</annotation></semantics></math>: we would rather see its magnitude getting smaller and smaller with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">\delta p</annotation></semantics></math>, like in the non-stochastic case. Computationally, this will make it very difficult to determine <span class="math inline">$\EE[X(p)]'$</span> by averaging <code>sample_δX(p, δp) / δp</code> over many samples: we’ll need a huge number of samples because the <em>variance</em>, the “spread” of random values, is huge for small&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">\delta p</annotation></semantics></math>.</p>
<p>Let’s try a different approach. It is natural to think of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)</annotation></semantics></math> for all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> as forming a <em>family</em> of random variables, all defined on the same <em>probability space</em>. A probability space, with some simplification, is a sample space <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math>, with a probability distribution <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ℙ</mi><annotation encoding="application/x-tex">\mathbb{P}</annotation></semantics></math> defined on the sample space. From this point of view, each <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)</annotation></semantics></math> can be expressed as a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Ω</mi><mo>→</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">\Omega \to \mathbb{R}</annotation></semantics></math>. To sample from a particular <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)</annotation></semantics></math>, we can imagine drawing a random <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ω</mi><annotation encoding="application/x-tex">\omega</annotation></semantics></math> from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math> according to <span class="math inline">$\PP$</span>, and then plugging this into <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)</annotation></semantics></math>, i.e.&nbsp;computing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>ω</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)(\omega)</annotation></semantics></math>. (Computationally, this is how most distributions are actually implemented: you start with a primitive pseudo-random number generator for a very simple distribution,<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> e.g.&nbsp;drawing values <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ω</mi><annotation encoding="application/x-tex">\omega</annotation></semantics></math> uniformly from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Ω</mi><mo>=</mo><mo stretchy="false" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\Omega = [0,1)</annotation></semantics></math>, and then you build other distributions on top of this by transforming <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ω</mi><annotation encoding="application/x-tex">\omega</annotation></semantics></math> somehow.) Intuitively, all of the “randomness” resides in the probability space, and crucially <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ℙ</mi><annotation encoding="application/x-tex">\mathbb{P}</annotation></semantics></math> does not depend on <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>: as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> varies, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)</annotation></semantics></math> just becomes a different <em>deterministic</em> map on <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math>.</p>
<p>The crux here is that all the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)</annotation></semantics></math> functions now depend on a shared source of randomness: the random draw of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ω</mi><annotation encoding="application/x-tex">\omega</annotation></semantics></math>. This means that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>+</mo><mi>δ</mi><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p+\delta p)</annotation></semantics></math> have a nontrivial joint distribution: what does it look like?</p>
<p>For concreteness, let’s study our exponential random variable <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∼</mo><mo>Exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p) \sim \operatorname{Exp}(p)</annotation></semantics></math> from above. Using the “inversion sampling” parameterization, it is possible to choose <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math> to be <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">[0,1)</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ℙ</mi><annotation encoding="application/x-tex">\mathbb{P}</annotation></semantics></math> to be the uniform distribution over <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math>; for any distribution, we can construct <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)</annotation></semantics></math> to be a corresponding nondecreasing function over <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math> (given by the inverse of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)</annotation></semantics></math>’s cumulative probability distribution). Applied to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∼</mo><mo>Exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p) \sim \operatorname{Exp}(p)</annotation></semantics></math>, the inversion method gives <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>ω</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>−</mi><mi>p</mi><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>ω</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)(\omega) = -p \log{(1-\omega)}</annotation></semantics></math>. This is implemented below, and is a theoretically equivalent way of sampling <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)</annotation></semantics></math> compared with the opaque <code>rand(Exponential(p))</code> function we used above:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>julia<span class="op">&gt;</span> <span class="fu">sample_X2</span>(p, ω) <span class="op">=</span> <span class="op">-</span>p <span class="op">*</span> <span class="fu">log</span>(<span class="fl">1</span> <span class="op">-</span> ω)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>sample_X2 (generic <span class="kw">function</span> with <span class="fl">1</span> method)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>julia<span class="op">&gt;</span> <span class="co"># rand() samples a uniform random number in [0,1)</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>julia<span class="op">&gt;</span> <span class="fu">sample_X2</span>(p) <span class="op">=</span> <span class="fu">sample_X2</span>(p, <span class="fu">rand</span>()) </span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>sample_X2 (generic <span class="kw">function</span> with <span class="fl">2</span> methods)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>julia<span class="op">&gt;</span> <span class="fu">sample_X2</span>(<span class="fl">10.0</span>)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="fl">8.380816941818618</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>julia<span class="op">&gt;</span> <span class="fu">sample_X2</span>(<span class="fl">10.0</span>)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="fl">2.073939134369733</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>julia<span class="op">&gt;</span> <span class="fu">sample_X2</span>(<span class="fl">10.0</span>)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a><span class="fl">29.94586208847568</span></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>julia<span class="op">&gt;</span> <span class="fu">sample_X2</span>(<span class="fl">10.0</span>)</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a><span class="fl">23.91658360124792</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Okay, so what does our joint distribution look like?</p>
<div id="fig:exp" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/exp_for_matrixcalc.pdf.svg" class="img-fluid figure-img" style="width:40.0%"></p>
<figcaption>For <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∼</mo><mo>Exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p) \sim \operatorname{Exp}(p)</annotation></semantics></math> parameterized via the inversion method, we can write <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>+</mo><mi>δ</mi><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p+\delta p)</annotation></semantics></math>, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\delta X(p)</annotation></semantics></math> as functions from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Ω</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">]</mo></mrow><mo>→</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">\Omega = [0,1] \to \mathbb{R}</annotation></semantics></math>, defined on a probability space with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ℙ</mi><mo>=</mo><mo>Unif</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbb{P} = \operatorname{Unif}(0,1)</annotation></semantics></math>.</figcaption>
</figure>
</div>
<p>As shown in Figure&nbsp;<a href="#fig:exp" data-reference-type="ref" data-reference="fig:exp">8</a>, we can plot <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo>+</mo><mi>δ</mi><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p+\delta p)</annotation></semantics></math> as functions over <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math>. To sample the two of them jointly, we use the <em>same</em> choice of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ω</mi><annotation encoding="application/x-tex">\omega</annotation></semantics></math>: thus, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\delta X(p)</annotation></semantics></math> can be formed by subtracting the two functions <em>pointwise</em> at each <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Ω</mi><annotation encoding="application/x-tex">\Omega</annotation></semantics></math>. Ultimately, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\delta X(p)</annotation></semantics></math> is itself a random variable over the same probability space, sampled in the same way: we pick a random <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ω</mi><annotation encoding="application/x-tex">\omega</annotation></semantics></math> according to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ℙ</mi><annotation encoding="application/x-tex">\mathbb{P}</annotation></semantics></math>, and evaluate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>ω</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\delta X(p)(\omega)</annotation></semantics></math>, using the function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\delta X(p)</annotation></semantics></math> depicted above. Our first approach with independent samples is depicted in red in Figure&nbsp;<a href="#fig:exp" data-reference-type="ref" data-reference="fig:exp">8</a>, while our second approach is in blue. We can now see the flaw of the independent-samples approach: the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝒪</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{O}(1)</annotation></semantics></math>-sized “noise” from the independent samples washes out the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>𝒪</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>δ</mi><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{O}(\delta p)</annotation></semantics></math>-sized “signal”.</p>
<p>What about our second question (Question <a href="#question:neglect" data-reference-type="ref" data-reference="question:neglect">[question:neglect]</a>): how can actually take the limit of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>p</mi><mo>→</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\delta p \to 0</annotation></semantics></math> and compute the derivative? The idea is to differentiate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\delta X(p)</annotation></semantics></math> at each fixed sample <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ω</mi><mo>∈</mo><mi>Ω</mi></mrow><annotation encoding="application/x-tex">\omega \in \Omega</annotation></semantics></math>. In probability theory terms, we take the limit of random variables <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mi>δ</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">\delta X(p) / \delta p</annotation></semantics></math> as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>p</mi><mo>→</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\delta p \to 0</annotation></semantics></math>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munder><mo>lim</mo><mrow><mi>δ</mi><mi>p</mi><mo>→</mo><mn>0</mn></mrow></munder><mfrac><mrow><mi>δ</mi><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>δ</mi><mi>p</mi></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">X'(p) = \lim_{\delta p \to 0} \frac{\delta X(p)}{\delta p}.</annotation></semantics></math> For <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∼</mo><mo>Exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p) \sim \operatorname{Exp}(p)</annotation></semantics></math> parameterized via the inversion method, we get: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>ω</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><munder><mo>lim</mo><mrow><mi>δ</mi><mi>p</mi><mo>→</mo><mn>0</mn></mrow></munder><mfrac><mrow><mi>−</mi><mi>δ</mi><mi>p</mi><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>ω</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>δ</mi><mi>p</mi></mrow></mfrac><mo>=</mo><mi>−</mi><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>ω</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">X'(p)(\omega) = \lim_{\delta p \to 0} \frac{-\delta p\log{(1-\omega)}}{\delta p} = -\log{(1-\omega)}.</annotation></semantics></math> Once again, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X'(p)</annotation></semantics></math> is a random variable over the same probability space. The claim is that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X'(p)</annotation></semantics></math> is the notion of derivative we were looking for! Indeed, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X'(p)</annotation></semantics></math> is itself in fact a valid gradient estimator: <span class="math display">$$\EE[X'(p)] = \EE\left[ \lim_{\delta p \to 0} \frac{\delta X(p)}{\delta p}\right] \stackrel{?}{=}
\lim_{\delta p \to 0} \frac{\EE[\delta X(p)]}{\delta p} =
    \frac{\partial \EE[X(p)]}{\partial p}.
    \label{eq:exchange}$$</span> Rigorously, one needs to justify the interchange of limit and expectation in the above. In this chapter, however, we will be content with a crude empirical justification:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>julia<span class="op">&gt;</span> <span class="fu">X′</span>(p, ω) <span class="op">=</span> <span class="fu">-log</span>(<span class="fl">1</span> <span class="op">-</span> ω)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>X′ (generic <span class="kw">function</span> with <span class="fl">1</span> method)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>julia<span class="op">&gt;</span> <span class="fu">X′</span>(p) <span class="op">=</span> <span class="fu">X′</span>(p, <span class="fu">rand</span>())</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>X′ (generic <span class="kw">function</span> with <span class="fl">2</span> methods)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>julia<span class="op">&gt;</span> <span class="fu">mean</span>(<span class="fu">X′</span>(<span class="fl">10.0</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10000</span>)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="fl">1.011689946421105</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>So <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X'(p)</annotation></semantics></math> does indeed average to 1, which makes sense since the expectation of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>Exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{Exp}(p)</annotation></semantics></math> is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>, which has derivative 1 for any choice of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>. However, the crux is that this notion of derivative also works for more complicated random variables that can be formed via <em>composition</em> of simple ones such as an exponential random variable. In fact, it turns out to obey the same chain rule as usual!</p>
<p>Let’s demonstrate this. Using the dual numbers introduced in Chapter&nbsp;<a href="#sec:AD" data-reference-type="ref" data-reference="sec:AD">8</a>, we can differentiate the expectation of the square of a sample from an exponential distribution <em>without</em> having an analytic expression for this quantity. (The expression for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">X'</annotation></semantics></math> we derived is already implemented as a dual-number rule in Julia by the <code>ForwardDiff.jl</code> package.) The primal and dual values of the outputted dual number are samples from the joint distribution of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mi>X</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(X(p), X'(p))</annotation></semantics></math>.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>julia<span class="op">&gt;</span> <span class="im">using</span> <span class="bu">Distributions</span>, <span class="bu">ForwardDiff</span>: Dual</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>julia<span class="op">&gt;</span> <span class="fu">sample_X</span>(p) <span class="op">=</span> <span class="fu">rand</span>(<span class="fu">Exponential</span>(p))<span class="op">^</span><span class="fl">2</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>sample_X (generic <span class="kw">function</span> with <span class="fl">1</span> method)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>julia<span class="op">&gt;</span> <span class="fu">sample_X</span>(<span class="fu">Dual</span>(<span class="fl">10.0</span>, <span class="fl">1.0</span>)) <span class="co"># sample a single dual number!</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="fu">Dual</span><span class="dt">{Nothing}</span>(<span class="fl">153.74964559529033</span>,<span class="fl">30.749929119058066</span>)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>julia<span class="op">&gt;</span> <span class="co"># obtain the derivative!</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>julia<span class="op">&gt;</span> <span class="fu">mean</span>(<span class="fu">sample_X</span>(<span class="fu">Dual</span>(<span class="fl">10.0</span>, <span class="fl">1.0</span>)).partials[<span class="fl">1</span>] <span class="cf">for</span> i <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10000</span>)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a><span class="fl">40.016569793650525</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Using the “reparameterization trick” to form a gradient estimator, as we have done here, is a fairly old idea. It is also called the “pathwise” gradient estimator. Recently, it has become very popular in machine learning due to its use in VAEs [e.g.&nbsp;Kingma&nbsp;&amp; Welling (2013): <a href="https://arxiv.org/abs/1312.6114" class="uri">https://arxiv.org/abs/1312.6114</a>], and lots of resources can be found online on it. Since composition simply works by the usual chain rule, it also works in reverse mode, and can differentiate functions far more complicated than the one above!</p>
</section>
<section id="handling-discrete-randomness" class="level2">
<h2 class="anchored" data-anchor-id="handling-discrete-randomness">Handling discrete randomness</h2>
<p>So far we have only considered a continuous random variable. Let’s see how the picture changes for a discrete random variable! Let’s take a simple Bernoulli variable <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∼</mo><mo>Ber</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p) \sim \operatorname{Ber}(p)</annotation></semantics></math>, which is 1 with probability <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> and 0 with probability <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">1-p</annotation></semantics></math>.</p>
<pre class="jlcon"><code>julia&gt; sample_X(p) = rand(Bernoulli(p))
sample_X (generic function with 1 method)

julia&gt; p = 0.5
0.6

julia&gt; sample_X(δp) # produces false/true, equivalent to 0/1
true


julia&gt; sample_X(δp)
false

julia&gt; sample_X(δp)
true</code></pre>
<p>The parameterization of a Bernoulli variable is shown in Figure 2. Using the inversion method once again, the parameterization of a Bernoulli variable looks like a step function: for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ω</mi><mo>&lt;</mo><mn>1</mn><mo>−</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">\omega &lt; 1-p</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>ω</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">X(p)(\omega) = 0</annotation></semantics></math>, while for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ω</mi><mo>≥</mo><mn>1</mn><mo>−</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">\omega \geq 1-p</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>ω</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">X(p)(\omega) = 1</annotation></semantics></math>.</p>
<p>Now, what happens when we perturb <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>? Let’s imagine perturbing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> by a positive amount <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">\delta p</annotation></semantics></math>. As shown in Figure 2, something qualitatively very different has happened here. At nearly every <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ω</mi><annotation encoding="application/x-tex">\omega</annotation></semantics></math> except a small region of probability <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">\delta p</annotation></semantics></math>, the output does not change. Thus, the quantity <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X'(p)</annotation></semantics></math> we defined in the previous subsection (which, strictly speaking, was defined by an "almost-sure" limit that neglects regions of probability 0) is 0 at every <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ω</mi><annotation encoding="application/x-tex">\omega</annotation></semantics></math>: after all, for every <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ω</mi><annotation encoding="application/x-tex">\omega</annotation></semantics></math>, there exists small enough <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">\delta p</annotation></semantics></math> such that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>ω</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\delta X(p)(\omega) = 0</annotation></semantics></math>.</p>
<figure id="fig:ber2" class="figure">
<figure id="fig:ber1" class="figure">
<p>
<embed src="figures/ber1_for_matrixcalc.pdf.svg" style="height:4.5cm">
<span id="fig:ber1" data-label="fig:ber1"></span>
</p>
</figure>
<figure id="fig:ber2" class="figure">
<p>
<embed src="figures/ber2_for_matrixcalc.pdf.svg" style="height:4.5cm">
<span id="fig:ber2" data-label="fig:ber2"></span>
</p>
</figure>
<figcaption>
For <span class="math inline"><em>X</em>(<em>p</em>) ∼ Ber (<em>p</em>)</span> parameterized via the inversion method, plots of <span class="math inline"><em>X</em>(<em>p</em>)</span>, <span class="math inline"><em>X</em>(<em>p</em> + <em>δ</em><em>p</em>)</span>, and <span class="math inline"><em>δ</em><em>X</em>(<em>p</em>)</span> as functions <span class="math inline"><em>Ω</em> : [0, 1] → ℝ</span>.
</figcaption>
</figure>
<p>However, there is certainly an important derivative contribution to consider here. The expectation of a Bernoulli is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>, so we would expect the derivative to be 1: but <span class="math inline">$\EE[X'(p)] = \EE[0] = 0$</span>. What has gone wrong is that, although <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\delta X(p)</annotation></semantics></math> is 0 with tiny probability, the value of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\delta X(p)</annotation></semantics></math> on this region of tiny probability is 1, which is <em>large</em>. In particular, it does not approach 0 as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">\delta p</annotation></semantics></math> approaches 0. Thus, to develop a notion of derivative of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">X(p)</annotation></semantics></math>, we need to somehow capture these large jumps with “infinitesimal” probability.</p>
<p>A recent (2022) publication (<a href="https://arxiv.org/abs/2210.08572" class="uri">https://arxiv.org/abs/2210.08572</a>) by the author of this chapter (Gaurav Arya), together with Frank&nbsp;Schäfer, Moritz&nbsp;Schauer, and Chris&nbsp;Rackauckas, worked to extend the above ideas to develop a notion of “stochastic derivative” for discrete randomness, implemented by a software package called <code>StochasticAD.jl</code> that performs automatic differentiation of such stochastic processes. It generalizes the idea of dual numbers to stochastic <em>triples</em>, which include a third component to capture exactly these large jumps. For example, the stochastic triple of a Bernoulli variable might look like:</p>
<pre class="jlcon"><code>julia&gt; using StochasticAD, Distributions
julia&gt; f(p) = rand(Bernoulli(p)) # 1 with probability p, 0 otherwise
julia&gt; stochastic_triple(f, 0.5) # Feeds 0.5 + δp into f
StochasticTriple of Int64:
0 + 0ε + (1 with probability 2.0ε)</code></pre>
<p>Here, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>p</mi></mrow><annotation encoding="application/x-tex">\delta p</annotation></semantics></math> is denoted by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>ε</mo><annotation encoding="application/x-tex">\upepsilon</annotation></semantics></math>, imagined to be an “infinitesimal unit”, so that the above triple indicates a flip from 0 to 1 with probability that has derivative <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>2</mn><annotation encoding="application/x-tex">2</annotation></semantics></math>.</p>
<p>However, many aspects of these problems are still difficult, and there are a lot of improvements awaiting future developments! If you’re interested in reading more, you may be interested in the paper and our package linked above, as well as the 2020 review article by Mohamed <em>et&nbsp;al.</em> (<a href="https://arxiv.org/abs/1906.10652" class="uri">https://arxiv.org/abs/1906.10652</a>), which is a great survey of the field of gradient estimation in general.</p>
<p>At the end of class, we considered a differentiable random walk example with <code>StochasticAD.jl</code>. Here it is!</p>
<pre class="jlcon"><code>julia&gt; using Distributions, StochasticAD

julia&gt; function X(p)
           n = 0
           for i in 1:100
               n += rand(Bernoulli(p * (1 - (n+i)/200)))
           end
           return n
       end
X (generic function with 1 method)

julia&gt; mean(X(0.5) for _ in 1:10000) # calculate E[X(p)] at p = 0.5
32.6956

julia&gt; st = stochastic_triple(X, 0.5) # sample a single stochastic triple at p = 0.5
StochasticTriple of Int64:
32 + 0δp + (1 with probability 74.17635818221052δp)

julia&gt; derivative_contribution(st) # derivative estimate produced by this triple
74.17635818221052

julia&gt; # compute d/dp of E[X(p)] by taking many samples
julia&gt; mean(derivative_contribution(stochastic_triple(f, 0.5)) for i in 1:10000)
56.65142976168479</code></pre>
</section>
</section>
<section id="sec:hessians" class="level1">
<h1>Second Derivatives, Bilinear Maps, and Hessian Matrices</h1>
<p>In this chapter, we apply the principles of this course to <em>second</em> derivatives, which are conceptually just derivatives of derivatives but turn out to have many interesting ramifications. We begin with a (probably) familiar case of scalar-valued functions from multi-variable calculus, in which the second derivative is simply a matrix called the <em>Hessian</em>. Subsequently, however, we will show that similar principles can be applied to more complicated input and output spaces, generalizing to a notion of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi></mrow><annotation encoding="application/x-tex">f''</annotation></semantics></math> as a <em>symmetric bilinear map</em>.</p>
<section id="sec:Hessian-scalar" class="level2">
<h2 class="anchored" data-anchor-id="sec:Hessian-scalar">Hessian matrices of scalar-valued functions</h2>
<p>Recall that for a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">f(x) \in \mathbb{R}</annotation></semantics></math> that maps column vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">x \in \mathbb{R}^n</annotation></semantics></math> to scalars (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>:</mo><msup><mi>ℝ</mi><mi>n</mi></msup><mo>↦</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">f: \mathbb{R}^n \mapsto \mathbb{R}</annotation></semantics></math>), the first derivative <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math> can be expressed in terms of the familiar gradient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>f</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\nabla f = (f')^T</annotation></semantics></math> of multivariable calculus: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mn>1</mn></msub></mrow></mfrac></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mn>2</mn></msub></mrow></mfrac></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mi>n</mi></msub></mrow></mfrac></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">\nabla f = \begin{pmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n}
\end{pmatrix} \, .</annotation></semantics></math> If we think of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> as a new (generally nonlinear) function mapping <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup><mo>↦</mo><mi>∇</mi><mi>f</mi><mo>∈</mo><msup><mi>ℝ</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">x \in \mathbb{R}^n \mapsto \nabla f \in \mathbb{R}^n</annotation></semantics></math>, then its derivative is an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \times n</annotation></semantics></math> Jacobian matrix (a linear operator mapping vectors to vectors), which we can write down explicitly in terms of <em>second</em> derivatives of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mfrac><mrow><msup><mi>∂</mi><mn>2</mn></msup><mi>f</mi></mrow><mrow><mi>∂</mi><msubsup><mi>x</mi><mn>1</mn><mn>2</mn></msubsup></mrow></mfrac></mtd><mtd columnalign="center" style="text-align: center"><mi>⋯</mi></mtd><mtd columnalign="center" style="text-align: center"><mfrac><mrow><msup><mi>∂</mi><mn>2</mn></msup><mi>f</mi></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mi>n</mi></msub><mi>∂</mi><msub><mi>x</mi><mn>1</mn></msub></mrow></mfrac></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd><mtd columnalign="center" style="text-align: center"><mo>⋱</mo></mtd><mtd columnalign="center" style="text-align: center"><mi>⋮</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mfrac><mrow><msup><mi>∂</mi><mn>2</mn></msup><mi>f</mi></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mn>1</mn></msub><mi>∂</mi><msub><mi>x</mi><mi>n</mi></msub></mrow></mfrac></mtd><mtd columnalign="center" style="text-align: center"><mi>⋯</mi></mtd><mtd columnalign="center" style="text-align: center"><mfrac><mrow><msup><mi>∂</mi><mn>2</mn></msup><mi>f</mi></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mi>n</mi></msub><mi>∂</mi><msub><mi>x</mi><mi>n</mi></msub></mrow></mfrac></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>H</mi><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">(\nabla f)' =
\begin{pmatrix}
   \frac{\partial^2 f}{ \partial x_1^2} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_n \partial x_1} \\
        \vdots  &amp;\ddots &amp; \vdots \\
        \frac{\partial^2 f}{\partial x_1 \partial x_n} &amp; \cdots &amp; \frac{\partial^2 f}{\partial x_n \partial x_n}
    \end{pmatrix} = H \, .</annotation></semantics></math> This matrix, denoted here by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math>, is known as the <strong>Hessian</strong> of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>, which has entries: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><msup><mi>∂</mi><mn>2</mn></msup><mi>f</mi></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mi>j</mi></msub><mi>∂</mi><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><msup><mi>∂</mi><mn>2</mn></msup><mi>f</mi></mrow><mrow><mi>∂</mi><msub><mi>x</mi><mi>i</mi></msub><mi>∂</mi><msub><mi>x</mi><mi>j</mi></msub></mrow></mfrac><mo>=</mo><msub><mi>H</mi><mrow><mi>j</mi><mo>,</mo><mi>i</mi></mrow></msub><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">H_{i,j} = \frac{\partial^2 f}{ \partial x_j \partial x_i} = \frac{\partial^2 f}{ \partial x_i \partial x_j} = H_{j,i}  \, .</annotation></semantics></math> The fact that you can take partial derivatives in either order is a familiar fact from multivariable calculus (sometimes called the “symmetry of mixed derivatives” or “equality of mixed partials”), and means that the Hessian is a <em>symmetric matrix</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>=</mo><msup><mi>H</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">H = H^T</annotation></semantics></math>. (We will later see that such symmetries arise very generally from the construction of second derivatives.)</p>
<div class="example">
<p>For <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi>ℝ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">x \in \mathbb{R}^2</annotation></semantics></math> and the function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>sin</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msubsup><mi>x</mi><mn>1</mn><mn>2</mn></msubsup><msubsup><mi>x</mi><mn>2</mn><mn>3</mn></msubsup></mrow><annotation encoding="application/x-tex">f(x) = \sin(x_1) + x_1^2 x_2^3</annotation></semantics></math>, its gradient is <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mo>cos</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mn>2</mn><msub><mi>x</mi><mn>1</mn></msub><msubsup><mi>x</mi><mn>2</mn><mn>3</mn></msubsup></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>3</mn><msubsup><mi>x</mi><mn>1</mn><mn>2</mn></msubsup><msubsup><mi>x</mi><mn>2</mn><mn>2</mn></msubsup></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">\nabla f = \begin{pmatrix} \cos(x_1) + 2x_1 x_2^3 \\
3 x_1^2 x_2^2
\end{pmatrix} \, ,</annotation></semantics></math> and its Hessian is <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>−</mi><mo>sin</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mn>2</mn><msubsup><mi>x</mi><mn>2</mn><mn>3</mn></msubsup></mtd><mtd columnalign="center" style="text-align: center"><mn>6</mn><msub><mi>x</mi><mn>1</mn></msub><msubsup><mi>x</mi><mn>2</mn><mn>2</mn></msubsup></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>6</mn><msub><mi>x</mi><mn>1</mn></msub><msubsup><mi>x</mi><mn>2</mn><mn>2</mn></msubsup></mtd><mtd columnalign="center" style="text-align: center"><mn>6</mn><msubsup><mi>x</mi><mn>1</mn><mn>2</mn></msubsup><msub><mi>x</mi><mn>2</mn></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>H</mi><mi>T</mi></msup><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">H = (\nabla f)' = \begin{pmatrix} -\sin(x_1) + 2x_2^3 &amp; 6 x_1 x_2^2 \\
6 x_1 x_2^2 &amp; 6 x_1^2 x_2 \end{pmatrix} = H^T \, .</annotation></semantics></math></p>
</div>
<p>If we think of the Hessian as the Jacobian of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math>, this tells us that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mspace width="0.167em"></mspace><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">H \, dx</annotation></semantics></math> predicts the change in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> to first order: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mrow><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi></mrow></msub><mo>−</mo><msub><mrow><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>x</mi></msub><mo>=</mo><mi>H</mi><mspace width="0.167em"></mspace><mi>d</mi><mi>x</mi><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">d(\nabla f) = \left. \nabla f \right|_{x+dx} - \left. \nabla f \right|_{x} = H \, dx \, .</annotation></semantics></math> Note that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mrow><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi></mrow></msub><annotation encoding="application/x-tex">\left. \nabla f \right|_{x+dx}</annotation></semantics></math> means <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> evaluated at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">x+dx</annotation></semantics></math>, which is very different from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">df = (\nabla f)^T dx</annotation></semantics></math>, where we act <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">f'(x)=(\nabla f)^T</annotation></semantics></math> <em>on</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math>.</p>
<p>Instead of thinking of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math> of predicting the <em>first</em>-order change in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math>, however, we can also think of it as predicting the <em>second</em>-order change in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>, a <strong>quadratic approximation</strong> (which could be viewed as the first three terms in a multidimensional Taylor series): <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mspace width="0.167em"></mspace><mi>δ</mi><mi>x</mi><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>δ</mi><msup><mi>x</mi><mi>T</mi></msup><mspace width="0.167em"></mspace><mi>H</mi><mspace width="0.167em"></mspace><mi>δ</mi><mi>x</mi><mo>+</mo><mi>o</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo stretchy="false" form="postfix">‖</mo><mi>δ</mi><mi>x</mi><msup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">f(x+\delta x) = f(x) + (\nabla f)^T \, \delta x + \frac{1}{2} \delta x^T \, H \, \delta x + o(\Vert \delta x \Vert^2) \, ,</annotation></semantics></math> where both <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math> are evaluated at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>, and we have switched from an infinitesimal <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> to a finite change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math> so that we emphasize the viewpoint of an <em>approximation</em> where terms higher than second-order in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">‖</mo><mi>δ</mi><mi>x</mi><mo stretchy="false" form="postfix">‖</mo></mrow><annotation encoding="application/x-tex">\Vert \delta x \Vert</annotation></semantics></math> are dropped. You can derive this in a variety of ways, e.g.&nbsp;by taking the derivative of both sides with respect to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math> to reproduce <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">|</mo></mrow><mrow><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi></mrow></msub><mo>=</mo><msub><mrow><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">|</mo></mrow><mi>x</mi></msub><mo>+</mo><mi>H</mi><mspace width="0.167em"></mspace><mi>δ</mi><mi>x</mi><mo>+</mo><mi>o</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\left. \nabla f \right|_{x+\delta x} = \left. \nabla f \right|_{x} + H \, \delta x + o(\delta x)</annotation></semantics></math>: a quadratic approximation for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> corresponds to a linear approximation for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math>. Related to this equation, another useful (and arguably more fundamental) relation that we can derive (and <em>will</em> derive much more generally below) is: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msup><mi>x</mi><mi>T</mi></msup><mi>H</mi><mi>d</mi><mi>x</mi><mi>′</mi><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo>,</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">]</mo></mrow><mspace width="0.167em"></mspace></mrow><annotation encoding="application/x-tex">dx^T H dx' = f(x + dx + dx') + f(x) - f(x + dx) - f(x + dx') = f''(x)[dx,dx'] \,</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">dx'</annotation></semantics></math> are two independent “infinitesimal” directions and we have dropped terms of higher than second order. This formula is very suggestive, because it uses <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math> to map <em>two</em> vectors into a <em>scalar</em>, which we will generalize below into the idea of a <strong>bilinear map</strong> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo>,</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">f''(x)[dx,dx']</annotation></semantics></math>. This formula is also obviously symmetric with respect to interchange of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">dx'</annotation></semantics></math> — <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo>,</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo>,</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">f''(x)[dx,dx'] = f''(x)[dx',dx]</annotation></semantics></math> — which will lead us once again to the symmetry <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>=</mo><msup><mi>H</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">H=H^T</annotation></semantics></math> below.</p>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>Consider the Hessian matrix versus other Jacobian matrices. The Hessian matrix expresses the <em>second</em> derivative of a scalar-valued multivariate function, and is always square and symmetric. A Jacobian matrix, in general, expresses the <em>first</em> derivative of a vector-valued multivariate function, may be non-square, and is rarely symmetric. (However, the Hessian matrix <em>is</em> the Jacobian of the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> function!)</p>
</div>
</section>
<section id="general-second-derivatives-bilinear-maps" class="level2">
<h2 class="anchored" data-anchor-id="general-second-derivatives-bilinear-maps">General second derivatives: Bilinear maps</h2>
<p>Recall, as we have been doing throughout this class, that we define the derivative of a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> by a linearization of its change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">df</annotation></semantics></math> for a small (“infinitesimal”) change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> in the input: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">df = f(x + dx) - f(x) = f'(x) [dx] \, ,</annotation></semantics></math> implicitly dropping higher-order terms. If we similarly consider the second derivative <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi></mrow><annotation encoding="application/x-tex">f''</annotation></semantics></math> as simply the same process applied to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math> instead of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>, we obtain the following formula, which is easy to write down but will take some thought to interpret: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mi>′</mi><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">]</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">d f' = f'(x + dx') - f'(x) = f''(x) [dx'].</annotation></semantics></math> (Notation: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">dx'</annotation></semantics></math> is not some kind of derivative of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math>; the prime simply denotes a <em>different</em> arbitrary small change in&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>.) What kind of “thing” is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">df'</annotation></semantics></math>? Let’s consider a simple concrete example:</p>
<div class="example">
<p>Consider the following function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>:</mo><msup><mi>ℝ</mi><mn>2</mn></msup><mo>↦</mo><msup><mi>ℝ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">f(x): \mathbb{R}^2 \mapsto \mathbb{R}^2</annotation></semantics></math> mapping two-component vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi>ℝ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">x \in \mathbb{R}^2</annotation></semantics></math> to two-component vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>∈</mo><msup><mi>ℝ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">f(x) \in \mathbb{R}^2</annotation></semantics></math>: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msubsup><mi>x</mi><mn>1</mn><mn>2</mn></msubsup><mo>sin</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>5</mn><msub><mi>x</mi><mn>1</mn></msub><mo>−</mo><msubsup><mi>x</mi><mn>2</mn><mn>3</mn></msubsup></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">f(x) = \begin{pmatrix} x_1^2 \sin(x_2) \\ 5x_1 - x_2^3
\end{pmatrix} \, .</annotation></semantics></math> Its first derivative is described by a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2\times 2</annotation></semantics></math> Jacobian matrix: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>2</mn><msub><mi>x</mi><mn>1</mn></msub><mo>sin</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="center" style="text-align: center"><msubsup><mi>x</mi><mn>1</mn><mn>2</mn></msubsup><mo>cos</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>5</mn></mtd><mtd columnalign="center" style="text-align: center"><mi>−</mi><mn>3</mn><msubsup><mi>x</mi><mn>2</mn><mn>2</mn></msubsup></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x) = \begin{pmatrix}
2x_1 \sin(x_2) &amp; x_1^2 \cos(x_2) \\
5 &amp; -3x_2^2
\end{pmatrix}</annotation></semantics></math> that maps a small change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> in the input vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> to the corresponding small change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">df = f'(x)dx</annotation></semantics></math> in the output vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>.</p>
<p>What is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mi>′</mi><mo>=</mo><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">df' = f''(x)[dx']</annotation></semantics></math>? It must take a small change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi><mi>′</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><msub><mi>x</mi><mn>1</mn></msub><mi>′</mi><mo>,</mo><mi>d</mi><msub><mi>x</mi><mn>2</mn></msub><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">dx' = (dx_1', dx_2')</annotation></semantics></math> in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> and return the first-order change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mi>′</mi><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">df' = f'(x+dx')-f'(x)</annotation></semantics></math> in our Jacobian matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math>. If we simply take the differential of each entry of our Jacobian (a function from vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> to matrices <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math>), we find: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mi>′</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>2</mn><mspace width="0.167em"></mspace><mi>d</mi><msub><mi>x</mi><mn>1</mn></msub><mi>′</mi><mo>sin</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mn>2</mn><msub><mi>x</mi><mn>1</mn></msub><mo>cos</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>d</mi><msub><mi>x</mi><mn>2</mn></msub><mi>′</mi></mtd><mtd columnalign="center" style="text-align: center"><mn>2</mn><msub><mi>x</mi><mn>1</mn></msub><mspace width="0.167em"></mspace><mi>d</mi><msub><mi>x</mi><mn>1</mn></msub><mi>′</mi><mo>cos</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msubsup><mi>x</mi><mn>1</mn><mn>2</mn></msubsup><mo>sin</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>d</mi><msub><mi>x</mi><mn>2</mn></msub><mi>′</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mn>0</mn></mtd><mtd columnalign="center" style="text-align: center"><mi>−</mi><mn>6</mn><msub><mi>x</mi><mn>2</mn></msub><mspace width="0.167em"></mspace><mi>d</mi><msub><mi>x</mi><mn>2</mn></msub><mi>′</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">df' = 
\begin{pmatrix}
2\,dx_1' \sin(x_2) + 2x_1 \cos(x_2) \,dx_2' &amp; 2 x_1 \, dx_1' \cos(x_2) - x_1^2 \sin(x_2)\, dx_2' \\
0 &amp; -6x_2\,dx_2'
\end{pmatrix}
= f''(x)[dx']</annotation></semantics></math> That is, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">df'</annotation></semantics></math> is a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2\times 2</annotation></semantics></math> matrix of “infinitesimal” entries, of the same shape as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math>.</p>
<p>From this viewpoint, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f''(x)</annotation></semantics></math> is a linear operator acting on vectors <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">dx'</annotation></semantics></math> and outputting <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2\times 2</annotation></semantics></math> matrices <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">f''(x)[dx']</annotation></semantics></math>, but this is one of the many cases where it is easier to write down the linear operator as a “rule” than as a “thing” like a matrix. The “thing” would have to either be some kind of “three-dimensional matrix” or we would have to “vectorize” <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math> into a “column vector” of 4 entries in order to write its <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>×</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">4\times 4</annotation></semantics></math> Jacobian, as in Sec.&nbsp;<a href="#sec:kronecker" data-reference-type="ref" data-reference="sec:kronecker">3</a> (which can obscure the underlying structure of the problem).</p>
<p>Furthermore, since this <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">df'</annotation></semantics></math> is a linear operator (a matrix), we can act it on <em>another</em> vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><mi>d</mi><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">dx = (dx_1, dx_2)</annotation></semantics></math> to obtain: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>d</mi><msub><mi>x</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>d</mi><msub><mi>x</mi><mn>2</mn></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mn>2</mn><mo>sin</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>d</mi><msub><mi>x</mi><mn>1</mn></msub><mi>′</mi><mspace width="0.167em"></mspace><mi>d</mi><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mn>2</mn><msub><mi>x</mi><mn>1</mn></msub><mo>cos</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mspace width="0.167em"></mspace><mi>d</mi><msub><mi>x</mi><mn>2</mn></msub><mi>′</mi><mspace width="0.167em"></mspace><mi>d</mi><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mspace width="0.167em"></mspace><mi>d</mi><msub><mi>x</mi><mn>1</mn></msub><mi>′</mi><mspace width="0.167em"></mspace><mi>d</mi><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msubsup><mi>x</mi><mn>1</mn><mn>2</mn></msubsup><mo>sin</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>d</mi><msub><mi>x</mi><mn>2</mn></msub><mi>′</mi><mspace width="0.167em"></mspace><mi>d</mi><msub><mi>x</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>−</mi><mn>6</mn><msub><mi>x</mi><mn>2</mn></msub><mspace width="0.167em"></mspace><mi>d</mi><msub><mi>x</mi><mn>2</mn></msub><mi>′</mi><mspace width="0.167em"></mspace><mi>d</mi><msub><mi>x</mi><mn>2</mn></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">df' \begin{pmatrix} dx_1 \\ dx_2 \end{pmatrix} =
\begin{pmatrix}
2 \sin(x_2) \,dx_1'\,dx_1 + 2x_1 \cos(x_2) (\,dx_2'\,dx_1 + \, dx_1' \, dx_2) - x_1^2 \sin(x_2)\, dx_2' \, dx_2 \\
-6x_2\,dx_2' \,dx_2
\end{pmatrix} = f''(x)[dx'][dx] \, .</annotation></semantics></math> Notice that <em>this</em> result, which we will call <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo>,</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">f''(x)[dx', dx]</annotation></semantics></math> below, is the “same shape” as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math> (a 2-component vector). Moreover, it doesn’t change if we swap <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">dx'</annotation></semantics></math>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo>,</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo>,</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">f''(x)[dx', dx] = f''(x)[dx, dx']</annotation></semantics></math>, a key symmetry of the second derivative that we will discuss further below.</p>
</div>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">df'</annotation></semantics></math> is an (infinitesimal) object of the same “shape” as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x)</annotation></semantics></math>, <em>not</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math>. Here, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math> is a linear operator, so its change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">df'</annotation></semantics></math> must <em>also</em> be an (infinitesimal) linear operator (a “small change” in a linear operator) that we can therefore act on an arbitrary <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> (or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math>), in the form: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>:=</mo><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo>,</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mspace width="0.167em"></mspace><mo>,</mo></mrow><annotation encoding="application/x-tex">df'[dx] = f''(x) [dx'][dx] := f''(x) [dx', dx] \, ,</annotation></semantics></math> where we combine the two brackets for brevity. This final result <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo>,</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">f''(x) [dx', dx]</annotation></semantics></math> is the same type of object (vector) as the original output <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math>. This implies that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f''(x)</annotation></semantics></math> is a <em>bilinear map</em>: acting on <strong>two</strong> vectors, and linear in either vector taken individually. (We will see shortly that the ordering of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">dx'</annotation></semantics></math> doesn’t matter: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo>,</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo>,</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">f''(x)[dx',dx]=f''(x)[dx,dx']</annotation></semantics></math>.)</p>
<p>More precisely, we have the following.</p>
<div class="definition">
<p>Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi><mo>,</mo><mi>V</mi><mo>,</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">U,V,W</annotation></semantics></math> be a vector spaces, not necessarily the same. Then, a <strong>bilinear map</strong> is a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo>:</mo><mi>U</mi><mo>×</mo><mi>V</mi><mo>→</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">B:U\times V \to W</annotation></semantics></math>, mapping a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>∈</mo><mi>U</mi></mrow><annotation encoding="application/x-tex">u \in U</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>∈</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">v \in V</annotation></semantics></math> to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>∈</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">B[u,v] \in W</annotation></semantics></math>, such that we have linearity in both arguments: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="left" style="text-align: left"><mi>B</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>u</mi><mo>,</mo><mi>α</mi><msub><mi>v</mi><mn>1</mn></msub><mo>+</mo><mi>β</mi><msub><mi>v</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>α</mi><mi>B</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>u</mi><mo>,</mo><msub><mi>v</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><mi>β</mi><mi>B</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>u</mi><mo>,</mo><msub><mi>v</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr><mtr><mtd columnalign="left" style="text-align: left"><mi>B</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>α</mi><msub><mi>u</mi><mn>1</mn></msub><mo>+</mo><mi>β</mi><msub><mi>u</mi><mn>2</mn></msub><mo>,</mo><mi>v</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>α</mi><mi>B</mi><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>u</mi><mn>1</mn></msub><mo>,</mo><mi>v</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><mi>β</mi><mi>B</mi><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>u</mi><mn>2</mn></msub><mo>,</mo><mi>v</mi><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex">\begin{cases}
        B[u, \alpha v_1 + \beta v_2] = \alpha B[u,v_1] + \beta B[u,v_2] \\
        B[\alpha u_1 + \beta u_2, v] = \alpha B[u_1,v] + \beta B[u_2,v]
    \end{cases}</annotation></semantics></math> for any scalars <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>,</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\alpha, \beta</annotation></semantics></math>,</p>
<p>If <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>=</mo><mi>ℝ</mi></mrow><annotation encoding="application/x-tex">W = \mathbb{R}</annotation></semantics></math>, i.e.&nbsp;the output is a scalar, then it is called a <strong>bilinear form</strong>.</p>
</div>
<p>Note that in general, even if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi><mo>=</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">U = V</annotation></semantics></math> (the two inputs <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>,</mo><mi>v</mi></mrow><annotation encoding="application/x-tex">u,v</annotation></semantics></math> are the “same type” of vector) we may have <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>u</mi><mo>,</mo><mi>v</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>≠</mo><mi>B</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>v</mi><mo>,</mo><mi>u</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">B[u,v] \neq B[v,u]</annotation></semantics></math>, but in the case of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi></mrow><annotation encoding="application/x-tex">f''</annotation></semantics></math> we have something very special that happens. In particular, we can show that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f''(x)</annotation></semantics></math> is a <em>symmetric bilinear map</em>, meaning <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo>,</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo>,</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">f''(x) [dx', dx] = f''(x) [dx, dx']</annotation></semantics></math> for any <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">dx'</annotation></semantics></math>. Why? Because, applying the definition of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi></mrow><annotation encoding="application/x-tex">f''</annotation></semantics></math> as giving the change in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math> from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">dx'</annotation></semantics></math>, and then the definition of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math> as giving the change in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math>, we can re-order terms to obtain: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo>,</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>−</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><munder><munder><mrow><mi>d</mi><mi>x</mi><mi>′</mi><mo>+</mo><mi>d</mi><mi>x</mi></mrow><mo accent="true">⏟</mo></munder><mrow><mo>=</mo><mi>d</mi><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mi>′</mi></mrow></munder><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><menclose notation="box"><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></menclose></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>−</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">]</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo>,</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">]</mo></mrow><mspace width="0.167em"></mspace></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    f''(x) [dx', dx] &amp;= f'(x + 
    dx') [dx] - f'(x) [dx] \\
    &amp;= \left(f(x+ \underbrace{dx' + dx}_{= dx + dx'}) - f(x+ dx')\right) - \left(f(x+ dx) - f(x)\right) \\ 
    &amp;= \boxed{f(x + dx + dx') + f(x) - f(x + dx) - f(x + dx')} \\
    &amp;= \left(f(x + dx + dx')  - f(x + dx)\right) - \left(f(x + dx') - f(x)\right) \\
    &amp;= f'(x + 
    dx) [dx'] - f'(x) [dx'] \\
    &amp;= f''(x) [dx, dx'] \, 
\end{aligned}</annotation></semantics></math> where we’ve boxed the middle formula for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi></mrow><annotation encoding="application/x-tex">f''</annotation></semantics></math> which emphasizes its symmetry in a natural way. (The basic reason why this works is that the “<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>+</mi><annotation encoding="application/x-tex">+</annotation></semantics></math>” operation is always <em>commutative</em> for any vector space. A geometric interpretation is depicted in Fig.&nbsp;<a href="#fig:second-deriv" data-reference-type="ref" data-reference="fig:second-deriv">12</a>.)</p>
<figure id="fig:second-deriv" class="figure">
<div class="center">
<embed src="figures/second-derivative.pdf.svg" style="width:80.0%">
</div>
<figcaption>
Geometric interpretation of <span class="math inline"><em>f</em><sup>″</sup>(<em>x</em>)[<em>d</em><em>x</em>, <em>d</em><em>x</em><sup>′</sup>]</span>: To first order, a function <span class="math inline"><em>f</em></span> maps parallelograms to parallelograms. To second order, however it “opens” parallelograms: The deviation from point <span class="math inline"><em>B</em></span> (the image of <span class="math inline"><em>A</em></span>) from point <span class="math inline"><em>C</em></span> (the completion of the parallelogram) is the second derivative <span class="math inline"><em>f</em><sup>″</sup>(<em>x</em>)[<em>d</em><em>x</em>, <em>d</em><em>x</em><sup>′</sup>]</span>. The symmetry of <span class="math inline"><em>f</em><sup>″</sup></span> as a bilinear form can be traced back geometrically to the mirror symmetry of the input parallelogram across its diagonal from <span class="math inline"><em>x</em></span> to point&nbsp;A. <span id="fig:second-deriv" data-label="fig:second-deriv"></span>
</figcaption>
</figure>
<div class="example">
<p>Let’s review the familiar example from multivariable calculus, <span class="math inline">$f: \R^n \to \R$</span>. That is, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math> is a scalar-valued function of a column vector <span class="math inline">$x\in \R^n$</span>. What is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi></mrow><annotation encoding="application/x-tex">f''</annotation></semantics></math>?</p>
</div>
<p>Recall that <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mo>⟹</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mrow><mtext mathvariant="normal">scalar </mtext><mspace width="0.333em"></mspace></mrow><mi>d</mi><mi>f</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mi>d</mi><mi>x</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">f'(x) = (\nabla f)^T \implies f'(x) [dx] = \text{scalar } df = (\nabla f)^T dx.</annotation></semantics></math> Similarly, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo>,</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">scalar from two vectors, linear in both</mtext></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>d</mi><mi>x</mi><msup><mi>′</mi><mi>T</mi></msup><mi>H</mi><mi>d</mi><mi>x</mi><mspace width="0.167em"></mspace><mo>,</mo></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    f''(x) [dx', dx] &amp;= \text{scalar from two vectors, linear in both} \\
    &amp;= dx'^T H dx \, ,
\end{aligned}</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math> must be exactly the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \times n</annotation></semantics></math> matrix <strong>Hessian matrix</strong> introduced in Sec.&nbsp;<a href="#sec:Hessian-scalar" data-reference-type="ref" data-reference="sec:Hessian-scalar">12.1</a>, since an expression like <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi><msup><mi>′</mi><mi>T</mi></msup><mi>H</mi><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx'^T H dx</annotation></semantics></math> is the most general possible bilinear form mapping two vectors to a scalar. Moreover, since we now know that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi></mrow><annotation encoding="application/x-tex">f''</annotation></semantics></math> is always a <em>symmetric</em> bilinear form, we must have: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo>,</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>d</mi><mi>x</mi><msup><mi>′</mi><mi>T</mi></msup><mi>H</mi><mi>d</mi><mi>x</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo>,</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>d</mi><msup><mi>x</mi><mi>T</mi></msup><mi>H</mi><mi>d</mi><mi>x</mi><mi>′</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><msup><mi>x</mi><mi>T</mi></msup><mi>H</mi><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mspace width="2.0em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi></mrow><mo>=</mo><msup><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi></mrow><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mi>d</mi><mi>x</mi><msup><mi>′</mi><mi>T</mi></msup><msup><mi>H</mi><mi>T</mi></msup><mi>d</mi><mi>x</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
f''(x) [dx', dx] &amp;=dx'^T H dx \\
&amp;= f''(x) [dx, dx'] = dx^T H dx' =
     (dx^T H dx')^T  \qquad (\mathrm{scalar} = \mathrm{scalar}^T)\\
    &amp;= dx'^T H^T dx 
\end{aligned}</annotation></semantics></math> for all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">dx'</annotation></semantics></math>. This implies that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>=</mo><msup><mi>H</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">H = H^T</annotation></semantics></math>: the Hessian matrix is symmetric. As discussed in Sec.&nbsp;<a href="#sec:Hessian-scalar" data-reference-type="ref" data-reference="sec:Hessian-scalar">12.1</a>, we already knew this from multi-variable calculus. Now, however, this “equality of mixed partial derivatives” is simply a special case of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi></mrow><annotation encoding="application/x-tex">f''</annotation></semantics></math> being a symmetric bilinear map.</p>
<p>As an example, let’s consider a special case of the general formula above:</p>
<div class="example">
<p>Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><mi>A</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">f(x) = x^T A x</annotation></semantics></math> for <span class="math inline">$x \in \R^n$</span> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \times n</annotation></semantics></math> matrix. As above, <span class="math inline">$f(x) \in \R$</span> (scalar outputs). Compute <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi></mrow><annotation encoding="application/x-tex">f''</annotation></semantics></math>.</p>
</div>
<p>The computation is fairly straightforward. Firstly, we have that <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>+</mo><msup><mi>A</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">f' = (\nabla f)^T = x^T(A + A^T).</annotation></semantics></math> This implies that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>+</mo><msup><mi>A</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">\nabla f = (A + A^T) x</annotation></semantics></math>, a linear function of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>. Hence, the Jacobian of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> is the Hessian <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi><mo>=</mo><mi>H</mi><mo>=</mo><mi>A</mi><mo>+</mo><msup><mi>A</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">f'' = H = A + A^T</annotation></semantics></math>. Furthermore, note that this implies <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><mi>A</mi><mi>x</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>x</mi><mi>T</mi></msup><mi>A</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mspace width="2.0em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi></mrow><mo>=</mo><msup><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi></mrow><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><msup><mi>A</mi><mi>T</mi></msup><mi>x</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>x</mi><mi>T</mi></msup><mi>A</mi><mi>x</mi><mo>+</mo><msup><mi>x</mi><mi>T</mi></msup><msup><mi>A</mi><mi>T</mi></msup><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mi>x</mi><mi>T</mi></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>+</mo><msup><mi>A</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>x</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mi>x</mi><mi>T</mi></msup><mi>H</mi><mi>x</mi><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>x</mi><mo>,</mo><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mspace width="0.167em"></mspace><mo>,</mo></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    f(x) &amp;= x^T A x = (x^T A x)^T  \qquad (\mathrm{scalar} = \mathrm{scalar}^T)\\
    &amp;= x^T A^T x \\
    &amp;= \frac{1}{2} (x^T A x + x^T A^T x) = \frac{1}{2} x^T (A + A^T) x\\
    &amp;= \frac{1}{2} x^T H x = \frac{1}{2} f''[x,x] \, ,
\end{aligned}</annotation></semantics></math> which will turn out to be a special case of the quadratic approximations of Sec.&nbsp;<a href="#sec:Hessian-quadratic" data-reference-type="ref" data-reference="sec:Hessian-quadratic">12.3</a> (exact in this example since <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><mi>A</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">f(x)=x^T A x</annotation></semantics></math> is quadratic to start with).</p>
<div class="example">
<p>Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>det</mo><mi>A</mi></mrow><annotation encoding="application/x-tex">f(A) = \det A</annotation></semantics></math> for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n\times n</annotation></semantics></math> matrix. Express <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f''(A)</annotation></semantics></math> as a rule for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>A</mi><mo>,</mo><mi>d</mi><mi>A</mi><mi>′</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">f''(A)[dA,dA']</annotation></semantics></math> in terms of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">dA</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>A</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">dA'</annotation></semantics></math>.</p>
</div>
<p>From lecture 3, we have the first derivative <span class="math display">$$f'(A) [dA] = df = \det (A) \tr(A^{-1} dA).$$</span> Now, we want to compute the change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>d</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>A</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>+</mo><mi>d</mi><mi>A</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>A</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>−</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>A</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">d'(df) = d'(f'(A)[dA]) = f'(A+dA')[dA] - f'(A)[dA]</annotation></semantics></math> in this formula, i.e.&nbsp;the differential (denoted <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">d'</annotation></semantics></math>) where we change <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>A</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">dA'</annotation></semantics></math> while treating <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>A</mi></mrow><annotation encoding="application/x-tex">dA</annotation></semantics></math> as a constant: <span class="math display">$$\begin{aligned}
    f''(A) [dA, dA'] &amp;= d' (\det A \tr (A^{-1} dA)) \\
    &amp;= \det A \tr(A^{-1} dA') \tr(A^{-1} dA) - \det A \tr(A^{-1} \,dA' A^{-1}\, dA) \\
    &amp;= f''(A) [dA', dA]
\end{aligned}$$</span> where the last line (symmetry) can be derived explicitly by the cyclic property of the trace (although of course it must be true for any <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi></mrow><annotation encoding="application/x-tex">f''</annotation></semantics></math>). Although <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi></mrow><annotation encoding="application/x-tex">f''</annotation></semantics></math> here is a perfectly good bilinear form acting on matrices <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>A</mi><mo>,</mo><mi>d</mi><mi>A</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">dA,dA'</annotation></semantics></math>, it is not very natural to express <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi></mrow><annotation encoding="application/x-tex">f''</annotation></semantics></math> as a “Hessian matrix.”</p>
<p>If we really wanted to express <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi></mrow><annotation encoding="application/x-tex">f''</annotation></semantics></math> in terms of an explicit Hessian matrix, we could use the the “vectorization” approach of Sec.&nbsp;<a href="#sec:kronecker" data-reference-type="ref" data-reference="sec:kronecker">3</a>. Let us consider, for example, the term <span class="math inline">$\tr(A^{-1} \,dA' A^{-1}\, dA)$</span> using Kronecker products (Sec.&nbsp;<a href="#sec:kronecker" data-reference-type="ref" data-reference="sec:kronecker">3</a>). In general, for matrices <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>,</mo><mi>Y</mi><mo>,</mo><mi>B</mi><mo>,</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">X,Y,B,C</annotation></semantics></math>: <span class="math display">$$(\operatorname{vec}{X})^T (B \otimes C) \operatorname{vec}{Y} = (\operatorname{vec}{X})^T \operatorname{vec}{(CYB^T)} = \tr(X^T CYB^T) = \tr(B^T X^T CY) \, ,$$</span> recalling that <span class="math inline">$(\operatorname{vec}{X})^T \operatorname{vec}{Y} = \tr(X^T Y)$</span> is the Frobenius inner product (Sec.&nbsp;<a href="#sec:generalvectorspaces" data-reference-type="ref" data-reference="sec:generalvectorspaces">5</a>). Thus, <span class="math display">$$\tr(A^{-1} \,dA' A^{-1}\, dA)
= \operatorname{vec}{(dA'^T)}^T (A^{-T} \otimes A^{-1}) \operatorname{vec}{(dA)} \, .$$</span> This is still not quite in the form we want for a Hessian matrix, however, because it involves <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>vec</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>A</mi><msup><mi>′</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{vec}{(dA'^T)}</annotation></semantics></math> rather than <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>vec</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>A</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{vec}{(dA')}</annotation></semantics></math> (the two vectors are related by a permutation matrix, sometimes called a “commutation” matrix). Completing this calculation would be a nice exercise in mastery of Kronecker products, but getting an explicit Hessian seems like a lot of algebra for a result of dubious utility!</p>
</section>
<section id="sec:Hessian-quadratic" class="level2">
<h2 class="anchored" data-anchor-id="sec:Hessian-quadratic">Generalized quadratic approximation</h2>
<p>So how do we ultimately think about <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi></mrow><annotation encoding="application/x-tex">f''</annotation></semantics></math>? We know that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math> is the linearization/linear approximation of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math>, i.e. <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><mi>o</mi><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">‖</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">‖</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">f(x + \delta x ) = f(x) + f'(x) [\delta x] + o(\lVert \delta x\rVert).</annotation></semantics></math> Now, just as we did for the simple case of Hessian matrices in Sec.&nbsp;<a href="#sec:Hessian-scalar" data-reference-type="ref" data-reference="sec:Hessian-scalar">12.1</a> above, we can use <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi></mrow><annotation encoding="application/x-tex">f''</annotation></semantics></math> to form a <em>quadratic approximation</em> of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math>. In particular, one can show that <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>δ</mi><mi>x</mi><mo>,</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><mi>o</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mrow><mo stretchy="true" form="prefix">‖</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">‖</mo></mrow><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">f(x+ \delta x) = f(x) + f'(x) [\delta x] + \frac{1}{2} f''(x) [\delta x, \delta x] +  o (\lVert \delta x\rVert^2).</annotation></semantics></math> Note that the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mn>1</mn><mn>2</mn></mfrac><annotation encoding="application/x-tex">\frac{1}{2}</annotation></semantics></math> factor is just as in the Taylor series. To derive this, simply plug the quadratic approximation into <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo>,</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>d</mi><mi>x</mi><mi>′</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">f''(x) [dx, dx'] = f(x + dx + dx') + f(x) - f(x + dx) - f(x + dx').</annotation></semantics></math> and check that the right-hand side reproduces <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f''(x)</annotation></semantics></math>. (Note how <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">dx'</annotation></semantics></math> appear symmetrically in this formula, which reflects the symmetry of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi></mrow><annotation encoding="application/x-tex">f''</annotation></semantics></math>.)</p>
</section>
<section id="hessians-and-optimization" class="level2">
<h2 class="anchored" data-anchor-id="hessians-and-optimization">Hessians and optimization</h2>
<p>Many important applications of second derivatives, Hessians, and quadratic approximations arise in optimization: minimization (or maximization) of functions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math>.<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a></p>
<section id="newton-like-methods" class="level3">
<h3 class="anchored" data-anchor-id="newton-like-methods">Newton-like methods</h3>
<p>When searching for a local minimum (or maximum) of a complicated function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math>, a common procedure is to approximate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x+\delta x)</annotation></semantics></math> by a simpler “model” function for small <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math>, and then to optimize this model to obtain a potential optimization step. For example, approximating <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>≈</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">f(x+\delta x) \approx f(x)+f'(x)[\delta x]</annotation></semantics></math> (an affine model, colloquially called “linear”) leads to gradient descent and related algorithms. A better approximation for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x + \delta x)</annotation></semantics></math> will often lead to faster-converging algorithms, and so a natural idea is to exploit the <em>second</em> derivative <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi></mrow><annotation encoding="application/x-tex">f''</annotation></semantics></math> to make a quadratic model, as above, and accelerate optimization.</p>
<p>For unconstrained optimization, minimizing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math> corresponds to finding a root of the derivative <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">f' = 0</annotation></semantics></math> (i.e., <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\nabla f = 0</annotation></semantics></math>), and a <em>quadratic</em> approximation for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> yields a first-order (affine) approximation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>≈</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">f'(x + \delta x) \approx f'(x) + f''(x)[\delta x]</annotation></semantics></math> for the derivative <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math>. In <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>ℝ</mi><mi>n</mi></msup><annotation encoding="application/x-tex">\mathbb{R}^n</annotation></semantics></math>, this is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>≈</mo><mi>H</mi><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta(\nabla f) \approx H \delta x</annotation></semantics></math>. So, minimizing a quadratic model is effectively a <em>Newton step</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi><mo>≈</mo><mi>−</mi><msup><mi>H</mi><mrow><mi>−</mi><mn>1</mn></mrow></msup><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\delta x \approx -H^{-1} \nabla f</annotation></semantics></math> to find a root of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\nabla f</annotation></semantics></math> via first-order approximation. Thus, optimization via quadratic approximations is often viewed as a form of Newton algorithm. As discussed below, it is also common to employ <em>approximate</em> Hessians in optimization, resulting in “quasi-Newton” algorithms.</p>
<p>More complicated versions of this idea arise in optimization with constraints, e.g.&nbsp;minimizing an objective function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math> subject to one or more nonlinear inequality constraints <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>k</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>≤</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">c_k(x) \le 0</annotation></semantics></math>. In such cases, there are a variety of methods that take both first and second derivatives into account, such as “sequential quadratic programming”<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> (SQP) algorithms that solve a sequence of “QP” approximations involving quadratic objectives with affine constraints (see e.g.&nbsp;the book <em>Numerical Optimization</em> by Nocedal and Wright, 2006).</p>
<p>There are many technical details, beyond the scope of this course, that must be resolved in order to translate such high-level ideas into practical algorithms. For example, a quadratic model is only valid for small enough <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math>, so there must be some mechanism to limit the step size. One possibility is “backtracking line search”: take a Newton step <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">x+\delta x</annotation></semantics></math> and, if needed, progressively “backtrack” to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mi>/</mi><mn>10</mn><mo>,</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mi>/</mi><mn>100</mn><mo>,</mo><mi>…</mi></mrow><annotation encoding="application/x-tex">x+\delta x/10, x+\delta x/100, \ldots</annotation></semantics></math> until a sufficiently decreased value of the objective is found. Another commonplace idea is a “trust region”: optimize the model with the constraint that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math> is sufficiently small, e.g.&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">‖</mo><mi>δ</mi><mi>x</mi><mo stretchy="false" form="postfix">‖</mo><mo>≤</mo><mi>s</mi></mrow><annotation encoding="application/x-tex">\Vert \delta x \Vert \le s</annotation></semantics></math> (a spherical trust region), along with some rules to adaptively enlarge or shrink the trust-region size (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>) depending on how well the model predicts <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">\delta f</annotation></semantics></math>. There are many variants of Newton/SQP-like algorithms depending on the choices made for these and other details.</p>
</section>
<section id="computing-hessians" class="level3">
<h3 class="anchored" data-anchor-id="computing-hessians">Computing Hessians</h3>
<p>In general, finding <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi></mrow><annotation encoding="application/x-tex">f''</annotation></semantics></math> or the Hessian is often computationally expensive in higher dimensions. If <span class="math inline">$f(x): \R^n\to\R$</span>, then the Hessian, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math>, is an <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n\times n</annotation></semantics></math> matrix, which can be huge if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> is large—even storing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math> may be prohibitive, much less computing it. When using automatic differentiation (AD), Hessians are often computed by a <em>combination</em> of forward and reverse modes (Sec.&nbsp;<a href="#sec:forward-over-reverse" data-reference-type="ref" data-reference="sec:forward-over-reverse">8.4.1</a>), but AD does not circumvent the fundamental scaling difficulty for large&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>.</p>
<p>Instead of computing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math> explicitly, however, one can instead <em>approximate</em> the Hessian in various ways; in the context of optimization, approximate Hessians are found in “quasi-Newton” methods such as the famous “BFGS” algorithm and its variants. One can also derive efficient methods to compute Hessian–vector products <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mi>v</mi></mrow><annotation encoding="application/x-tex">Hv</annotation></semantics></math> without computing <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math> explicitly, e.g.&nbsp;for use in Newton–Krylov methods. (Such a product <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mi>v</mi></mrow><annotation encoding="application/x-tex">Hv</annotation></semantics></math> is equivalent to a directional derivative of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi></mrow><annotation encoding="application/x-tex">f'</annotation></semantics></math>, which is efficiently computed by “forward-over-reverse” AD as in Sec.&nbsp;<a href="#sec:forward-over-reverse" data-reference-type="ref" data-reference="sec:forward-over-reverse">8.4.1</a>.)</p>
</section>
<section id="minima-maxima-and-saddle-points" class="level3">
<h3 class="anchored" data-anchor-id="minima-maxima-and-saddle-points">Minima, maxima, and saddle points</h3>
<p>Generalizing the rules you may recall from single- and multi-variable calculus, we can use the second derivative to determine whether an extremum is a minimum, maximum, or saddle point. Firstly, an extremum of a scalar function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math> is a point <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mn>0</mn></msub><annotation encoding="application/x-tex">x_0</annotation></semantics></math> such that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">f'(x_0) = 0</annotation></semantics></math>. That is, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">f'(x_0) [\delta x] = 0</annotation></semantics></math> for <em>any</em> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\delta x</annotation></semantics></math>. Equivalently, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi><msub><mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">|</mo><msub><mi>x</mi><mn>0</mn></msub></msub><mo>=</mo><mi>f</mi><mi>′</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mo>=</mo><mn>0</mn><mi>.</mi></mrow><annotation encoding="application/x-tex">\nabla f \bigr|_{x_0} = f'(x_0)^T = 0.</annotation></semantics></math></p>
<p>Using our quadratic approximation around <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mn>0</mn></msub><annotation encoding="application/x-tex">x_0</annotation></semantics></math>, we then have that <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><munder><munder><mrow><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><mo accent="true">⏟</mo></munder><mrow><mo>=</mo><mn>0</mn></mrow></munder><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>δ</mi><mi>x</mi><mo>,</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><mi>o</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mrow><mo stretchy="true" form="prefix">‖</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">‖</mo></mrow><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">f(x_0 + \delta x) = f(x_0) + \underbrace{f'(x_0) [\delta x]}_{=0} + \frac{1}{2} f''(x_0) [\delta x, \delta x] + o(\lVert \delta x\rVert^2).</annotation></semantics></math> The definition of a local minimum <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mn>0</mn></msub><annotation encoding="application/x-tex">x_0</annotation></semantics></math> is that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>&gt;</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x_0 + \delta x ) &gt; f(x_0)</annotation></semantics></math> for any <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mi>x</mi><mo>≠</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\delta x \neq 0</annotation></semantics></math> with <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">‖</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">‖</mo></mrow><annotation encoding="application/x-tex">\lVert \delta x\rVert</annotation></semantics></math> sufficiently small. To achieve this at a point where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">f' = 0</annotation></semantics></math>, it is enough to have <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi></mrow><annotation encoding="application/x-tex">f''</annotation></semantics></math> be a positive-definite quadratic form: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>δ</mi><mi>x</mi><mo>,</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>&gt;</mo><mn>0</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> for all </mtext><mspace width="0.333em"></mspace></mrow><mi>δ</mi><mi>x</mi><mo>≠</mo><mn>0</mn><mo>⇔</mo><mrow><mtext mathvariant="bold">𝐩𝐨𝐬𝐢𝐭𝐢𝐯𝐞-𝐝𝐞𝐟𝐢𝐧𝐢𝐭𝐞 </mtext><mspace width="0.333em"></mspace></mrow><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>.</mi></mrow><annotation encoding="application/x-tex">f''(x_0) [\delta x, \delta x] &gt;0 \text{ for all } \delta x \ne 0 \iff \textbf{positive-definite } f''(x_0) \, .</annotation></semantics></math></p>
<p>For example, for inputs <span class="math inline">$x\in \R^n$</span>, so that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi></mrow><annotation encoding="application/x-tex">f''</annotation></semantics></math> is a real-symmetric <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \times n</annotation></semantics></math> Hessian matrix, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>H</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">f''(x_0) = H(x_0) = H(x_0)^T</annotation></semantics></math>, this corresponds to the usual criteria for a positive-definite matrix: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>δ</mi><mi>x</mi><mo>,</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>δ</mi><msup><mi>x</mi><mi>T</mi></msup><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>δ</mi><mi>x</mi><mo>&gt;</mo><mn>0</mn><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> for all </mtext><mspace width="0.333em"></mspace></mrow><mi>δ</mi><mi>x</mi><mo>≠</mo><mn>0</mn><mo>⇔</mo><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> positive-definite </mtext><mspace width="0.333em"></mspace></mrow><mo>⇔</mo><mrow><mtext mathvariant="normal">all eigenvalues of </mtext><mspace width="0.333em"></mspace></mrow><mi>H</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>&gt;</mo><mn>0</mn><mi>.</mi></mrow><annotation encoding="application/x-tex">f''(x_0) [\delta x, \delta x] = \delta x^T H(x_0) \delta x &gt;0 \text{ for all } \delta x \ne 0 \iff H(x_0) \text{ positive-definite } \iff \text{all eigenvalues of } H(x_0) &gt; 0.</annotation></semantics></math></p>
<p>In first-year calculus, one often focuses in particular on the 2-dimensional case, where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math> is a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2\times 2</annotation></semantics></math> matrix. In the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2\times 2</annotation></semantics></math> case, there is a simple way to check the signs of the two eigenvalues of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math>, in order to check whether an extremum is a minimum or maximum: the eigenvalues are both positive if and only if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>det</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>H</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\det (H) &gt;0</annotation></semantics></math> and <span class="math inline">$\tr(H)  &gt;0$</span>, since <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>det</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>H</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>λ</mi><mn>1</mn></msub><msub><mi>λ</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\det (H) = \lambda_1 \lambda_2</annotation></semantics></math> and <span class="math inline">$\tr(H) = \lambda_1 + \lambda_2$</span>. In higher dimensions, however, one needs more complicated techniques to compute eigenvalues and/or check positive-definiteness, e.g.&nbsp;as discussed in MIT courses 18.06 (Linear Algebra) and/or 18.335 (Introduction to Numerical Methods). (In practice, one typically checks positive-definiteness by performing a form of Gaussian elimination, called a Cholesky factorization, and checking that the diagonal “pivot” elements are <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">&gt; 0</annotation></semantics></math>, rather than by computing eigenvalues which are much more expensive.)</p>
<p>Similarly, a point <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mn>0</mn></msub><annotation encoding="application/x-tex">x_0</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\nabla f = 0</annotation></semantics></math> is a local <em>maximum</em> if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi></mrow><annotation encoding="application/x-tex">f''</annotation></semantics></math> is negative-definite, or equivalently if the eigenvalues of the Hessian are all negative. Additionally, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mn>0</mn></msub><annotation encoding="application/x-tex">x_0</annotation></semantics></math> is a <em>saddle</em> point if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi></mrow><annotation encoding="application/x-tex">f''</annotation></semantics></math> is indefinite, i.e.&nbsp;the eigenvalues include both positive and negative values. However, cases where some eigenvalues are zero are more complicated to analyze; e.g.&nbsp;if the eigenvalues are all <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\ge 0</annotation></semantics></math> but some are <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">=0</annotation></semantics></math>, then whether the point is a minimum depends upon higher derivatives.</p>
</section>
</section>
<section id="further-reading-1" class="level2">
<h2 class="anchored" data-anchor-id="further-reading-1">Further Reading</h2>
<p>All of this formalism about “bilinear forms” and so forth may seem like a foray into abstraction for the sake of abstraction. Can’t we always reduce things to ordinary matrices by choosing a basis (“vectorizing” our inputs and outputs)? However, we often don’t <em>want</em> to do this for the same reason that we often prefer to express first derivatives as linear operators rather than as explicit Jacobian matrices. Writing linear or bilinear operators as explicit matrices, e.g.&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>vec</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mspace width="0.167em"></mspace><mi>d</mi><mi>A</mi><mo>+</mo><mi>d</mi><mi>A</mi><mspace width="0.167em"></mspace><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>I</mi><mo>⊗</mo><mi>A</mi><mo>+</mo><msup><mi>A</mi><mi>T</mi></msup><mo>⊗</mo><mi>I</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>vec</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\operatorname{vec}(A\,dA + dA\,A) = (I\otimes A + A^T \otimes I)\operatorname{vec}(dA)</annotation></semantics></math> as in Sec.&nbsp;<a href="#sec:kronecker" data-reference-type="ref" data-reference="sec:kronecker">3</a>, often disguises the underlying structure of the operator and introduces a lot of algebraic complexity for no purpose, as well as being potentially computationally costly (e.g.&nbsp;exchanging small matrices <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> for large ones <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo>⊗</mo><mi>A</mi></mrow><annotation encoding="application/x-tex">I \otimes A</annotation></semantics></math>).</p>
<p>As we discussed in this chapter, an important generalization of quadratic operations to arbitrary vector spaces come in the form of <a href="https://en.wikipedia.org/wiki/Bilinear_map">bilinear maps</a> and <a href="https://en.wikipedia.org/wiki/Bilinear_form">bilinear forms</a>, and there are many textbooks and other sources discussing these ideas and variations thereof. For example, we saw that the second derivative can be seen as a <a href="https://en.wikipedia.org/wiki/Symmetric_bilinear_form">symmetric bilinear form</a>. This is closely related to a <a href="https://en.wikipedia.org/wiki/Quadratic_form">quadratic form</a> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">Q[x]</annotation></semantics></math>, which what we get by plugging the same vector twice into a symmetric bilinear form <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>B</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>y</mi><mo>,</mo><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">B[x,y]=B[y,x]</annotation></semantics></math>, i.e.&nbsp;<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>B</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>x</mi><mo>,</mo><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">Q[x] = B[x,x]</annotation></semantics></math>. (At first glance, it may seem like <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> carries “less information” than <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>, but in fact this is not the case. It is easy to see that one can recover <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> via <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Q</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>x</mi><mo>+</mo><mi>y</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>−</mo><mi>Q</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>x</mi><mo>−</mo><mi>y</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mn>4</mn></mrow><annotation encoding="application/x-tex">B[x,y] = (Q[x+y] - Q[x-y])/4</annotation></semantics></math>, called a “polarization identity.”) For example, the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>δ</mi><mi>x</mi><mo>,</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">f''(x) [\delta x, \delta x]/2</annotation></semantics></math> term that appears in quadratic approximations of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>+</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x+ \delta x)</annotation></semantics></math> is a quadratic form. The most familiar multivariate version of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f''(x)</annotation></semantics></math> is the <a href="https://en.wikipedia.org/wiki/Hessian_matrix">Hessian matrix</a> when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> is a column vector and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math> is a scalar, and Khan Academy has an elementary <a href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/quadratic-approximations/a/quadratic-approximation">introduction to quadratic approximation</a>.</p>
<p><a href="https://en.wikipedia.org/wiki/Definite_matrix">Positive-definite</a> Hessian matrices, or more generally <a href="https://en.wikipedia.org/wiki/Definite_quadratic_form">definite quadratic forms</a> <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>″</mi></mrow><annotation encoding="application/x-tex">f''</annotation></semantics></math>, appear at extrema (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mi>′</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">f' =0</annotation></semantics></math>) of scalar-valued functions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math> that are local minima. There are a lot <a href="http://www.columbia.edu/~md3405/Unconstrained_Optimization.pdf">more formal treatments</a> of the same idea, and conversely Khan Academy has the <a href="https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/optimizing-multivariable-functions/a/second-partial-derivative-test">simple 2-variable version</a> where you can check the sign of the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2\times 2</annotation></semantics></math> eignevalues just by looking at the determinant and a single entry (or the trace). There’s a nice <a href="https://math.stackexchange.com/questions/2285282/relating-condition-number-of-hessian-to-the-rate-of-convergence">stackexchange discussion</a> on why an <a href="https://nhigham.com/2020/03/19/what-is-a-condition-number/">ill-conditioned</a> Hessian tends to make steepest descent converge slowly. Some Toronto <a href="https://www.cs.toronto.edu/~rgrosse/courses/csc421_2019/slides/lec07.pdf">course notes on the topic</a> may also be useful.</p>
<p>Lastly, see for example these Stanford notes on <a href="https://web.stanford.edu/class/ee364b/lectures/seq_notes.pdf">sequential quadratic optimization</a> using trust regions (Section 2.2), as well as the 18.335 <a href="https://github.com/mitmath/18335/blob/spring21/notes/BFGS.pdf">notes on BFGS quasi-Newton methods</a>. The fact that a quadratic optimization problem in a sphere has <a href="https://en.wikipedia.org/wiki/Strong_duality">strong duality</a>, and hence is efficiently solvable, is discussed in Section 5.2.4 of the <a href="https://web.stanford.edu/~boyd/cvxbook/"><em>Convex Optimization</em> book</a>. There has been a lot of work on <a href="https://en.wikipedia.org/wiki/Hessian_automatic_differentiation">automatic Hessian computation</a>, but for large-scale problems you may only be able to compute Hessian–vector products efficiently in general, which are equivalent to a directional derivative of the gradient and can be used (for example) for <a href="https://en.wikipedia.org/wiki/Newton%E2%80%93Krylov_method">Newton–Krylov methods</a>.</p>
<p>The Hessian matrix is also known as the “curvature matrix" especially in optimization. If we have a scalar function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math> of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> variables, its”graph” is the set of points <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(x,f(x))</annotation></semantics></math> in <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>R</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msup><annotation encoding="application/x-tex">R^{n+1}</annotation></semantics></math>; we call the last dimension the “vertical" dimension. At a”critical point” <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> (where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><mi>f</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\nabla f = 0</annotation></semantics></math>), then <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>v</mi><mi>T</mi></msup><mi>H</mi><mi>v</mi></mrow><annotation encoding="application/x-tex">v^T H v</annotation></semantics></math> is the ordinary curvature sometimes taught in first-year calculus, of the curve obtained by intersecting the graph with the plane in the direction <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> from <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> and the vertical (the “normal section”). The determinant of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math>, sometimes known as the Hessian determinant, yields the Gaussian curvature.</p>
<p>A closely related idea is the derivative of the unit normal. For a graph as in the preceding paragraph we may assume that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><mi>H</mi><mi>x</mi><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">f(x)=x^THx/2</annotation></semantics></math> to second order. It is easy to see that at any point <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> the tangents have the form <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>x</mi><mo>,</mo><mi>f</mi><mi>′</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>x</mi><mo>,</mo><msup><mi>x</mi><mi>T</mi></msup><mi>H</mi><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">(dx, f'(x)[dx])=(dx,x^THdx)</annotation></semantics></math> and the normal is then <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>H</mi><mi>x</mi><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(Hx,1)</annotation></semantics></math>. Near <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">x=0</annotation></semantics></math> this a unit normal to second order, and its derivative is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>H</mi><mi>d</mi><mi>x</mi><mo>,</mo><mn>0</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(Hdx,0)</annotation></semantics></math>. Projecting onto the horizontal, we see that the Hessian is the derivative of the unit normal. This is called the “shape operator" in differential geometry.</p>
</section>
</section>
<section id="derivatives-of-eigenproblems" class="level1">
<h1>Derivatives of Eigenproblems</h1>
<section id="differentiating-on-the-unit-sphere" class="level2">
<h2 class="anchored" data-anchor-id="differentiating-on-the-unit-sphere">Differentiating on the Unit Sphere</h2>
<p>Geometrically, we know that velocity vectors (equivalently, tangents) on the sphere are orthogonal to the radii. Out differentials say this algebraically, since given <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>∈</mo><msup><mi>𝕊</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">x\in \mathbb{S}^n</annotation></semantics></math> we have <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mi>T</mi></msup><mi>x</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">x^T x = 1</annotation></semantics></math>, this implies that <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><msup><mi>x</mi><mi>T</mi></msup><mi>d</mi><mi>x</mi><mo>=</mo><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>x</mi><mi>T</mi></msup><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn><mi>.</mi></mrow><annotation encoding="application/x-tex">2x^T dx = d(x^T x) = d(1) = 0.</annotation></semantics></math> In other words, at the point <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> on the sphere (a radius, if you will), <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math>, the linearization of the constraint of moving along the sphere satisfies <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi><mo>⊥</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">dx \perp x</annotation></semantics></math>. This is our first example where we have seen the infinitesimal perturbation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> being constrained. See Figure <a href="#fig:tangentcircle" data-reference-type="ref" data-reference="fig:tangentcircle">13</a>.</p>
<div id="fig:tangentcircle" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="figures/tangent_to_radius.pdf.svg" class="img-fluid figure-img" style="width:30.0%"></p>
<figcaption>Differentials on a sphere (<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mi>T</mi></msup><mi>x</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">x^T x = 1</annotation></semantics></math>): the differential <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> is constrained to be perpendicular to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>.</figcaption>
</figure>
</div>
<section id="special-case-a-circle" class="level3">
<h3 class="anchored" data-anchor-id="special-case-a-circle">Special Case: A Circle</h3>
<p>Let us simply consider the unit circle in the plane where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mo>cos</mo><mi>θ</mi><mo>,</mo><mo>sin</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">x = (\cos \theta, \sin \theta)</annotation></semantics></math> for some <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>∈</mo><mo stretchy="false" form="prefix">[</mo><mn>0</mn><mo>,</mo><mn>2</mn><mi>π</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\theta \in [0,2\pi)</annotation></semantics></math>. Then, <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mi>T</mi></msup><mi>d</mi><mi>x</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mo>cos</mo><mi>θ</mi><mo>,</mo><mo>sin</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>⋅</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>−</mi><mo>sin</mo><mi>θ</mi><mo>,</mo><mo>cos</mo><mi>θ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>θ</mi><mo>=</mo><mn>0</mn><mi>.</mi></mrow><annotation encoding="application/x-tex">x^T dx = (\cos \theta, \sin \theta) \cdot (-\sin \theta, \cos \theta) d\theta = 0.</annotation></semantics></math> Here, we can think of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> as “extrinsic” coordinates, in that it is a vector in <span class="math inline">$\R^2$</span>. On the other hand, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> is an “intrinsic” coordinate, as every point on the circle is specified by one <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>.</p>
</section>
<section id="on-the-sphere" class="level3">
<h3 class="anchored" data-anchor-id="on-the-sphere">On the Sphere</h3>
<p>You may remember that the rank-1 matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><msup><mi>x</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">x x^T</annotation></semantics></math>, for any unit vector <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mi>T</mi></msup><mi>x</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">x^T x = 1</annotation></semantics></math>, is a <strong>projection matrix</strong> (meaning that it is equal to its square and it is symmetric) which projects vectors onto their components in the direction of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>. Correspondingly, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo>−</mo><mi>x</mi><msup><mi>x</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">I - x x^T</annotation></semantics></math> is also a projection matrix, but onto the directions <em>perpendicular</em> to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>: geometrically, the matrix removes components in the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> direction. In particular, if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mi>T</mi></msup><mi>d</mi><mi>x</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">x^T dx= 0</annotation></semantics></math>, then <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>I</mi><mo>−</mo><mi>x</mi><msup><mi>x</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>x</mi><mo>=</mo><mi>d</mi><mi>x</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">(I - x x^T) dx = dx.</annotation></semantics></math> It follows that if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mi>T</mi></msup><mi>d</mi><mi>x</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">x^T dx = 0</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is a symmetric matrix, we have <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>d</mi><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mi>x</mi><mi>T</mi></msup><mi>A</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mi>d</mi><mi>x</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>I</mi><mo>−</mo><mi>x</mi><msup><mi>x</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>x</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>I</mi><mo>−</mo><mi>x</mi><msup><mi>x</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>A</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mi>d</mi><mi>x</mi><mi>.</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}
    d\left(\frac{1}{2} x^T A x\right) &amp;= (A x)^T dx \\
    &amp;= x^T A (dx) \\
    &amp;= x^T A ( I - x x^T) dx  \\
    &amp;= ((I - x x^T) A x )^T dx.
\end{aligned}</annotation></semantics></math> In other words, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>I</mi><mo>−</mo><mi>x</mi><msup><mi>x</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>A</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">(I - x x^T)A x</annotation></semantics></math> is the gradient of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mi>x</mi><mi>T</mi></msup><mi>A</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\frac{1}{2} x^T A x</annotation></semantics></math> <em>on the sphere.</em></p>
<p>So what did we just do? To obtain the gradient on the sphere, we needed (i) a linearization of the function that is correct on tangents, and (ii) a direction that <em>is</em> tangent (i.e.&nbsp;satisfies the linearized constraint). Using this, we obtain the gradient of a general scalar function on the sphere:</p>
<div class="theorem">
<p>Given <span class="math inline">$f: \mathbb{S}^n \to \R$</span>, we have <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mi>g</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mi>d</mi><mi>x</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>I</mi><mo>−</mo><mi>x</mi><msup><mi>x</mi><mi>T</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mi>d</mi><mi>x</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">df = g(x)^T dx = (( I - x x^T) g(x))^T dx.</annotation></semantics></math></p>
</div>
<p>The proof of this is precisely the same as we did before for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mi>x</mi><mi>T</mi></msup><mi>A</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">f(x) = \frac{1}{2} x^T A x</annotation></semantics></math>.</p>
</section>
</section>
<section id="differentiating-on-orthogonal-matrices" class="level2">
<h2 class="anchored" data-anchor-id="differentiating-on-orthogonal-matrices">Differentiating on Orthogonal Matrices</h2>
<p>Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> be an orthogonal matrix. Then, computationally (as is done in the Julia notebook), one can see that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Q</mi><mi>T</mi></msup><mi>d</mi><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q^T dQ</annotation></semantics></math> is an anti-symmetric matrix (sometimes called skew-symmetric).</p>
<div class="definition">
<p>A matrix <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> is anti-symmetric if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>=</mo><mi>−</mi><msup><mi>M</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">M = - M^T</annotation></semantics></math>. Note that all anti-symmetric matrices thus have zeroes on their diagonals.</p>
</div>
<p>In fact, we can prove that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Q</mi><mi>T</mi></msup><mi>d</mi><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q^T dQ</annotation></semantics></math> is anti-symmetric.</p>
<div class="theorem">
<p>Given <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> is an orthogonal matrix, we have that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Q</mi><mi>T</mi></msup><mi>d</mi><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q^T dQ</annotation></semantics></math> is anti-symmetric.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><em>Proof.</em> The constraint of being orthogonal implies that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Q</mi><mi>T</mi></msup><mi>Q</mi><mo>=</mo><mi>I</mi></mrow><annotation encoding="application/x-tex">Q^T Q = I</annotation></semantics></math>. Differentiating this equation, we obtain <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Q</mi><mi>T</mi></msup><mi>d</mi><mi>Q</mi><mo>+</mo><mi>d</mi><msup><mi>Q</mi><mi>T</mi></msup><mspace width="0.167em"></mspace><mi>Q</mi><mo>=</mo><mn>0</mn><mo>⟹</mo><msup><mi>Q</mi><mi>T</mi></msup><mi>d</mi><mi>Q</mi><mo>=</mo><mi>−</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Q</mi><mi>T</mi></msup><mi>d</mi><mi>Q</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>T</mi></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">Q^T dQ + dQ^T \, Q = 0 \implies Q^T dQ = - (Q^T dQ)^T.</annotation></semantics></math> This is precisely the definition of being anti-symmetric.&nbsp;◻</p>
</div>
<p>Before we move on, we may ask what the dimension of the “surface” of orthogonal matrices is in <span class="math inline">$\R^{n^2}$</span>.</p>
<p>When <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">n = 2</annotation></semantics></math>, all orthogonal matrices are rotations and reflections, and rotations have the form <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mo>cos</mo><mi>θ</mi></mtd><mtd columnalign="center" style="text-align: center"><mo>sin</mo><mi>θ</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>−</mi><mo>sin</mo><mi>θ</mi></mtd><mtd columnalign="center" style="text-align: center"><mo>cos</mo><mi>θ</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">Q = \begin{pmatrix}
    \cos \theta &amp; \sin \theta \\ 
    - \sin \theta &amp; \cos \theta
\end{pmatrix}.</annotation></semantics></math> Hence, when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">n=2</annotation></semantics></math> we have one parameter.</p>
<p>When <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">n = 3</annotation></semantics></math>, airplane pilots know about “roll, pitch, and yaw”, which are the three parameters for the orthogonal matrices when <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>3</mn><mi>.</mi></mrow><annotation encoding="application/x-tex">n =3.</annotation></semantics></math> In general, in <span class="math inline">$\R^{n^2}$</span>, the orthogonal group has dimension <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">n(n-1)/2</annotation></semantics></math>.</p>
<p>There are a few ways to see this.</p>
<ul>
<li><p>Firstly, orthogonality <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Q</mi><mi>T</mi></msup><mi>Q</mi><mo>=</mo><mi>I</mi></mrow><annotation encoding="application/x-tex">Q^T Q = I</annotation></semantics></math> imposes <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">n(n+1)/2</annotation></semantics></math> constraints, leaving <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">n(n-1)/2</annotation></semantics></math> free parameters.</p></li>
<li><p>When we do <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">QR</annotation></semantics></math> decomposition, the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math> “eats” up <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">n(n+1)/2</annotation></semantics></math> of the parameters, again leaving <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">n(n-1)/2</annotation></semantics></math> for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>.</p></li>
<li><p>Lastly, If we think about the symmetric eigenvalue problem where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>=</mo><mi>Q</mi><mi>Λ</mi><msup><mi>Q</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">S = Q \Lambda Q^T</annotation></semantics></math>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math> has <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">n(n+1) /2</annotation></semantics></math> parameters and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math> has <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>, so <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> has <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">n(n-1)/2</annotation></semantics></math>.</p></li>
</ul>
<section id="differentiating-the-symmetric-eigendecomposition" class="level3">
<h3 class="anchored" data-anchor-id="differentiating-the-symmetric-eigendecomposition">Differentiating the Symmetric Eigendecomposition</h3>
<p>Let <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math> be a symmetric matrix, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math> be diagonal containing eigenvalues of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>, and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> be orthogonal with column vectors as eigenvectors of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math> such that <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>=</mo><mi>Q</mi><mi>Λ</mi><msup><mi>Q</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">S = Q \Lambda Q^T</annotation></semantics></math>. [For simplicity, let’s assume that the eigenvalues are “simple” (multiplicity&nbsp;1); repeated eigenvalues turn out to greatly complicate the analysis of perturbations because of the ambiguity in their eigenvector basis.] Then, we have <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>S</mi><mo>=</mo><mi>d</mi><mi>Q</mi><mspace width="0.167em"></mspace><mi>Λ</mi><msup><mi>Q</mi><mi>T</mi></msup><mo>+</mo><mi>Q</mi><mspace width="0.167em"></mspace><mi>d</mi><mi>Λ</mi><mspace width="0.167em"></mspace><msup><mi>Q</mi><mi>T</mi></msup><mo>+</mo><mi>Q</mi><mi>Λ</mi><mi>d</mi><msup><mi>Q</mi><mi>T</mi></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">dS = dQ \, \Lambda Q^T + Q \, d\Lambda \, Q^T + Q \Lambda dQ^T,</annotation></semantics></math> which may be written as <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Q</mi><mi>T</mi></msup><mi>d</mi><mi>S</mi><mspace width="0.167em"></mspace><mi>Q</mi><mo>=</mo><msup><mi>Q</mi><mi>T</mi></msup><mi>d</mi><mi>Q</mi><mi>Λ</mi><mo>−</mo><mi>Λ</mi><msup><mi>Q</mi><mi>T</mi></msup><mi>d</mi><mi>Q</mi><mo>+</mo><mi>d</mi><mi>Λ</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">Q^T dS \, Q = Q^T dQ \Lambda - \Lambda Q^T dQ + d\Lambda.</annotation></semantics></math></p>
<p>As an exercise, one may check that the left and right hand sides of the above are both symmetric. This may be easier if one looks at the diagonal entries on their own, as there <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Q</mi><mi>T</mi></msup><mi>d</mi><mi>S</mi><mspace width="0.167em"></mspace><mi>Q</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>i</mi><mi>i</mi></mrow></msub><mo>=</mo><msubsup><mi>q</mi><mi>i</mi><mi>T</mi></msubsup><mi>d</mi><mi>S</mi><mspace width="0.167em"></mspace><msub><mi>q</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">(Q^T dS \, Q)_{ii} = q_i^T dS \, q_i</annotation></semantics></math>. Since <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>q</mi><mi>i</mi></msub><annotation encoding="application/x-tex">q_i</annotation></semantics></math> is the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>th eigenvector, this implies <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>q</mi><mi>i</mi><mi>T</mi></msubsup><mi>d</mi><mi>S</mi><mspace width="0.167em"></mspace><msub><mi>q</mi><mi>i</mi></msub><mo>=</mo><mi>d</mi><msub><mi>λ</mi><mi>i</mi></msub><mi>.</mi></mrow><annotation encoding="application/x-tex">q_i^T dS\, q_i = d\lambda_i.</annotation></semantics></math> (In physics, this is sometimes called the “Hellman–Feynman” theorem, or non-degenerate first-order eigenvalue-perturbation theory.)</p>
<p>Sometimes we think of a curve of matrices <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">S(t)</annotation></semantics></math> depending on a parameter such as time. If we ask for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>d</mi><msub><mi>λ</mi><mi>i</mi></msub></mrow><mrow><mi>d</mi><mi>t</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{d\lambda_i }{dt}</annotation></semantics></math>, this implies it is thus equal to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>q</mi><mi>i</mi><mi>T</mi></msubsup><mfrac><mrow><mi>d</mi><mi>S</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>d</mi><mi>t</mi></mrow></mfrac><msub><mi>q</mi><mi>i</mi></msub><mi>.</mi></mrow><annotation encoding="application/x-tex">q_i^T \frac{ dS(t)}{dt} q_i.</annotation></semantics></math> So how can we get the gradient <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∇</mi><msub><mi>λ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\nabla \lambda_i</annotation></semantics></math> for one of the eigenvalues? Well, firstly, note that <span class="math display">$$\tr (q_i q_i^T)^T dS) = d\lambda_i \implies \nabla \lambda_i = q_i q_i^T.$$</span></p>
<p>What about the eigenvectors? Those come from off diagonal elements, where for <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mo>≠</mo><mi>j</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">i \neq j,</annotation></semantics></math> <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Q</mi><mi>T</mi></msup><mi>d</mi><mi>S</mi><mspace width="0.167em"></mspace><mi>Q</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msub><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Q</mi><mi>T</mi></msup><mfrac><mrow><mi>d</mi><mi>Q</mi></mrow><mrow><mi>d</mi><mi>t</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mi>i</mi><mi>j</mi></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>λ</mi><mi>j</mi></msub><mo>−</mo><msub><mi>λ</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">(Q^T dS \, Q)_{ij} = \left(Q^T \frac{dQ}{
dt}\right)_{ij} (\lambda_j - \lambda_i).</annotation></semantics></math> Therefore, we can form the elements of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Q</mi><mi>T</mi></msup><mfrac><mrow><mi>d</mi><mi>Q</mi></mrow><mrow><mi>d</mi><mi>t</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">Q^T \frac{dQ}{ dt}</annotation></semantics></math>, and left multiply by <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> to obtain <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mfrac><mrow><mi>d</mi><mi>Q</mi></mrow><mrow><mi>d</mi><mi>t</mi></mrow></mfrac><annotation encoding="application/x-tex">\frac{dQ}{dt}</annotation></semantics></math> (as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> is orthogonal).</p>
<p>It is interesting to get the second derivative of eigenvalues when moving along a line in symmetric matrix space. For simplicity, suppose <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Λ</mi><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math> is diagonal and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>Λ</mi><mo>+</mo><mi>t</mi><mi>E</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">S(t) = \Lambda + t E.</annotation></semantics></math> Therefore, differentiating <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>d</mi><mi>Λ</mi></mrow><mrow><mi>d</mi><mi>t</mi></mrow></mfrac><mo>=</mo><mo>diag</mo><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Q</mi><mi>T</mi></msup><mfrac><mrow><mi>d</mi><mi>S</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>d</mi><mi>t</mi></mrow></mfrac><mi>Q</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\frac{d\Lambda}{dt} = \operatorname{diag}\left(Q^T \frac{dS(t)}{dt} Q\right),</annotation></semantics></math> we get <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><msup><mi>d</mi><mn>2</mn></msup><mi>Λ</mi></mrow><mrow><mi>d</mi><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac><mo>=</mo><mo>diag</mo><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Q</mi><mi>T</mi></msup><mfrac><mrow><msup><mi>d</mi><mn>2</mn></msup><mi>S</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>d</mi><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac><mi>Q</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mn>2</mn><mo>diag</mo><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Q</mi><mi>T</mi></msup><mfrac><mrow><mi>d</mi><mi>S</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>d</mi><mi>t</mi></mrow></mfrac><mfrac><mrow><mi>d</mi><mi>Q</mi></mrow><mrow><mi>d</mi><mi>t</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\frac{d^2 \Lambda}{dt^2} = \operatorname{diag}\left(Q^T \frac{d^2 S(t)}{dt^2} Q\right) + 2 \operatorname{diag}\left(Q^T \frac{dS(t)}{dt} \frac{dQ}{dt}\right).</annotation></semantics></math> Evaluating this at <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>=</mo><mi>I</mi></mrow><annotation encoding="application/x-tex">Q = I</annotation></semantics></math> and recognizing the first term is zero as we are on a line, we have that <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><msup><mi>d</mi><mn>2</mn></msup><mi>Λ</mi></mrow><mrow><mi>d</mi><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac><mo>=</mo><mn>2</mn><mo>diag</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>E</mi><mo>⋅</mo><mfrac><mrow><mi>d</mi><mi>Q</mi></mrow><mrow><mi>d</mi><mi>t</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\frac{d^2 \Lambda}{dt^2} = 2 \operatorname{diag}\left(E \cdot  \frac{dQ}{dt}\right),</annotation></semantics></math> or <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><msup><mi>d</mi><mn>2</mn></msup><mi>Λ</mi></mrow><mrow><mi>d</mi><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac><mo>=</mo><mn>2</mn><munder><mo>∑</mo><mrow><mi>k</mi><mo>≠</mo><mi>i</mi></mrow></munder><msubsup><mi>E</mi><mrow><mi>i</mi><mi>k</mi></mrow><mn>2</mn></msubsup><mi>/</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>λ</mi><mi>i</mi></msub><mo>−</mo><msub><mi>λ</mi><mi>k</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>.</mi></mrow><annotation encoding="application/x-tex">\frac{d^2 \Lambda}{dt^2} = 2 \sum_{k \neq i} E_{ik}^2/(\lambda_i - \lambda_k).</annotation></semantics></math> Using this, we can write out the eigenvalues as a Taylor series: <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>ϵ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>λ</mi><mi>i</mi></msub><mo>+</mo><mi>ϵ</mi><msub><mi>E</mi><mrow><mi>i</mi><mi>i</mi></mrow></msub><mo>+</mo><msup><mi>ϵ</mi><mn>2</mn></msup><munder><mo>∑</mo><mrow><mi>k</mi><mo>≠</mo><mi>i</mi></mrow></munder><msubsup><mi>E</mi><mrow><mi>i</mi><mi>k</mi></mrow><mn>2</mn></msubsup><mi>/</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>λ</mi><mi>i</mi></msub><mo>−</mo><msub><mi>λ</mi><mi>k</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>…</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">\lambda_i (\epsilon) = \lambda_i + \epsilon E_{ii} + \epsilon^2 \sum_{k \neq i} E_{ik}^2/(\lambda_i - \lambda_k) + \dots.</annotation></semantics></math> (In physics, this is known as second-order eigenvalue perturbation theory.)</p>
</section>
</section>
</section>
<section id="where-we-go-from-here" class="level1">
<h1>Where We Go From Here</h1>
<p>There are many topics that we did not have time to cover, even in 16 hours of lectures. If you came into this class thinking that taking derivatives is easy and you already learned everything there is to know about it in first-year calculus, hopefully we’ve convinced you that it is an enormously rich subject that is impossible to exhaust in a single course. Some of the things it might have been nice to include are:</p>
<ul>
<li><p>When automatic differentiation (AD) hits something it cannot handle, you may have to write a custom Jacobian–vector product (a “Jvp,” “frule,” or “pushforward”) in forward-mode, and/or a custon row vector–Jacobian product (a “vJp,” “rrule,” “pullback,” or “Jacobian<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi></mi><mi>T</mi></msup><annotation encoding="application/x-tex">^T</annotation></semantics></math>-vector product”) in reverse-mode. In Julia with Zygote AD, this is done using <a href="https://github.com/JuliaDiff/ChainRulesCore.jl">the ChainRules packages</a>. In Python with JAX, this is done with <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.custom_jvp.html">jax.custon_jvp</a> and/or <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.custom_vjp.html">jax.custon_vjp</a> respectively. In principle, this is straightforward, but the APIs can take some getting used to because the of the generality that they support.</p></li>
<li><p>For functions <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(z)</annotation></semantics></math> with complex arguments <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math> (i.e.&nbsp;complex vector spaces), you cannot take “ordinary” complex derivatives whenever the function involves the conjugate <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>z</mi><mo accent="true">¯</mo></mover><annotation encoding="application/x-tex">\overline{z}</annotation></semantics></math>, for example, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">|</mo><mi>z</mi><mo stretchy="true" form="postfix">|</mo></mrow><mo>,</mo><mi>ℜ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">|z|, \Re(z),</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ℑ</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Im(z)</annotation></semantics></math>. This <em>must</em> occur if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(z)</annotation></semantics></math> is purely real-valued and not constant, as in optimization problems involving complex-number calculations. One option is to write <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>=</mo><mi>x</mi><mo>+</mo><mi>i</mi><mi>y</mi></mrow><annotation encoding="application/x-tex">z = x+iy</annotation></semantics></math> and treat <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(z)</annotation></semantics></math> as a two-argument function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x,y)</annotation></semantics></math> with real derivatives, but this can be awkward if your problem is “naturally” expressed in terms of complex variables (for instance, the <a href="https://en.wikipedia.org/wiki/Frequency_domain">Fourier frequency domain</a>). A common alternative is the “CR calculus” (or “Wirtinger calculus”), in which you write <math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>f</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><mi>z</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mi>z</mi><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow><mrow><mi>∂</mi><mover><mi>z</mi><mo accent="true">¯</mo></mover></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mi>d</mi><mover><mi>z</mi><mo accent="true">¯</mo></mover><mo>,</mo></mrow><annotation encoding="application/x-tex">df = \left(\frac{\partial f}{\partial z}\right) dz + \left(\frac{\partial f}{\partial \overline{z}}\right)d\overline{z},</annotation></semantics></math> as if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>z</mi><mo accent="true">¯</mo></mover><annotation encoding="application/x-tex">\overline{z}</annotation></semantics></math> were independent variables. This can be extended to gradients, Jacobians, steepest-descent, and Newton iterations, for example. A nice review of this concept can be found in these <a href="https://arxiv.org/abs/0906.4835">UCSD course notes</a> by K. Kreuz Delgado.</p></li>
<li><p>Many, many more derivative results for matrix functions and factorizations can be found in the literature, some of them quite tricky to derive. For example, a number of references are listed in this <a href="https://github.com/JuliaDiff/ChainRules.jl/issues/117">GitHub issue for the ChainRules package</a>.</p></li>
<li><p>Another important generalization of differential calculus is to derivatives on curved manifolds and differential geometry, leading to the <a href="https://en.wikipedia.org/wiki/Exterior_derivative">exterior derivative</a>.</p></li>
<li><p>When differentiating eigenvalues <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>λ</mi><annotation encoding="application/x-tex">\lambda</annotation></semantics></math> of matrices <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">A(x)</annotation></semantics></math>, a complication arises at eigenvalue crossings (where multiplicity <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k&gt;1</annotation></semantics></math>). Here, the eigenvalues and eigenvectors usually cease to be differentiable. More generally, this problem arises for any <a href="https://en.wikipedia.org/wiki/Implicit_function">implicit function</a> with a repeated root. In this case, one option is use an expanded definition of sensitivity analysis called a <strong>generalized gradient</strong> (a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>×</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">k\times k</annotation></semantics></math> matrix-valued linear operator <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">[</mo><mi>d</mi><mi>x</mi><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">G(x) [dx]</annotation></semantics></math> whose <em>eigenvalues</em> are the perturbations <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>λ</mi></mrow><annotation encoding="application/x-tex">d\lambda</annotation></semantics></math>. See for example <a href="https://doi.org/10.1006/jfan.1995.1117">Cox (1995)</a>, <a href="https://doi.org/10.1007/BF01742705">Seyranian <em>et al.</em> (1994)</a>, and <a href="https://doi.org/10.1016/j.laa.2022.04.019">Stechlinski (2022)</a>. Physicistss call a related idea “degenerate perturbation theory.” A recent formulation of similar ideas is called the <strong>lexicographic directional derivative</strong>. See for example <a href="https://doi.org/10.1007/s10107-005-0633-0">Nesterov (2005)</a> and <a href="https://doi.org/10.1080/10556788.2017.1374385">Barton <em>et al.</em> (2017)</a>.</p>
<p>Sometimes, optimization problems involving eigenvalues can be reformulated to avoid this difficulty by using <a href="https://en.wikipedia.org/wiki/Semidefinite_programming">SDP constraints</a>. See for example <a href="http://doi.org/10.1364/OE.22.022632">Men <em>et al.</em> (2014)</a>.</p>
<p>For a <a href="https://en.wikipedia.org/wiki/Defective_matrix">defective matrix</a> the situation is worse: even the generalized derivatives blow up because <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>λ</mi></mrow><annotation encoding="application/x-tex">d\lambda</annotation></semantics></math> can be proportional to (e.g.) the square root of the perturbation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">‖</mo><mi>d</mi><mi>A</mi><mo stretchy="true" form="postfix">‖</mo></mrow><annotation encoding="application/x-tex">\lVert dA\rVert</annotation></semantics></math> (for an eigenvalue with algebraic multiplicity <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">=2</annotation></semantics></math> and geometric multiplicity <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">=1</annotation></semantics></math>).</p></li>
<li><p>Famous generalizations of differentation are the “<a href="https://en.wikipedia.org/wiki/Distributional_derivative">distributional</a>” and “<a href="https://en.wikipedia.org/wiki/Weak_derivative">weak</a>” derivatives. For example, to obtain <a href="https://en.wikipedia.org/wiki/Dirac_delta_function">Dirac delta “functions”</a> by differentiating discontinuities. This requires changing not only the definition of “derivative,” but also changing the definition of <em>function</em>, as reviewed at an elementary level in these <a href="https://math.mit.edu/~stevenj/18.303/delta-notes.pdf">MIT course notes</a>.</p></li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Briefly, a function <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\delta x)</annotation></semantics></math> is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>o</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">o(\delta x)</annotation></semantics></math> if <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>lim</mo><mrow><mi>δ</mi><mi>x</mi><mo>→</mo><mn>0</mn></mrow></msub><mfrac><mrow><mo stretchy="false" form="postfix">‖</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>δ</mi><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="false" form="postfix">‖</mo></mrow><mrow><mo stretchy="false" form="postfix">‖</mo><mi>δ</mi><mi>x</mi><mo stretchy="false" form="postfix">‖</mo></mrow></mfrac><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lim_{\delta x \to 0} \frac{\Vert g(\delta x) \Vert}{\Vert \delta x \Vert} = 0</annotation></semantics></math>. We will return to this subject in Section&nbsp;<a href="#sec:banach" data-reference-type="ref" data-reference="sec:banach">5.2</a>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Informally, one can think of the vector space of infinitesimals <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">dx</annotation></semantics></math> as living in the same space as <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math> (understood as a small change in a vector, but still a vector nonetheless). Formally, one can define a distinct “vector space of infinitesimals” in various ways, e.g.&nbsp;as a cotangent space in differential geometry, though we won’t go into more detail here.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>In some Julia AD software, this is done with by defining a <a href="https://github.com/JuliaDiff/ChainRulesCore.jl">“ChainRule”</a>, and in Python autograd/JAX it is done by defining a custom “vJp” (row-vector—Jacobian product) and/or “Jvp” (Jacobian–vector product).<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>The concept of a “row vector” can be formalized as something called a “covector,” a “dual vector,” or an element of a “<a href="https://en.wikipedia.org/wiki/Dual_space">dual space</a>,” not to be confused with the <em>dual numbers</em> used in automatic differentiation (Sec.&nbsp;<a href="#sec:AD" data-reference-type="ref" data-reference="sec:AD">8</a>).<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Some authors distinguish the “dot product” from an “inner product” for complex vector spaces, saying that a dot product has no complex conjugation <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>⋅</mo><mi>y</mi><mo>=</mo><mi>y</mi><mo>⋅</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">x \cdot y = y \cdot x</annotation></semantics></math> (in which case <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>⋅</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">x \cdot x</annotation></semantics></math> need not be real and does not equal <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="postfix">‖</mo><mi>x</mi><msup><mo stretchy="false" form="postfix">‖</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\Vert x \Vert^2</annotation></semantics></math>), whereas the inner product must be conjugate-symmetric, via <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">⟩</mo><mo>=</mo><mover><mi>x</mi><mo accent="true">‾</mo></mover><mo>⋅</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">\langle x, y \rangle = \bar{x} \cdot y</annotation></semantics></math>. Another source of confusion for complex vector spaces is that some fields of mathematics define <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">⟨</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false" form="postfix">⟩</mo><mo>=</mo><mi>x</mi><mo>⋅</mo><mover><mi>y</mi><mo accent="true">‾</mo></mover></mrow><annotation encoding="application/x-tex">\langle x, y \rangle = x \cdot \bar{y}</annotation></semantics></math>, i.e.&nbsp;they conjugate the <em>right</em> argument instead of the left (so that it is linear in the left argument and conjugate-linear in the right argument). Aren’t you glad we’re sticking with real numbers?<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Completeness means that any Cauchy sequence of points in the vector space—any sequence of points that gets closer and closer together—has a limit lying within the vector space. This criterion usually holds in practice for vector spaces over real or complex scalars, but can get trickier when talking about vector spaces of functions, since e.g.&nbsp;the limit of a sequence of continuous functions can be a discontinuous function.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Proving the triangle inequality for an arbitrary inner product is not so obvious; one uses a result called the Cauchy–Schwarz inequality.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>In fact, extraordinarily difficult: “NP-complete” <a href="https://dl.acm.org/doi/abs/10.5555/3114201.3114717">(Naumann, 2006)</a>.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p><a href="https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html">The Autodiff Cookbook</a>, part of the JAX documentation, discusses this algorithm in a section on Hessian–vector products. It notes that one could also interchange the <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>∂</mi><mi>/</mi><mi>∂</mi><mi>α</mi></mrow><annotation encoding="application/x-tex">\partial/\partial \alpha</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>∇</mi><mi>x</mi></msub><annotation encoding="application/x-tex">\nabla_x</annotation></semantics></math> derivatives and employ reverse-over-forward mode, but suggests that this is less efficient in practice: “because forward-mode has less overhead than reverse-mode, and since the outer differentiation operator here has to differentiate a larger computation than the inner one, keeping forward-mode on the outside works best.” It also presents another alternative: using the identity <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>′</mi><mi>v</mi><mo>=</mo><mi>∇</mi><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>v</mi><mi>T</mi></msup><mi>∇</mi><mi>f</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">(\nabla f)'v = \nabla (v^T \nabla f)</annotation></semantics></math>, one can apply reverse-over-reverse mode to take the gradient of <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>v</mi><mi>T</mi></msup><mi>∇</mi><mi>f</mi></mrow><annotation encoding="application/x-tex">v^T \nabla f</annotation></semantics></math>, but this has even more computational overhead.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Of course, the independent variable need not be time, it just needs to be a real scalar. But in a generic context it is convenient to imagine ODE solutions as evolving in time.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>For example, the second-order ODE <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><msup><mi>d</mi><mn>2</mn></msup><mi>v</mi></mrow><mrow><mi>d</mi><msup><mi>t</mi><mn>2</mn></msup></mrow></mfrac><mo>+</mo><mfrac><mrow><mi>d</mi><mi>v</mi></mrow><mrow><mi>d</mi><mi>t</mi></mrow></mfrac><mo>=</mo><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>v</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\frac{d^{2}v}{dt^{2}}+\frac{dv}{dt}=h(v,t)</annotation></semantics></math> could be re-written in first-order form by defining <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>u</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>u</mi><mn>2</mn></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>v</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>d</mi><mi>v</mi><mi>/</mi><mi>d</mi><mi>t</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u=\left(\begin{array}{c}
u_{1}\\
u_{2}
\end{array}\right)=\left(\begin{array}{c}
v\\
dv/dt
\end{array}\right)</annotation></semantics></math>, in which case <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mi>u</mi><mi>/</mi><mi>d</mi><mi>t</mi><mo>=</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>u</mi><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">du/dt=f(u,t)</annotation></semantics></math> where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>u</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>h</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>u</mi><mn>1</mn></msub><mo>,</mo><mi>t</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><msub><mi>u</mi><mn>2</mn></msub></mtd></mtr></mtable><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f=\left(\begin{array}{c}
u_{2}\\
h(u_{1},t)-u_{2}
\end{array}\right)</annotation></semantics></math>.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>For a modern and full-featured example, see the DifferentialEquations.jl suite of ODE solvers in the Julia language.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>This “left-to-right” picture can be made very explicit if we imagine discretizing the ODE into a recurrence, e.g.&nbsp;via Euler’s method for an arbitrarily small <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Δ</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">\Delta t</annotation></semantics></math>, as described in the MIT course notes <em><a href="https://math.mit.edu/~stevenj/18.336/recurrence2.pdf">Adjoint methods and sensitivity analysis for recurrence relations</a></em> by S.&nbsp;G.&nbsp;Johnson (2011).<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>Being fully mathematically rigorous with vector spaces of functions requires a lot of tedious care in specifying a well-behaved set of functions, inserting annoying caveats about functions that differ only at isolated points, and so forth. In this lecture, we will mostly ignore such technicalities—we will implicitly assume that our functions are integrable, differentiable, etcetera, as needed. The subject of <em>functional analysis</em> exists to treat such matters with more care.<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>Technically, it only needs to be small “almost everywhere” since jumps that occur only at isolated points don’t affect the integral.<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p>For more discussion of these concepts, see (e.g.) the review article “Monte Carlo gradient estimation in machine learning” (2020) by Mohamed <em>et al.</em> (<a href="https://arxiv.org/abs/1906.10652" class="uri">https://arxiv.org/abs/1906.10652</a>).<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p>Most computer hardware cannot generate numbers that are actually random, only numbers that <em>seem</em> random, called “pseudo-random” numbers. The design of these random-seeming numeric sequences is a subtle subject, steeped in number theory, with a long history of mistakes. A famous ironic quotation in this field is (Robert Coveyou, 1970): “Random number generation is too important to be left to chance.”<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p>Much of machine learning uses only variations on gradient descent, without incorporating Hessian information except implicitly via “momentum” terms. Partly this can be explained by the fact that optimization problems in ML are typically solved only to low accuracy, often have nonsmooth/stochastic aspects, rarely involve nonlinear constraints, and are often very high-dimensional. This is only a small corner of the wider universe of computational optimization!<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p>The term “programming” in optimization theory does not refer to software engineering, but is rather an anachronistic term for optimization problems. For example, “linear programming” (LP) refers to optimizing affine objectives and affine constraints, while “quadratic programming” (QP) refers to optimizing convex quadratic objectives with affine constraints. {% endraw %}<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/ivanstepanovftw\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>