There are many topics that we did not have time to cover, even in 16 hours of lectures. If you came into this class thinking that taking derivatives is easy and you already learned everything there is to know about it in first-year calculus, hopefully we've convinced you that it is an enormously rich subject that is impossible to exhaust in a single course. Some of the things it might have been nice to include are:
\begin{itemize}
    \item When automatic differentiation (AD) hits something it cannot handle, you may have to write a custom Jacobian--vector product (a ``Jvp,'' ``frule,'' or ``pushforward'') in forward-mode, and/or a custon row vector--Jacobian product (a ``vJp,'' ``rrule,'' ``pullback,'' or ``Jacobian$^T$-vector product'') in reverse-mode. In Julia with Zygote AD, this is done using \href{https://github.com/JuliaDiff/ChainRulesCore.jl}{the ChainRules packages}. In Python with JAX, this is done with \href{https://jax.readthedocs.io/en/latest/_autosummary/jax.custom_jvp.html}{jax.custon\_jvp} and/or \href{https://jax.readthedocs.io/en/latest/_autosummary/jax.custom_vjp.html}{jax.custon\_vjp} respectively. In principle, this is straightforward, but the APIs can take some getting used to because the of the generality that they support.
    \item For functions $f(z)$ with complex arguments $z$ (i.e. complex vector spaces), you cannot take ``ordinary'' complex derivatives whenever the function involves the conjugate $\overline{z}$, for example, $|z|, \Re(z),$ and $\Im(z)$. This \textit{must} occur if $f(z)$ is purely real-valued and not constant, as in optimization problems involving complex-number calculations. One option is to write $z = x+iy$ and treat $f(z)$ as a two-argument function $f(x,y)$ with real derivatives, but this can be awkward if your problem is ``naturally'' expressed in terms of complex variables (for instance, the \href{https://en.wikipedia.org/wiki/Frequency_domain}{Fourier frequency domain}). A common alternative is the ``CR calculus'' (or ``Wirtinger calculus''), in which you write 
    \[
    \d f = \left(\frac{\partial f}{\partial z}\right) \d z + \left(\frac{\partial f}{\partial \overline{z}}\right)\d \overline{z},
    \]
    as if $z$ and $\overline{z}$ were independent variables. This can be extended to gradients, Jacobians, steepest-descent, and Newton iterations, for example. A nice review of this concept can be found in these \href{https://arxiv.org/abs/0906.4835}{UCSD course notes} by K. Kreuz Delgado.
    \item Many, many more derivative results for matrix functions and factorizations can be found in the literature, some of them quite tricky to derive. For example, a number of references are listed in this \href{https://github.com/JuliaDiff/ChainRules.jl/issues/117}{GitHub issue for the ChainRules package}.
    \item Another important generalization of differential calculus is to derivatives on curved manifolds and differential geometry, leading to the \href{https://en.wikipedia.org/wiki/Exterior_derivative}{exterior derivative}.
    \item When differentiating eigenvalues $\lambda$ of matrices $A(x)$, a complication arises at eigenvalue crossings (where multiplicity $k>1$). Here, the eigenvalues and eigenvectors usually cease to be differentiable. More generally, this problem arises for any \href{https://en.wikipedia.org/wiki/Implicit_function}{implicit function} with a repeated root. In this case, one option is use an expanded definition of sensitivity analysis called a \textbf{generalized gradient} (a $k\times k$ matrix-valued linear operator $G(x) [\d x]$ whose \textit{eigenvalues} are the perturbations $\d \lambda$. See for example \href{https://doi.org/10.1006/jfan.1995.1117}{Cox (1995)}, \href{https://doi.org/10.1007/BF01742705}{Seyranian \textit{et al.} (1994)}, and \href{https://doi.org/10.1016/j.laa.2022.04.019}{Stechlinski (2022)}. Physicistss call a related idea ``degenerate perturbation theory.'' A recent formulation of similar ideas is called the \textbf{lexicographic directional derivative}. See for example \href{https://doi.org/10.1007/s10107-005-0633-0}{Nesterov (2005)} and \href{https://doi.org/10.1080/10556788.2017.1374385}{Barton \textit{et al.} (2017)}.

    Sometimes, optimization problems involving eigenvalues can be reformulated to avoid this difficulty by using \href{https://en.wikipedia.org/wiki/Semidefinite_programming}{SDP constraints}. See for example \href{http://doi.org/10.1364/OE.22.022632}{Men \textit{et al.} (2014)}.

    For a \href{https://en.wikipedia.org/wiki/Defective_matrix}{defective matrix} the situation is worse: even the generalized derivatives blow up because $\d \lambda$ can be proportional to (e.g.) the square root of the perturbation $\lVert \d A\rVert$ (for an eigenvalue with algebraic multiplicity $=2$ and geometric multiplicity $=1$).

    \item Famous generalizations of differentation are the ``\href{https://en.wikipedia.org/wiki/Distributional_derivative}{distributional}'' and ``\href{https://en.wikipedia.org/wiki/Weak_derivative}{weak}'' derivatives. For example, to obtain \href{https://en.wikipedia.org/wiki/Dirac_delta_function}{Dirac delta ``functions''} by differentiating discontinuities. This requires changing not only the definition of ``derivative,'' but also changing the definition of \textit{function}, as reviewed at an elementary level in these \href{https://math.mit.edu/~stevenj/18.303/delta-notes.pdf}{MIT course notes}.
\end{itemize}