[
  {
    "objectID": "matrix-calculus/en-US/index.html",
    "href": "matrix-calculus/en-US/index.html",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "",
    "text": "{% raw %} # Introduction {#introduction .unnumbered}\nThese notes are based on the class as it was run for the second time in January 2023, taught by Professors Alan Edelman and Steven G. Johnson at MIT. The previous version of this course, run in January 2022, can be found on OCW here.\nBoth Professors Edelman and Johnson use he/him pronouns and are in the Department of Mathematics at MIT; Prof. Edelman is also a Professor in the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) running the Julia lab, while Prof. Johnson is also a Professor in the Department of Physics.\nHere is a description of the course.:\nThe class involved numerous example numerical computations using the Julia language, which you can install on your own computer following these instructions. The material for this class is also located on GitHub at https://github.com/mitmath/matrixcalc."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#applications",
    "href": "matrix-calculus/en-US/index.html#applications",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Applications",
    "text": "Applications\n\nApplications: Machine learning\nMachine learning has numerous buzzwords associated with it, including but not limited to: parameter optimization, stochastic gradient descent, automatic differentiation, and backpropagation. In this whole collage you can see a fraction of how matrix calculus applies to machine learning. It is recommended that you look into some of these topics yourself if you are interested.\n\n\nApplications: Physical modeling\nLarge physical simulations, such as engineering-design problems, are increasingly characterized by huge numbers of parameters, and the derivatives of simulation outputs with respect to these parameters is crucial in order to evaluate sensitivity to uncertainties as well as to apply large-scale optimization.\nFor example, the shape of an airplane wing might be characterized by thousands of parameters, and if you can compute the derivative of the drag force (from a large fluid-flow simulation) with respect to these parameters then you could optimize the wing shape to minimize the drag for a given lift or other constraints.\nAn extreme version of such parameterization is known as “topology optimization,” in which the material at “every point” in space is potentially a degree of freedom, and optimizing over these parameters can discover not only a optimal shape but an optimal topology (how materials are connected in space, e.g. how many holes are present). For example, topology optimization has been applied in mechanical engineering to design the cross sections of airplane wings, artificial hips, and more into a complicated lattice of metal struts (e.g. minimizing weight for a given strength).\nBesides engineering design, complicated differentiation problems arise in fitting unknown parameters of a model to experimental data, and also in evaluating uncertainties in the outputs of models with imprecise parameters/inputs. This is closely related to regression problems in statistics, as discussed below, except that here the model might be a giant set of differential equations with some unknown parameters.\n\n\nApplications: Data science and multivariable statistics\nIn multivariate statistics, models are often framed in terms of matrix inputs and outputs (or even more complicated objects such as tensors). For example, a “simple” linear multivariate matrix model might be Y(X)=XB+UY(X) = XB + U, where BB is an unknown matrix of coefficients (to be determined by some form of fit/regression) and UU is unknown matrix of random noise (that prevents the model from exactly fitting the data). Regression then involves minimizing some function of the error U(B)=Y−XBU(B) = Y - XB between the model XBXB and data YY; for example, a matrix norm $\\Vert U\\Vert_F^2 = \\tr U^T U$, a determinant detUTU\\det U^T U, or more complicated functions. Estimating the best-fit coefficients BB, analyzing uncertainties, and many other statistical analyses require differentiating such functions with respect to BB or other parameters. A recent review article on this topic is Liu et al. (2022): “Matrix differential calculus with applications in the multivariate linear model and its diagnostics” (https://doi.org/10.1016/j.sctalk.2023.100274).\n\n\nApplications: Automatic differentiation\nTypical differential calculus classes are based on symbolic calculus, with students essentially learning to do what Mathematica or Wolfram Alpha can do. Even if you are using a computer to take derivatives symbolically, to use this effectively you need to understand what is going on beneath the hood. But while, similarly, some numerics may show up for a small portion of this class (such as to approximate a derivative using the difference quotient), today’s automatic differentiation is neither of those two things. It is more in the field of the computer science topic of compiler technology than mathematics. However, the underlying mathematics of automatic differentiation is interesting, and we will learn about this in this class!\nEven approximate computer differentiation is more complicated than you might expect. For single-variable functions f(x)f(x), derivatives are defined as the limit of a difference [f(x+δx)−f(x)]/δx[f(x+\\delta x)-f(x)]/\\delta x as δx→0\\delta x\\to 0. A crude “finite-difference” approximation is simply to approximate f′(x)f'(x) by this formula for a small δx\\delta x, but this turns out to raise many interesting issues involving balancing truncation and roundoff errors, higher-order approximations, and numerical extrapolation."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#first-derivatives",
    "href": "matrix-calculus/en-US/index.html#first-derivatives",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "First Derivatives",
    "text": "First Derivatives\nThe derivative of a function of one variable is itself a function of one variable– it simply is (roughly) defined as the linearization of a function. I.e., it is of the form (f(x)−f(x0))≈f′(x0)(x−x0)(f(x)-f(x_0)) \\approx f'(x_0) (x-x_0). In this sense, “everything is easy” with scalar functions of scalars (by which we mean, functions that take in one number and spit out one number).\nThere are occasionally other notations used for this linearization:\n\nδy≈f′(x)δx\\delta y \\approx f'(x) \\delta x,\ndy=f′(x)dxdy = f'(x) dx,\n(y−y0)≈f′(x0)(x−x0)(y-y_0) \\approx f'(x_0)(x-x_0),\nand df=f′(x)dxdf = f'(x)  dx.\n\nThis last one will be the preferred of the above for this class. One can think of dxdx and dydy as “really small numbers.” In mathematics, they are called infinitesimals, defined rigorously via taking limits. Note that here we do not want to divide by dxdx. While this is completely fine to do with scalars, once we get to vectors and matrices you can’t always divide!\nThe numerics of such derivatives are simple enough to play around with. For instance, consider the function f(x)=x2f(x) = x^2 and the point (x0,f(x0))=(3,9)(x_0,f(x_0)) = (3,9). Then, we have the following numerical values near (3,9)(3,9): f(3.𝟎𝟎𝟎𝟏)=9.𝟎𝟎𝟎𝟔0001f(3.𝟎𝟎𝟎𝟎𝟏)=9.𝟎𝟎𝟎𝟎𝟔00001f(3.𝟎𝟎𝟎𝟎𝟎𝟏)=9.𝟎𝟎𝟎𝟎𝟎𝟔000001f(3.𝟎𝟎𝟎𝟎𝟎𝟎𝟏)=9.𝟎𝟎𝟎𝟎𝟎𝟎𝟔0000001.\\begin{aligned}\n    f(3.\\mathbf{0001}) &= 9.\\mathbf{0006}0001 \\\\\n    f(3.\\mathbf{00001}) &= 9.\\mathbf{00006}00001 \\\\\n    f(3.\\mathbf{000001}) &= 9.\\mathbf{000006}000001 \\\\\n    f(3.\\mathbf{0000001}) &= 9.\\mathbf{0000006}0000001.\n\\end{aligned} Here, the bolded digits on the left are Δx\\Delta x and the bolded digits on the right are Δy\\Delta y. Notice that Δy=6Δx\\Delta y = 6\\Delta x. Hence, we have that f(3+Δx)=9+Δy=9+6Δx⟹f(3+Δx)−f(3)=6Δx≈f′(3)Δx.f(3+\\Delta x) = 9 + \\Delta y = 9+6\\Delta x \\implies f(3+\\Delta x) - f(3) = 6\\Delta x \\approx f'(3)\\Delta x. Therefore, we have that the linearization of x2x^2 at x=3x=3 is the function f(x)−f(3)≈6(x−3)f(x)-f(3) \\approx 6(x-3).\n\nWe now leave the world of scalar calculus and enter the world of vector/matrix calculus! Professor Edelman invites us to think about matrices holistically—not just as a table of numbers.\nThe notion of linearizing your function will conceptually carry over as we define the derivative of functions which take in/spit out more than one number. Of course, this means that the derivative will have a different “shape” than a single number. Here is a table on the shape of the first derivative. The inputs of the function are given on the left hand side of the table, and the outputs of the function are given across the top.\n\n\n\n\n\n\n\n\n\n2-4\ninput ↓\\downarrow and output →\\rightarrow\nscalar\nvector\nmatrix\n\n\n\n\nscalar\nscalar\nvector (for instance, velocity)\nmatrix\n\n\nvector\ngradient = (column) vector\nmatrix (called the Jacobian matrix)\nhigher order array\n\n\nmatrix\nmatrix\nhigher order array\nhigher order array\n\n\n\nYou will ultimately learn how to do any of these in great detail eventually in this class! The purpose of this table is to plant the notion of differentials as linearization. Let’s look at an example.\n\nLet f(x)=xTxf(x) = x^T x, where xx is a 2×12\\times 1 matrix and the output is thus a 1×11\\times 1 matrix. Confirm that 2x0Tdx2x_0^T dx is indeed the differential of ff at x0=(34)Tx_0 = \\begin{pmatrix}  3 & 4\\end{pmatrix}^T.\n\nFirstly, let’s compute f(x0)f(x_0): f(x0)=x0Tx0=32+42=25.f(x_0) = x_0^T x_0 = 3^2 + 4^2 = 25. Then, suppose dx=[.001,.002]dx = [.001,.002]. Then, we would have that f(x+dx)=(3.001)2+(4.002)2=25.𝟎𝟐𝟐005.f(x+dx) = (3.001)^2 + (4.002)^2 = 25.\\mathbf{022}005. Then, notice that 2x0Tdx=2(34)Tdx=.0222x_0^T \\,dx = 2\\begin{pmatrix}\n    3  & 4\n\\end{pmatrix}^T dx = .022. Hence, we have that f(x0+dx)−f(x0)≈2x0Tdx=.022.f(x_0 + dx) - f(x_0) \\approx 2x_0^{T} dx = .022. As we will see right now, the 2x0Tdx2x_0^Tdx didn’t come from nowhere!"
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#intro-matrix-and-vector-product-rule",
    "href": "matrix-calculus/en-US/index.html#intro-matrix-and-vector-product-rule",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Intro: Matrix and Vector Product Rule",
    "text": "Intro: Matrix and Vector Product Rule\nFor matrices, we in fact still have a product rule! We will discuss this in much more detail in later chapters, but let’s begin here with a small taste.\n\nLet A,BA,B be two matrices. Then, we have the differential product rule for ABAB: d(AB)=(dA)B+A(dB).d(AB) = (dA)B + A(dB). By the differential of the matrix AA, we think of it as a small (unconstrained) change in the matrix A.A. Later, constraints may be places on the allowed perturbations.\n\nNotice however, that (by our table) the derivative of a matrix is a matrix! So generally speaking, the products will not commute.\nIf xx is a vector, then by the differential product rule we have d(xTx)=(dxT)x+xT(dx).d(x^Tx) = (dx^T)x + x^T (dx). However, notice that this is a dot product, and dot products commute (since ∑ai⋅bi=∑bi⋅ai\\sum a_i \\cdot b_i = \\sum b_i \\cdot a_i), we have that d(xTx)=(2x)Tdx.d(x^T x) = (2x)^T dx.\n\nRemark. The way the product rule works for vectors as matrices is that transposes “go for the ride.” See the next example below.\n\n\nBy the product rule. we have\n\nd(uTv)=(du)Tv+uT(dv)=vTdu+uTdvd(u^Tv) = (du)^T v + u^T (dv) = v^T du + u^T dv since dot products commute.\nd(uvT)=(du)vT+u(dv)T.d(uv^T) = (du )v^T + u( dv)^T.\n\n\n\nRemark. The way to prove these sorts of statements can be seen in Section 2."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#revisiting-single-variable-calculus",
    "href": "matrix-calculus/en-US/index.html#revisiting-single-variable-calculus",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Revisiting single-variable calculus",
    "text": "Revisiting single-variable calculus\n\n\n\nThe essence of a derivative is linearization: predicting a small change δf\\delta f in the output f(x)f(x) from a small change δx\\delta x in the input xx, to first order in δx\\delta x.\n\n\nIn a first-semester single-variable calculus course (like 18.01 at MIT), the derivative f′(x)f'(x) is introduced as the slope of the tangent line at the point (x,f(x))(x,f(x)), which can also be viewed as a linear approximation of ff near xx. In particular, as depicted in Fig. 1, this is equivalent to a prediction of the change δf\\delta f in the “output” of f(x)f(x) from a small change δx\\delta x in the “input” to first order (linear) in δx\\delta x: δf=f(x+δx)−f(x)=f′(x)δx+(higher-order terms)⏟o(δx).\\delta f = f(x+\\delta x) - f(x) =  f'(x) \\, \\delta x  + \\underbrace{(\\text{higher-order terms})}_{o(\\delta x)}. We can more precisely express these higher-order terms using asymptotic “little-o” notation “o(δx)o(\\delta x)”, which denotes any function whose magnitude shrinks much faster than |δx||\\delta x| as δx→0\\delta x \\to 0, so that for sufficiently small δx\\delta x it is negligible compared to the linear f′(x)δxf'(x) \\, \\delta x term. (Variants of this notation are commonly used in computer science, and there is a formal definition that we omit here.1) Examples of such higher-order terms include (δx)2(\\delta x)^2, (δx)3(\\delta x)^3, (δx)1.001(\\delta x)^{1.001}, and (δx)/log(δx)(\\delta x)/\\log(\\delta x).\n\nRemark. Here, δx\\delta x is not an infinitesimal but rather a small number. Note that our symbol “δ\\delta” (a Greek lowercase “delta”) is not the same as the symbol “∂\\partial” commonly used to denote partial derivatives.\n\nThis notion of a derivative may remind you of the first two terms in a Taylor series f(x+δx)=f(x)+f′(x)δx+⋯f(x+\\delta x) = f(x) + f'(x) \\, \\delta x + \\cdots (though in fact it is much more basic than Taylor series!), and the notation will generalize nicely to higher dimensions and other vector spaces. In differential notation, we can express the same idea as: df=f(x+dx)−f(x)=f′(x)dx.df = f(x+dx) - f(x) = f'(x) \\, dx. In this notation we implicitly drop the o(δx)o(\\delta x) term that vanishes in the limit as δx\\delta x becomes infinitesimally small.\nWe will use this as the more generalized definition of a derivative. In this formulation, we avoid dividing by dxdx, because soon we will allow xx (and hence dxdx) to be something other than a number—if dxdx is a vector, we won’t be able to divide by it!"
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#linear-operators",
    "href": "matrix-calculus/en-US/index.html#linear-operators",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Linear operators",
    "text": "Linear operators\nFrom the perspective of linear algebra, given a function ff, we consider the derivative of ff, to be the linear operator f′(x)f'(x) such that df=f(x+dx)−f(x)=f′(x)[dx].df = f(x+ dx) - f(x) = f'(x)[dx].\nAs above, you should think of the differential notation dxdx as representing an arbitrary small change in xx, where we are implicitly dropping any o(dx)o(dx) terms, i.e. terms that decay faster than linearly as dx→0dx\\to 0. Often, we will omit the square brackets and write simply f′(x)dxf'(x)dx instead of f′(x)[dx]f'(x)[dx], but this should be understood as the linear operator f′(x)f'(x) acting on dxdx—don’t write dxf′(x)dx\\,f'(x), which will generally be nonsense!\nThis definition will allow us to extend differentiation to arbitrary vector spaces of inputs xx and outputs f(x)f(x). (More technically, we will require vector spaces with a norm ‖x‖\\Vert x \\Vert, called “Banach spaces,” in order to precisely define the o(δx)o(\\delta x) terms that are dropped. We will come back to the subject of Banach spaces later.)\n\nLoosely, a vector space (over $\\R$) is a set of elements in which addition and subtraction between elements is defined, along with multiplication by real scalars. For instance, while it does not make sense to multiply arbitrary vectors in $\\R^n$, we can certainly add them together, and we can certainly scale the vectors by a constant factor.\n\nSome examples of vector spaces include:\n\n$\\R^n$, as described in the above. More generally, $\\R^{n\\times m}$, the space of n×mn\\times m matrices with real entries. Notice again that, if n≠mn\\neq m, then multiplication between elements is not defined.\n$C^0(\\R^n)$, the set of continuous functions over $\\R^n$, with addition defined pointwise.\n\n\nRecall that a linear operator is a map LL from a vector vv in vector space VV to a vector L[v]L[v] (sometimes denoted simply LvLv) in some other vector space. Specifically, LL is linear if L[v1+v2]=Lv1+Lv2  and  L[αv]=αL[v]L[v_1+v_2] = Lv_1 + Lv_2 \\text{~~and~~} L[\\alpha v] = \\alpha L[v] for scalars α∈ℝ\\alpha \\in \\mathbb{R}.\nRemark: In this course, f′f' is a map that takes in an xx and spits out a linear operator f′(x)f'(x) (the derivative of ff at xx). Furthermore, f′(x)f'(x) is a linear map that takes in an input direction vv and gives an output vector f′(x)[v]f'(x)[v] (which we will later interpret as a directional derivative, see Sec. 2.2.1). When the direction vv is an infinitesimal dxd x, the output f′(x)[dx]=dff'(x)[dx] = df is the differential of ff (the corresponding infinitesimal change in ff).\n\n\nThere are multiple notations for derivatives in common use, along with multiple related concepts of derivative, differentiation, and differentials. In the table below, we summarize several of these notations, and put boxes\\boxed{\\mathrm{boxes}} around the notations adopted for this course:\n\n\n\nname\nnotations\nremark\n\n\n\n\nderivative\nf′\\boxed{f'}, also dfdx\\frac{df}{dx}, DfDf, fxf_{x}, ∂xf\\partial_{x}f,\nlinear operator f′(x)f'(x) that maps a small change dxdx in the input to a small\n\n\n\n…\nchange df=f′(x)[dx]{df=f'(x)[dx]} in the output\n\n\n\n                                                                                         In single-variable calculus, this linear operator can be represented by a *single number*, the \"slope,\" e.g. if $f(x) = \\sin(x)$ then $f'(x) = \\cos(x)$ is the number that we multiply by $dx$ to get $dy = \\cos(x) dx$. In multi-variable calculus, the linear operator $f'(x)$ can be represented by a *matrix*, the **Jacobian** $J$ (see Sec. [3](#sec:kronecker){reference-type=\"ref\" reference=\"sec:kronecker\"}), so that $df = f'(x)[dx] = J\\, dx$. But we will see that it is not always convenient to express $f'$ as a matrix, even if we can.\ndifferentiation ′\\boxed{^{\\prime}}, ddx\\frac{d}{dx}, DD, … linear operator that maps a function ff to its derivative f′f' difference δx\\boxed{\\delta x} and δf=f(x+δx)−f(x)\\boxed{\\delta f} = f(x+\\delta x) - f(x) small (but not infinitesimal) change in the input xx and output ff (depending implicitly on xx and δx\\delta x), respectively: an element of a vector space, not a linear operator differential dx\\boxed{d x} and df=f(x+dx)−f(x)\\boxed{d f} = f(x+dx) - f(x) arbitrarily small (“infinitesimal”2 — we drop higher-order terms) change in the input xx and output ff, respectively: an element of a vector space, not a linear operator gradient ∇f\\boxed{\\nabla f} the vector whose inner product df=⟨∇f,dx⟩df = \\langle \\nabla f, dx \\rangle with a small change dxdx in the input gives the small change dfdf in the output. The “transpose of the derivative” ∇f=(f′)T\\nabla f = (f')^T. (See Sec. 2.3.) partial derivative ∂f∂x\\boxed{\\frac{\\partial f}{\\partial x}}, fxf_{x}, ∂xf\\partial_{x}f linear operator that maps a small change dxdx in a single argument of a multi-argument function to the corresponding change in output, e.g. for f(x,y)f(x,y) we have df=∂f∂x[dx]+∂f∂y[dy]df=\\frac{\\partial f}{\\partial x}[dx]+\\frac{\\partial f}{\\partial y}[dy].\n\nSome examples of linear operators include\n\nMultiplication by scalars α\\alpha, i.e. Lv=αvLv = \\alpha v. Also multiplication of column vectors vv by matrices AA, i.e. Lv=AvLv = Av.\nSome functions like f(x)=x2f(x)=x^2 are obviously nonlinear. But what about f(x)=x+1f(x)=x+1? This may look linear if you plot it, but it is not a linear operation, because f(2x)=2x+1≠2f(x)f(2x)=2x+1\\ne 2f(x)—such functions, which are linear plus a nonzero constant, are known as affine.\nThere are also many other examples of linear operations that are not so convenient or easy to write down as matrix–vector products. For example, if AA is a 3×33\\times 3 matrix, then L[A]=AB+CAL[A]=AB+CA is a linear operator given 3×33\\times 3 matrices B,CB,C. The transpose f(x)=xTf(x)=x^T of a column vector xx is linear, but is not given by any matrix multiplied by xx. Or, if we consider vector spaces of functions, then the calculus operations of differentiation and integration are linear operators too!\n\n\nDirectional derivatives\nThere is an equivalent way to interpret this linear-operator viewpoint of a derivative, which you may have seen before in multivariable calculus: as a directional derivative.\nIf we have a function f(x)f(x) of arbitrary vectors xx, then the directional derivative at xx in a direction (vector) vv is defined as: ∂∂αf(x+αv)|α=0=limδα→0f(x+δαv)−f(x)δα\\left. \\frac{\\partial}{\\partial \\alpha} f(x+\\alpha v) \\right|_{\\alpha = 0} = \\lim_{\\delta\\alpha\\to 0} \\frac{f(x+\\delta\\alpha \\, v) - f(x)}{\\delta \\alpha} where α\\alpha is a scalar. This transforms derivatives back into single-variable calculus from arbitrary vector spaces. It measures the rate of change of ff in the direction vv from xx. But it turns out that this has a very simple relationship to our linear operator f′(x)f'(x) from above, because (dropping higher-order terms due to the limit δα→0\\delta\\alpha \\to 0): f(x+dαv⏟dx)−f(x)=f′(x)[dx]=dαf′(x)[v],f(x + \\underbrace{d\\alpha \\, v}_{d x}) - f(x) = f'(x)[dx] = d\\alpha \\, f'(x)[v] \\, , where we have factored out the scalar dαd\\alpha in the last step thanks to f′(x)f'(x) being a linear operator. Comparing with above, we immediately find that the directional derivative is: ∂∂αf(x+αv)|α=0=f′(x)[v].\\boxed{ \\left. \\frac{\\partial}{\\partial \\alpha} f(x+\\alpha v) \\right|_{\\alpha = 0} = f'(x)[v]  }\\, . It is exactly equivalent to our f′(x)f'(x) from before! (We can also see this as an instance of the chain rule from Sec. 2.5.) One lesson from this viewpoint is that it is perfectly reasonable to input an arbitrary non-infinitesimal vector vv into f′(x)[v]f'(x)[v]: the result is not a dfdf, but is simply a directional derivative."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#sec:multivarPart1",
    "href": "matrix-calculus/en-US/index.html#sec:multivarPart1",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Revisiting multivariable calculus, Part 1: Scalar-valued functions",
    "text": "Revisiting multivariable calculus, Part 1: Scalar-valued functions\n\n\n\nFor a real-valued f(x)f(x), the gradient ∇f\\nabla f is defined so that it corresponds to the “uphill” direction at a point xx, which is perpendicular to the contours of ff. Although this may not point exactly towards the nearest local maximum of ff (unless the contours are circular), “going uphill” is nevertheless the starting point for many computational-optimization algorithms to search for a maximum.\n\n\nLet ff be a scalar-valued function, which takes in “column” vectors $x \\in \\R^n$ and produces a scalar (in $\\R$). Then, df=f(x+dx)−f(x)=f′(x)[dx]=scalar.df = f(x+ dx) - f(x) = f'(x) [dx] = \\text{scalar}. Therefore, since dxdx is a column vector (in an arbitrary direction, representing an arbitrary small change in xx), the linear operator f′(x)f'(x) that produces a scalar dfdf must be a row vector (a “1-row matrix”, or more formally something called a covector or “dual” vector or “linear form”)! We call this row vector the transpose of the gradient (∇f)T(\\nabla f)^T, so that dfdf is the dot (“inner”) product of dxdx with the gradient. So we have that $$df = \\nabla f \\cdot dx = \\underbrace{(\\nabla f)^T}_{f'(x)} dx \\hspace{.25cm} \\text{where} \\hspace{.25cm} dx = \\begin{pmatrix}\ndx_1 \\\\  dx_2 \\\\ \\vdots \\\\ dx_n.\n\\end{pmatrix} .$$ Some authors view the gradient as a row vector (equating it with f′f' or the Jacobian), but treating it as a “column vector” (the transpose of f′f'), as we do in this course, is a common and useful choice. As a column vector, the gradient can be viewed as the “uphill” (steepest-ascent) direction in the xx space, as depicted in Fig. 2. Furthermore, it is also easier to generalize to scalar functions of other vector spaces. In any case, for this class, we will always define ∇f\\nabla f to have the same “shape” as xx, so that dfdf is a dot product (“inner product”) of dxdx with the gradient.\nThis is perfectly consistent with the viewpoint of the gradient that you may remember from multivariable calculus, in which the gradient was a vector of components ∇f=(∂f∂x1∂f∂x2⋮∂f∂xn);\\nabla f = \\begin{pmatrix}\n\\frac{\\partial f}{\\partial x_1} \\\\  \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n}\n\\end{pmatrix} \\, ; or, equivalently, df=f(x+dx)−f(x)=∇f⋅dx=∂f∂x1dx1+∂f∂x2dx2+⋯+∂f∂xndxn.df = f(x+dx)-f(x) = \\nabla f \\cdot dx = \\frac{\\partial f}{\\partial x_1} dx_1 + \\frac{\\partial f}{\\partial x_2} dx_2 + \\cdots + \\frac{\\partial f}{\\partial x_n} dx_n \\, . While a component-wise viewpoint may sometimes be convenient, we want to encourage you to view the vector xx as a whole, not simply a collection of components, and to learn that it is often more convenient and elegant to differentiate expressions without taking the derivative component-by-component, a new approach that will generalize better to more complicated inputs/output vector spaces.\nLet’s look at an example to see how we compute this differential.\n\nConsider f(x)=xTAxf(x) = x^T Ax where $x\\in \\R^n$ and AA is a square n×nn\\times n matrix, and thus $f(x) \\in \\R$. Compute dfdf, f′(x)f'(x), and ∇f\\nabla f.\n\nWe can do this directly from the definition. $$\\begin{aligned}\n    df &= f(x+ dx) - f(x) \\\\\n    &=  (x+ dx)^T A (x+dx) - x^T A x \\\\\n    &= \\cancel{x^T A x} + dx^T\\, Ax + x^T A dx + \\cancelto{\\text{higher order}}{dx^T \\,A dx} - \\cancel{x^T A x} \\\\\n    &= \\underbrace{x^T(A+A^T)}_{f'(x)=(\\nabla f)^T }dx \\implies \\nabla f = (A+A^T)x \\, .\n\\end{aligned}$$ Here, we dropped terms with more than one dxdx factor as these are asymptotically negligible. Another trick was to combine dxTAxdx^T A x and xTAdxx^T A dx by realizing that these are scalars and hence equal to their own transpose: dxTAx=(dxTAx)T=xTATdxdx^T A x = (dx^T A x)^T = x^T A^T dx. Hence, we have found that f′(x)=xT(A+AT)=(∇f)Tf'(x) = x^T(A+A^T) = (\\nabla f)^T, or equivalently ∇f=[xT(A+AT)]T=(A+AT)x\\nabla f = [x^T(A+A^T)]^T = (A+A^T)x.\nIt is, of course, also possible to compute the same gradient component-by-component, the way you probably learned to do in multivariable calculus. First, you would need to write f(x)f(x) explicitly in terms of the components of xx, as f(x)=xTAx=∑i,jxiAi,jxjf(x) = x^T A x = \\sum_{i,j} x_i A_{i,j} x_j. Then, you would compute ∂f/∂xk\\partial f/\\partial x_k for each kk, taking care that xx appears twice in the ff summation. However, this approach is awkward, error-prone, labor-intensive, and quickly becomes worse as we move on to more complicated functions. It is much better, we feel, to get used to treating vectors and matrices as a whole, not as mere collections of numbers."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#revisiting-multivariable-calculus-part-2-vector-valued-functions",
    "href": "matrix-calculus/en-US/index.html#revisiting-multivariable-calculus-part-2-vector-valued-functions",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Revisiting multivariable calculus, Part 2: Vector-valued functions",
    "text": "Revisiting multivariable calculus, Part 2: Vector-valued functions\nNext time, we will revisit multi-variable calculus (18.02 at MIT) again in a Part 2, where now ff will be a vector-valued function, taking in vectors $x\\in \\R^n$ and giving vector outputs $f(x) \\in \\R^m$. Then, dfdf will be a mm-component column vector, dxdx will be an nn-component column vector, and we must get a linear operator f′(x)f'(x) satisfying df⏟m components=f′(x)⏟m×ndx⏟n components,\\underbrace{df}_{m\\text{ components}} = \\underbrace{f'(x)}_{m \\times n} \\underbrace{dx}_{n\\text{ components}} \\, , so f′(x)f'(x) must be an m×nm \\times n matrix called the Jacobian of ff!\nThe Jacobian matrix JJ represents the linear operator that takes dxdx to dfdf: df=Jdx.df = J dx \\, . The matrix JJ has entries Jij=∂fi∂xjJ_{ij}=\\frac{\\partial f_i}{\\partial x_j} (corresponding to the ii-th row and the jj-th column of JJ).\nSo now, suppose that $f: \\R^2 \\to \\R^2$. Let’s understand how we would compute the differential of ff: df=(∂f1∂x1∂f1∂x2∂f2∂x1∂f2∂x2)(dx1dx2)=(∂f1∂x1dx1+∂f1∂x2dx2∂f2∂x1dx1+∂f2∂x2dx2).df = \\begin{pmatrix}\n   \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} \\\\ \\frac{\\partial f_2}{\\partial x_1} & \\frac{\\partial f_2}{\\partial x_2}\n\\end{pmatrix} \\begin{pmatrix}\n     dx_1 \\\\ dx_2\n\\end{pmatrix} = \\begin{pmatrix}\n    \\frac{\\partial f_1}{\\partial x_1}dx_1 +  \\frac{\\partial f_1}{\\partial x_2}dx_2 \\\\ \\frac{\\partial f_2}{\\partial x_1}dx_1 +  \\frac{\\partial f_2}{\\partial x_2} dx_2\n\\end{pmatrix}.\nLet’s compute an example.\n\n Consider the function f(x)=Axf(x) = Ax where AA is a constant m×nm\\times n matrix. Then, by applying the distributive law for matrix–vector products, we have $$\\begin{aligned}\n    df &= f(x+dx ) - f(x) =  A(x + dx) - Ax  \\\\\n    &= \\cancel{Ax} + Adx - \\cancel{Ax} = Adx = f'(x) dx.\n\\end{aligned}$$ Therefore, f′(x)=Af'(x) = A.\n\nNotice then that the linear operator AA is its own Jacobian matrix!\nLet’s now consider some derivative rules.\n\nSum Rule: Given f(x)=g(x)+h(x)f(x) = g(x) + h(x), we get that df=dg+dh⟹f′(x)dx=g′(x)dx+h′(x)dx.df = dg + dh \\implies f'(x) dx = g'(x) dx + h'(x)dx. Hence, f′=g′+h′f' = g'+h' as we should expect. This is the linear operator f′(x)[v]=g′(x)[v]+h′(x)[v]f'(x)[v] = g'(x)[v] + h'(x)[v], and note that we can sum linear operators (like g′g' and h′h') just like we can sum matrices! In this way, linear operators form a vector space.\nProduct Rule: Suppose f(x)=g(x)h(x)f(x) = g(x) h(x). Then, $$\n$$\\begin{aligned}\n        df &= f(x+ dx) - f(x) \\\\\n        &= g(x+dx) h(x+dx) - g(x) h(x) \\\\\n        &= (g(x) + \\underbrace{g'(x) dx}_{dg}) (h(x) + \\underbrace{h'(x)dx}_{dh}) - g(x)h(x) \\\\\n        &= g h + dg\\, h + g\\,dh + \\cancelto{0}{dg \\,dh} - gh \\\\\n        &= dg \\, h + g \\,dh \\, ,\n\n\\end{aligned}$$\n$$ where the dgdhdg \\, dh term is higher-order and hence dropped in infinitesimal notation. Note, as usual, that dgdg and hh may not commute now as they may no longer be scalars!\n\nLet’s look at some short examples of how we can apply the product rule nicely.\n\n Let f(x)=Axf(x) = Ax (mapping $\\R^n \\to \\R^m$) where AA is a constant m×nm\\times n matrix. Then, $$df = d(Ax) =  \\cancelto{0}{dA} \\, x + Adx = Adx \\implies f'(x) = A.$$ We have dA=0{dA} = 0 here because AA does not change when we change xx.\n\n\nLet f(x)=xTAxf(x) = x^T A x (mapping $\\R^n \\to \\R$). Then, df=dxT(Ax)+xTd(Ax)=dxTAx⏟=xTATdx+xTAdx=xT(A+AT)dx=(∇f)Tdx,df = dx^T(Ax) + x^T d(Ax) = \\underbrace{dx^T \\, Ax}_{=\\, x^T A^Tdx} + x^T Adx = x^T(A+A^T)dx  = (\\nabla f)^T dx\\, , and hence f′(x)=xT(A+AT)f'(x) = x^T(A + A^T). (In the common case where AA is symmetric, this simplifies to f′(x)=2xTAf'(x) = 2x^T A.) Note that here we have applied Example [ex:Ax] in computing d(Ax)=Adxd(Ax) = A dx. Furthermore, ff is a scalar valued function, so we may also obtain the gradient ∇f=(A+AT)x\\nabla f = (A+A^T) x as before (which simplifies to 2Ax2Ax if AA is symmetric).\n\n\nGiven $x,y\\in \\R^m$, define x.*y=(x1y1⋮xmym)=(x1x2⋱xm)⏟diag(x)(y1⋮ym),x\\mathbin{.*}y = \\begin{pmatrix}\n        x_1y_1 \\\\ \\vdots \\\\ x_my_m\n    \\end{pmatrix} = \\underbrace{\\begin{pmatrix} x_1 & & & \\\\ & x_2 & & \\\\ & & \\ddots & \\\\ & & & x_m \\end{pmatrix}}_{\\mathrm{diag}(x)} \\begin{pmatrix}\n        y_1 \\\\ \\vdots \\\\ y_m\n    \\end{pmatrix}, the element-wise product of vectors (also called the Hadamard product), where for convenience below we also define diag(x)\\mathrm{diag}(x) as the m×mm \\times m diagonal matrix with xx on the diagonal. Then, given $A \\in \\R^{m,n}$, define $f:\\R^n \\to \\R^m$ via f(x)=A(x.*x).f(x) = A(x\\mathbin{.*}x).\nAs an exercise, one can verify the following:\n\nx.*y=y.*xx\\mathbin{.*}y = y\\mathbin{.*}x,\nA(x.*y)=Adiag(x)yA(x\\mathbin{.*}y) = A \\, \\mathrm{diag}(x)\\, y.\nd(x.*y)=(dx).*y+x.*(dy)d(x\\mathbin{.*}y) = (dx)\\mathbin{.*}y + x\\mathbin{.*}(dy). So if we take yy to be a constant and define g(x)=y.*xg(x) = y \\mathbin{.*}x, its Jacobian matrix is diag(y)\\mathrm{diag}(y).\ndf=A(2x.*dx)=2Adiag(x)dx=f′(x)[dx]df = A(2x\\mathbin{.*}dx) = 2A \\, \\mathrm{diag}(x) \\, dx = f'(x)[dx], so the Jacobian matrix is J=2Adiag(x)J = 2A \\, \\mathrm{diag}(x).\nNotice that the directional derivative (Sec. 2.2.1) of ff at xx in the direction vv is simply given by f′(x)[v]=2A(x.*v)f'(x)[v] = 2A (x \\mathbin{.*}v). One could also check numerically for some arbitrary A,x,vA,x,v that f(x+10−8v)−f(x)≈10−8(2A(x.*v))f(x + 10^{-8} v) - f(x) \\approx 10^{-8} ( 2A (x \\mathbin{.*}v))."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#sec:chainrule",
    "href": "matrix-calculus/en-US/index.html#sec:chainrule",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "The Chain Rule",
    "text": "The Chain Rule\nOne of the most important rules from differential calculus is the chain rule, because it allows us to differentiate complicated functions built out of compositions of simpler functions. This chain rule can also be generalized to our differential notation in order to work for functions on arbitrary vector spaces:\n\nChain Rule: Let f(x)=g(h(x))f(x) = g(h(x)). Then, $$\ndf=f(x+dx)−f(x)=g(h(x+dx))−g(h(x))=g′(h(x))[h(x+dx)−h(x)]=g′(h(x))[h′(x)[dx]]=g′(h(x))h′(x)[dx]\\begin{aligned}\n    df = f(x + dx) - f(x) &= g(h( x + dx)) - g(h(x)) \\\\\n    &= g'(h(x))[h(x + dx) - h(x)] \\\\\n    &= g'(h(x))[h'(x)[dx]] \\\\\n    &= g'(h(x)) h'(x) [dx]\n\n\\end{aligned}\n$$ where g′(h(x))h′(x)g'(h(x))h'(x) is a composition of g′g' and h′h' as matrices.\nIn other words, f′(x)=g′(h(x))h′(x)f'(x) = g'(h(x)) h'(x): the Jacobian (linear operator) f′f' is simply the product (composition) of the Jacobians, g′h′g' h'. Ordering matters because linear operators do not generally commute: left-to-right = outputs-to-inputs.\n\nLet’s look more carefully at the shapes of these Jacobian matrices in an example where each function maps a column vector to a column vector:\n\nLet $x\\in \\R^n$, $h(x) \\in \\R^p$, and $g(h(x)) \\in \\R^m$. Then, let f(x)=g(h(x))f(x) = g(h(x)) mapping from $\\R^n$ to $\\R^m$. The chain rule then states that f′(x)=g′(h(x))h′(x),f'(x) = g'(h(x)) h'(x), which makes sense as g′g' is an m×pm\\times p matrix and h′h' is a p×np\\times n matrix, so that the product gives an m×nm \\times n matrix f′f'! However, notice that this is not the same as h′(x)g′(h(x))h'(x) g'(h(x)) as you cannot (if n≠mn\\neq m) multiply a p×np\\times n and an m×pm\\times p matrix together, and even if n=mn = m you will get the wrong answer since they probably won’t commute.\n\nNot only does the order of the multiplication matter, but the associativity of matrix multiplication matters practically. Let’s consider a function f(x)=a(b(c(x)))f(x)= a(b(c(x))) where $c: \\R^n \\to \\R^p$, $b:\\R^p \\to \\R^q$, and $a: \\R^q\\to \\R^m$. Then, we have that, by the chain rule, f′(x)=a′(b(c(x)))b′(c(x))c′(x).f'(x) = a'(b(c(x))) b'(c(x))c'(x). Notice that this is the same as f′=(a′b′)c′=a′(b′c′)f'  = (a' b')c' = a'(b'c') by associativity (omitting the function arguments for brevity). The left-hand side is multiplication from left to right, and the right-hand side is multiplication from right to left.\nBut who cares? Well it turns out that associativity is deeply important. So important that the two orderings have names: multiplying left-to-right is called “reverse mode” and multiplying right-to-left is called “forward mode” in the field of automatic differentiation (AD). Reverse-mode differentation is also known as an “adjoint method” or “backpropagation” in some contexts, which we will explore in more detail later. Why does this matter? Let’s think about the computational cost of matrix multiplication.\n\nCost of Matrix Multiplication\n\n\n\nMatrix multiplication is associative—that is, (AB)C=A(BC)(AB)C = A(BC) for all A,B,CA,B,C—but multiplying left-to-right can be much more efficient than right-to-left if the leftmost matrix has only one (or few) rows, as shown here. Correspondingly, the order in which you carry out the chain rule has dramatic consequences for the computational effort required. Left-to-right is known as “reverse mode” or “backpropagation”, and is best suited to situations where there are many fewer outputs than inputs.\n\n\nIf you multiply a m×qm\\times q matrix by a q×pq\\times p matrix, you normally do it by computing mpmp dot products of length qq (or some equivalent re-ordering of these operations). To do a dot product of length qq requires qq multiplications and q−1q-1 additions of scalars. Overall, this is approximately 2mpq2mpq scalar operations in total. In computer science, you would write that this is “Θ(mpq)\\Theta(mpq)”: the computational effort is asymptotically proportional to mpqmpq for large m,p,qm,p,q.\nSo why does the order of the chain rule matter? Consider the following two examples.\n\nSuppose you have a lot of inputs n≫1n \\gg 1, and only one output m=1m=1, with lots of intermediate values, i.e. q=p=nq=p=n. Then reverse mode (left-to-right) will cost Θ(n2)\\Theta(n^2) scalar operations while forward mode (right-to-left) would cost Θ(n3)\\Theta(n^3)! This is a huge cost difference, depicted schematically in Fig. 3.\nConversely, suppose you have a lot of outputs m≫1m \\gg 1 and only one input n=1n=1, with lots of intermediate values q=p=mq=p=m. Then reverse mode would cost Θ(m3)\\Theta(m^3) operations but forward mode would be only Θ(m2)\\Theta(m^2)!\nMoral: If you have a lot of inputs and few outputs (the usual case in machine learning and optimization), compute the chain rule left-to-right (reverse mode). If you have a lot of outputs and few inputs, compute the chain rule right-to-left (forward mode). We return to this in Sec. 8.4."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#beyond-multi-variable-derivatives",
    "href": "matrix-calculus/en-US/index.html#beyond-multi-variable-derivatives",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Beyond Multi-Variable Derivatives",
    "text": "Beyond Multi-Variable Derivatives\nNow let’s compute some derivatives that go beyond first-year calculus, where the inputs and outputs are in more general vector spaces. For instance, consider the following examples:\n\nLet AA be an n×nn\\times n matrix. You could have the following matrix-valued functions. For example:\n\nf(A)=A3f(A) = A^3,\nf(A)=A−1f(A) = A^{-1} if AA is invertible,\nor UU, where UU is the resulting matrix after applying Gaussian elimination to AA!\n\nYou could also have scalar outputs. For example:\n\nf(A)=detAf(A) = \\det A,\nf(A)=f(A) = trace AA,\nor f(A)=σ1(A)f(A) = \\sigma_1(A), the largest singular value of AA.\n\n\nLet’s focus on two simpler examples for this lecture.\n\nLet f(A)=A3f(A) = A^3 where AA is a square matrix. Compute dfdf.\n\nHere, we apply the chain rule one step at a time: df=dAA2+AdAA+A2dA=f′(A)[dA].df = dA\\, A^2 + A \\,dA \\, A + A^2 \\,dA = f'(A) [dA]. Notice that this is not equal to 3A23A^2 (unless dAdA and AA commute, which won’t generally be true since dAdA represents an arbitrary small change in AA). The right-hand side is a linear operator f′(A)f'(A) acting on dAdA, but it is not so easy to interpret it as simply a single “Jacobian” matrix multiplying dAdA!\n\nLet f(A)=A−1f(A) = A^{-1} where AA is a square invertible matrix. Compute df=d(A−1)df = d(A^{-1}).\n\nHere, we use a slight trick. Notice that $AA^{-1} = \\Id$, the identity matrix. Thus, we can compute the differential using the product rule (noting that $d\\Id = 0$, since changing AA does not change $\\Id$) so $$d(AA^{-1}) = dA\\, A^{-1} + A\\,d(A^{-1}) = d(\\Id) = 0\\implies d(A^{-1}) = -A^{-1} \\,dA \\,A^{-1}.$$"
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#derivatives-of-matrix-functions-linear-operators",
    "href": "matrix-calculus/en-US/index.html#derivatives-of-matrix-functions-linear-operators",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Derivatives of matrix functions: Linear operators",
    "text": "Derivatives of matrix functions: Linear operators\nAs we have already emphasized, the derivative f′f' is the linear operator that maps a small change in the input to a small change in the output. This idea can take an unfamiliar form, however, when applied to functions f(A)f(A) that map matrix inputs AA to matrix outputs. For example, we’ve already considered the following functions on square m×mm\\times m matrices:\n\nf(A)=A3f(A)=A^{3}, which gives df=f′(A)[dA]=dAA2+AdAA+A2dAdf=f'(A)[dA]=dA\\,A^{2}+A\\,dA\\,A+A^{2}\\,dA.\nf(A)=A−1f(A)=A^{-1}, which gives df=f′(A)[dA]=−A−1dAA−1df=f'(A)[dA]=-A^{-1}\\,dA\\,A^{-1}\n\n\nAn even simpler example is the matrix-square function:\nf(A)=A2,f(A)=A^{2}\\,, which by the product rule gives df=f′(A)[dA]=dAA+AdA.df=f'(A)[dA]=dA\\,A+A\\,dA\\,. You can also work this out explicitly from df=f(A+dA)−f(A)=(A+dA)2−A2df=f(A+dA)-f(A)=(A+dA)^{2}-A^{2}, dropping the (dA)2(dA)^{2} term.\n\nIn all of these examples, f′(A)f'(A) is described by a simple formula for f′(A)[dA]f'(A)[dA] that relates an arbitrary change dAdA in AA to the change df=f(A+dA)−f(A)df=f(A+dA)-f(A) in ff, to first order. If the differential is distracting you, realize that we can plug any matrix XX we want into this formula, not just an “infinitesimal” change dAdA, e.g. in our matrix-square example we have f′(A)[X]=XA+AXf'(A)[X]=XA+AX for an arbitrary XX (a directional derivative, from Sec. 2.2.1). This is linear in XX: if we scale or add inputs, it scales or adds outputs, respectively: f′(A)[2X]=2XA+A2X=2(XA+AX)=2f′(A)[X],f'(A)[2X]=2XA+A\\,2X=2(XA+AX)=2f'(A)[X]\\,, f′(A)[X+Y]=(X+Y)A+A(X+Y)=XA+YA+AX+AY=XA+AX+YA+AY=f′(A)[X]+f′(A)[Y].\\begin{aligned}\nf'(A)[X+Y] & =(X+Y)A+A(X+Y)=XA+YA+AX+AY=XA+AX+YA+AY\\\\\n & =f'(A)[X]+f'(A)[Y]\\,.\n\\end{aligned} This is a perfectly good way to define a linear operation! We are not expressing it here in the familiar form f′(A)[X]=(some matrix?)×(X vector?)f'(A)[X]=(\\text{some matrix?})\\times(X\\text{ vector?}), and that’s okay! A formula like XA+AXXA+AX is easy to write down, easy to understand, and easy to compute with.\nBut sometimes you still may want to think of f′f' as a single “Jacobian” matrix, using the most familiar language of linear algebra, and it is possible to do that! If you took a sufficiently abstract linear-algebra course, you may have learned that any linear operator can be represented by a matrix once you choose a basis for the input and output vector spaces. Here, however, we will be much more concrete, because there is a conventional “Cartesian” basis for matrices AA called “vectorization”, and in this basis linear operators like AX+XAAX+XA are particularly easy to represent in matrix form once we introduce a new type of matrix product that has widespread applications in “multidimensional” linear algebra."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#a-simple-example-the-two-by-two-matrix-square-function",
    "href": "matrix-calculus/en-US/index.html#a-simple-example-the-two-by-two-matrix-square-function",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "A simple example: The two-by-two matrix-square function",
    "text": "A simple example: The two-by-two matrix-square function\nTo begin with, let’s look in more detail at our matrix-square function f(A)=A2f(A)=A^{2} for the simple case of 2×22\\times2 matrices, which are described by only four scalars, so that we can look at every term in the derivative explicitly. In particular,\n\nFor a 2×22\\times2 matrix A=(prqs),A=\\begin{pmatrix}p & r\\\\\nq & s\n\\end{pmatrix}, the matrix-square function is f(A)=A2=(prqs)(prqs)=(p2+qrpr+rspq+qsqr+s2).f(A)=A^{2}=\\begin{pmatrix}p & r\\\\\nq & s\n\\end{pmatrix}\\begin{pmatrix}p & r\\\\\nq & s\n\\end{pmatrix}=\\begin{pmatrix}p^{2}+qr & pr+rs\\\\\npq+qs & qr+s^{2}\n\\end{pmatrix}.\n\nWritten out explicitly in terms of the matrix entries (p,q,r,s)(p,q,r,s) in this way, it is natural to think of our function as mapping 4 scalar inputs to 4 scalar outputs. That is, we can think of ff as equivalent to a “vectorized” function $\\tilde{f}:\\R^{4}\\to\\R^{4}$ given by f̃((pqrs))=(p2+qrpq+qspr+rsqr+s2).\\tilde{f}(\\left(\\begin{array}{c}\np\\\\\nq\\\\\nr\\\\\ns\n\\end{array}\\right))=\\left(\\begin{array}{c}\np^{2}+qr\\\\\npq+qs\\\\\npr+rs\\\\\nqr+s^{2}\n\\end{array}\\right)\\,. Converting a matrix into a column vector in this way is called vectorization, and is commonly denoted by the operation “vec\\operatorname{vec}”: $$\\begin{aligned}\n\\operatorname{vec}A & =\\operatorname{vec}\\begin{pmatrix}p & r\\\\\nq & s\n\\end{pmatrix}=%\n\\begingroup\n    %   \\br@kwd depends on font size, so compute it now.\n    \\setbox 0=\\hbox{$\\left(\\right.$}\n    \\setlength{\\br@kwd}{\\wd 0}\n        %   Compute the array strut based on current value of \\arraystretch.\n    \\setbox\\@arstrutbox\\hbox{\\vrule\n        \\@height\\arraystretch\\ht\\strutbox   \n        \\@depth\\arraystretch\\dp\\strutbox\n        \\@width\\z@}\n        %   Compute height of first row and extra space.\n    \\setlength{\\k@bordht}{\\kbrowsep}\n    \\addtolength{\\k@bordht}{\\ht\\@arstrutbox}\n    \\addtolength{\\k@bordht}{\\dp\\@arstrutbox}\n        %   turn off mathsurround\n    \\m@th\n        %   Set the first row style\n    \\def\\@kbrowstyle{\\scriptstyle}\n        %   Swallow the alignment into box0:\n    \\setbox 0=\\vbox{%\n            %   Define \\cr for first row to include the \\kbrowsep\n            %   and to reset the row style\n        \\def\\cr{\\crcr\\noalign{\\kern\\kbrowsep\n            \\global\\let\\cr=\\endline\n            \\global\\let\\@kbrowstyle=\\relax}}\n            %   Redefine \\\\ a la LaTeX:\n        \\let\\\\\\@arraycr\n            %   The following are needed to make a solid \\vrule with no gaps\n            %   between the lines.\n        \\lineskip\\z@skip    \n        \\baselineskip\\z@skip    \n            %   Compute the length of the skip after the first column\n        \\dimen 0\\kbcolsep \\advance\\dimen 0\\br@kwd\n            %   Here begins the alignment:\n        \\ialign{\\tabskip\\dimen 0        %   This space will show up after the first column\n            \\kern\\arraycolsep\\hfil\\@arstrut$\\scriptstyle##$\\hfil\\kern\\arraycolsep&\n            \\tabskip\\z@skip         %   Cancel extra space for other columns\n            \\kern\\arraycolsep\\hfil$\\@kbrowstyle ##$\\hfil\\kern\\arraycolsep&&\n            \\kern\\arraycolsep\\hfil$\\@kbrowstyle ##$\\hfil\\kern\\arraycolsep\\crcr\n                %   That ends the template.\n                %   Here is the argument:\n             & \\\\\nA_{1,1} & p\\\\\nA_{2,1} & q\\\\\nA_{1,2} & r\\\\\nA_{2,2} & s\n\\crcr}%     End \\ialign\n    }%  End \\setbox0.\n        %   \\box0 now holds the array.\n        %\n        %   This next line uses \\box2 to hold a throwaway\n        %   copy of \\box0, leaving \\box0 intact,\n        %   while putting the last row in \\box5.\n    \\setbox 2=\\vbox{\\unvcopy 0 \\global\\setbox 5=\\lastbox}\n        %   We want the width of the first column,\n        %   so we lop off columns until there is only one left.\n        %   It's not elegant or efficient, but at 1 gHz, who cares.\n    \\loop\n        \\setbox 2=\\hbox{\\unhbox 5 \\unskip \\global\\setbox 3=\\lastbox}\n        \\ifhbox 3\n            \\global\\setbox 5=\\box 2\n            \\global\\setbox 1=\\box 3\n    \\repeat\n        %   \\box1 now holds the first column of last row.\n        %\n        %   This next line stores the alignment in \\box2,\n        %   while calculating the proper\n        %   delimiter height and placement.\n    \\setbox 2=\\hbox{$\\kern\\wd 1\\kern\\kbcolsep\\kern-\\arraycolsep\n        \\left(\n        \\kern-\\wd 1\\kern-\\kbcolsep\\kern-\\br@kwd\n            %\n            %   Here is the output.  The \\vcenter aligns the array with the \"math axis.\"\n            %   The negative vertical \\kern only shrinks the delimiter's height.\n            %   BTW, I didn't find this in the TeXbook,\n            %   I had to try various \\kerns to see what they did in a\n            %   \\left[\\vcenter{}\\right].\n    \\vcenter{\\kern-\\k@bordht\\vbox{\\unvbox 0}}\n    \\right)$}\n    \\null\\vbox{\\kern\\k@bordht\\box 2}\n        %\n    \\endgroup\n\\, ,\\\\\n\\operatorname{vec}f(A) & =\\operatorname{vec}\\begin{pmatrix}p^{2}+qr & pr+rs\\\\\npq+qs & qr+s^{2}\n\\end{pmatrix}=\\left(\\begin{array}{c}\np^{2}+qr\\\\\npq+qs\\\\\npr+rs\\\\\nqr+s^{2}\n\\end{array}\\right) \\, .\n\\end{aligned}$$ In terms of vec\\operatorname{vec}, our “vectorized” matrix-squaring function f̃\\tilde{f} is defined by f̃(vecA)=vecf(A)=vec(A2).\\tilde{f}(\\operatorname{vec}A)=\\operatorname{vec}f(A)=\\operatorname{vec}(A^{2})\\,. More generally,\n\nThe vectorization vecA∈ℝmn\\operatorname{vec}A\\in\\mathbb{R}^{mn} of any m×nm\\times n matrix A∈ℝm×nA\\in\\mathbb{R}^{m\\times n} is a defined by simply stacking the columns of AA, from left to right, into a column vector vecA\\operatorname{vec}A. That is, if we denote the nn columns of AA by mm-component vectors a→1,a→2,…∈ℝm\\vec{a}_{1},\\vec{a}_{2},\\ldots\\in\\mathbb{R}^{m}, then vecA=vec(a→1a→2⋯a→n)⏟A∈ℝm×n=(a→1a→2⋮a→n)∈ℝmn\\operatorname{vec}A=\\operatorname{vec}\\underbrace{\\left(\\begin{array}{cccc}\n\\vec{a}_{1} & \\vec{a}_{2} & \\cdots & \\vec{a}_{n}\\end{array}\\right)}_{A\\in\\mathbb{R}^{m\\times n}}=\\left(\\begin{array}{c}\n\\vec{a}_{1}\\\\\n\\vec{a}_{2}\\\\\n\\vdots\\\\\n\\vec{a}_{n}\n\\end{array}\\right)\\in\\mathbb{R}^{mn} is an mnmn-component column vector containing all of the entries of AA.\nOn a computer, matrix entries are typically stored in a consecutive sequence of memory locations, which can be viewed a form of vectorization. In fact, vecA\\operatorname{vec}A corresponds exactly to what is known as “column-major” storage, in which the column entries are stored consecutively; this is the default format in Fortran, Matlab, and Julia, for example, and the venerable Fortran heritage means that column major is widely used in linear-algebra libraries.\n\n.\n\nThe vector vecA\\operatorname{vec}A corresponds to the coefficients you get when you express the m×nm\\times n matrix AA in a basis of matrices. What is that basis?\n\nVectorization turns unfamilar things (like matrix functions and derivatives thereof) into familiar things (like vector functions and Jacobians or gradients thereof). In that way, it can be a very attractive tool, almost too attractive—why do “matrix calculus” if you can turn everything back into ordinary multivariable calculus? Vectorization has its drawbacks, however: conceptually, it can obscure the underlying mathematical structure (e.g. f̃\\tilde{f} above doesn’t look much like a matrix square A2A^{2}), and computationally this loss of structure can sometimes lead to severe inefficiencies (e.g. forming huge m2×m2m^2\\times m^2 Jacobian matrices as discussed below). Overall, we believe that the primary way to study matrix functions like this should be to view them as having matrix inputs (AA) and matrix outputs (A2A^{2}), and that one should likewise generally view the derivatives as linear operators on matrices, not vectorized versions thereof. However, it is still useful to be familiar with the vectorization viewpoint in order to have the benefit of an alternative perspective.\n\nThe matrix-squaring four-by-four Jacobian matrix\nTo understand Jacobians of functions (from matrices to matrices), let’s begin by considering a basic question:\n\nWhat is the size of the Jacobian of the matrix-square function?\n\nWell, if we view the matrix squaring function via its vectorized equivalent f̃\\tilde{f}, mapping ℝ4↦ℝ4\\mathbb{R}^{4}\\mapsto\\mathbb{R}^{4} (4-component column vectors to 4-component column vectors), the Jacobian would be a 4×44\\times4 matrix (formed from the derivatives of each output component with respect to each input component). Now let’s think about a more general square matrix AA: an m×mm\\times m matrix. If we wanted to find the Jacobian of f(A)=A2f(A)=A^{2}, we could do so by the same process and (symbolically) obtain an m2×m2m^{2}\\times m^{2} matrix (since there are m2m^{2} inputs, the entries of AA, and m2m^{2} outputs, the entries of A2A^{2}). Explicit computation of these m4m^{4} partial derivatives is rather tedious even for small mm, but is a task that symbolic computational tools in e.g. Julia or Mathematica can handle. In fact, as seen in the Notebook, Julia spits out the Jacobian quite easily. For the m=2m=2 case that we wrote out explicitly above, you can either take the derivative of f̃\\tilde{f} by hand or use Julia’s symbolic tools to obtain the Jacobian: $$\\tilde{f}'=%\n\\begingroup\n    %   \\br@kwd depends on font size, so compute it now.\n    \\setbox 0=\\hbox{$\\left(\\right.$}\n    \\setlength{\\br@kwd}{\\wd 0}\n        %   Compute the array strut based on current value of \\arraystretch.\n    \\setbox\\@arstrutbox\\hbox{\\vrule\n        \\@height\\arraystretch\\ht\\strutbox   \n        \\@depth\\arraystretch\\dp\\strutbox\n        \\@width\\z@}\n        %   Compute height of first row and extra space.\n    \\setlength{\\k@bordht}{\\kbrowsep}\n    \\addtolength{\\k@bordht}{\\ht\\@arstrutbox}\n    \\addtolength{\\k@bordht}{\\dp\\@arstrutbox}\n        %   turn off mathsurround\n    \\m@th\n        %   Set the first row style\n    \\def\\@kbrowstyle{\\scriptstyle}\n        %   Swallow the alignment into box0:\n    \\setbox 0=\\vbox{%\n            %   Define \\cr for first row to include the \\kbrowsep\n            %   and to reset the row style\n        \\def\\cr{\\crcr\\noalign{\\kern\\kbrowsep\n            \\global\\let\\cr=\\endline\n            \\global\\let\\@kbrowstyle=\\relax}}\n            %   Redefine \\\\ a la LaTeX:\n        \\let\\\\\\@arraycr\n            %   The following are needed to make a solid \\vrule with no gaps\n            %   between the lines.\n        \\lineskip\\z@skip    \n        \\baselineskip\\z@skip    \n            %   Compute the length of the skip after the first column\n        \\dimen 0\\kbcolsep \\advance\\dimen 0\\br@kwd\n            %   Here begins the alignment:\n        \\ialign{\\tabskip\\dimen 0        %   This space will show up after the first column\n            \\kern\\arraycolsep\\hfil\\@arstrut$\\scriptstyle##$\\hfil\\kern\\arraycolsep&\n            \\tabskip\\z@skip         %   Cancel extra space for other columns\n            \\kern\\arraycolsep\\hfil$\\@kbrowstyle ##$\\hfil\\kern\\arraycolsep&&\n            \\kern\\arraycolsep\\hfil$\\@kbrowstyle ##$\\hfil\\kern\\arraycolsep\\crcr\n                %   That ends the template.\n                %   Here is the argument:\n            & (1,1) & (2,1) & (1,2) & (2,2) \\\\\n(1,1) & 2p & r & q & 0\\\\\n(2,1) & q & p+s & 0 & q\\\\\n(1,2) & r & 0 & p+s & r\\\\\n(2,2) & 0 & r & q & 2s\n\\crcr}%     End \\ialign\n    }%  End \\setbox0.\n        %   \\box0 now holds the array.\n        %\n        %   This next line uses \\box2 to hold a throwaway\n        %   copy of \\box0, leaving \\box0 intact,\n        %   while putting the last row in \\box5.\n    \\setbox 2=\\vbox{\\unvcopy 0 \\global\\setbox 5=\\lastbox}\n        %   We want the width of the first column,\n        %   so we lop off columns until there is only one left.\n        %   It's not elegant or efficient, but at 1 gHz, who cares.\n    \\loop\n        \\setbox 2=\\hbox{\\unhbox 5 \\unskip \\global\\setbox 3=\\lastbox}\n        \\ifhbox 3\n            \\global\\setbox 5=\\box 2\n            \\global\\setbox 1=\\box 3\n    \\repeat\n        %   \\box1 now holds the first column of last row.\n        %\n        %   This next line stores the alignment in \\box2,\n        %   while calculating the proper\n        %   delimiter height and placement.\n    \\setbox 2=\\hbox{$\\kern\\wd 1\\kern\\kbcolsep\\kern-\\arraycolsep\n        \\left(\n        \\kern-\\wd 1\\kern-\\kbcolsep\\kern-\\br@kwd\n            %\n            %   Here is the output.  The \\vcenter aligns the array with the \"math axis.\"\n            %   The negative vertical \\kern only shrinks the delimiter's height.\n            %   BTW, I didn't find this in the TeXbook,\n            %   I had to try various \\kerns to see what they did in a\n            %   \\left[\\vcenter{}\\right].\n    \\vcenter{\\kern-\\k@bordht\\vbox{\\unvbox 0}}\n    \\right)$}\n    \\null\\vbox{\\kern\\k@bordht\\box 2}\n        %\n    \\endgroup\n\\,.$$ For example, the first row of f̃′\\tilde{f}' consists of the partial derivatives of p2+qrp^{2}+qr (the first output) with respect to the 4 inputs p,q,r,and sp,q,r,\\mbox{and }s. Here, we have labeled the rows by the (row,column) indices (jout,kout)(j_\\mathrm{out}, k_\\mathrm{out}) of the entries in the “output” matrix d(A2)d(A^2), and have labeled the columns by the indices (jin,kin)(j_\\mathrm{in}, k_\\mathrm{in}) of the entries in the “input” matrix AA. Although we have written the Jacobian f̃′\\tilde{f}' as a “2d” matrix, you can therefore also imagine it to be a “4d” matrix indexed by jout,kout,jin,kinj_\\mathrm{out}, k_\\mathrm{out}, j_\\mathrm{in}, k_\\mathrm{in}.\nHowever, the matrix-calculus approach of viewing the derivative f′(A)f'(A) as a linear transformation on matrices (as we derived above), f′(A)[X]=XA+AX,f'(A)[X]=XA+AX\\,, seems to be much more revealing than writing out an explicit component-by-component “vectorized” Jacobian f̃′\\tilde{f}', and gives a formula for any m×mm\\times m matrix without laboriously requiring us to take m4m^{4} partial derivatives one-by-one. If we really want to pursue the vectorization perspective, we need a way to recapture some of the structure that is obscured by tedious componentwise differentiation. A key tool to bridge the gap between the two perspectives is a type of matrix operation that you may not be familiar with: Kronecker products (denoted ⊗\\otimes)."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#kronecker-products",
    "href": "matrix-calculus/en-US/index.html#kronecker-products",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Kronecker Products",
    "text": "Kronecker Products\nA linear operation like f′(A)[X]=XA+AXf'(A)[X]=XA+AX can be thought of as a “higher-dimensional matrix:” ordinary “2d” matrices map “1d” column vectors to 1d column vectors, whereas to map 2d matrices to 2d matrices you might imagine a “4d” matrix (sometimes called a tensor). To transform 2d matrices back into 1d vectors, we already saw the concept of vectorization (vecA\\operatorname{vec}A). A closely related tool, which transforms “higher dimensional” linear operations on matrices back into “2d” matrices for the vectorized inputs/outputs, is the Kronecker product A⊗BA\\otimes B. Although they don’t often appear in introductory linear-algebra courses, Kronecker products show up in a wide variety of mathematical applications where multidimensional data arises, such as multivariate statistics and data science or multidimensional scientific/engineering problems.\n\nIf AA is an m×nm\\times n matrix with entries aija_{ij} and BB is a p×qp\\times q matrix, then their Kronecker product A⊗BA\\otimes B is defined by A=(a11⋯a1n⋮⋱⋮am1⋯amn)⇒A⏟m×n⊗B⏟p×q=(a11B⋯a1nB⋮⋱⋮am1B⋯amnB)⏟mp×nq,A=\\left(\\begin{array}{ccc}\na_{11} & \\cdots & a_{1n}\\\\\n\\vdots & \\ddots & \\vdots\\\\\na_{m1} & \\cdots & a_{mn}\n\\end{array}\\right)\\Longrightarrow\\underbrace{A}_{m\\times n}\\otimes\\underbrace{B}_{p\\times q}=\\underbrace{\\left(\\begin{array}{ccc}\na_{11}B & \\cdots & a_{1n}B\\\\\n\\vdots & \\ddots & \\vdots\\\\\na_{m1}B & \\cdots & a_{mn}B\n\\end{array}\\right)}_{mp\\times nq}\\,, so that A⊗BA\\otimes B is an mp×nqmp\\times nq matrix formed by “pasting in” a copy of BB multiplying every element of AA.\n\nFor example, consider 2×22\\times2 matrices A=(prqs)  and  B=(acbd).A=\\begin{pmatrix}p & r\\\\\nq & s\n\\end{pmatrix}\\text{~~and~~}B=\\begin{pmatrix}a & c\\\\\nb & d\n\\end{pmatrix} \\, . Then A⊗BA\\otimes B is a 4×44\\times4 matrix containing all possible products of entries AA with entries of BB. Note that A⊗B≠B⊗AA\\otimes B\\ne B\\otimes A (but the two are related by a re-ordering of the entries): $$A\\otimes B=\\begin{pmatrix}p\\textcolor{red}{B} & rB\\\\\nqB & sB\n\\end{pmatrix}=\\begin{pmatrix}p\\textcolor{red}{a} & p\\textcolor{red}{c} & ra & rc\\\\\np\\textcolor{red}{b} & p\\textcolor{red}{d} & rb & rd\\\\\nqa & qc & sa & sc\\\\\nqb & qd & sb & sd\n\\end{pmatrix}\\qquad\\ne\\qquad B\\otimes A=\\begin{pmatrix}aA & cA\\\\\nbA & dA\n\\end{pmatrix}=\\begin{pmatrix}\\textcolor{red}{a}p & ar & \\textcolor{red}{c}p & cr\\\\\naq & as & cq & cs\\\\\n\\textcolor{red}{b}p & br & \\textcolor{red}{d}p & dr\\\\\nbq & bs & dq & ds\n\\end{pmatrix} \\, ,$$ where we’ve colored one copy of BB red for illustration. See the Notebook for more examples of Kronecker products of matrices (including some with pictures rather than numbers!).\nAbove, we saw that f(A)=A2f(A)=A^{2} at A=(prqs)A=\\begin{pmatrix}p & r\\\\\nq & s\n\\end{pmatrix} could be thought of as an equivalent function f̃(vecA)\\tilde{f}(\\operatorname{vec}A) mapping column vectors of 4 inputs to 4 outputs (ℝ4↦ℝ4\\mathbb{R}^{4}\\mapsto\\mathbb{R}^{4}), with a 4×44\\times4 Jacobian that we (or the computer) laboriously computed as 16 element-by-element partial derivatives. It turns out that this result can be obtained much more elegantly once we have a better understanding of Kronecker products. We will find that the 4×44\\times4 “vectorized” Jacobian is simply $$\\tilde{f}'=\\Id_{2}\\otimes A+A^{T}\\otimes\\Id_{2}\\,,$$ where $\\Id_{2}$ is the 2×22\\times2 identity matrix. That is, the matrix linear operator f′(A)[dA]=dAA+AdAf'(A)[dA]=dA\\,A+A\\,dA is equivalent, after vectorization, to:\n$$\\operatorname{vec}\\underbrace{f'(A)[dA]}_{dA\\,A+A\\,dA}=\\underbrace{(\\Id_{2}\\otimes A+A^{T}\\otimes\\Id_{2})}_{\\tilde{f}'}\\operatorname{vec}dA=\\underbrace{\\begin{pmatrix}2p & r & q & 0\\\\\nq & p+s & 0 & q\\\\\nr & 0 & p+s & r\\\\\n0 & r & q & 2s\n\\end{pmatrix}}_{\\tilde{f}'}\\underbrace{\\begin{pmatrix}dp\\\\\ndq\\\\\ndr\\\\\nds\n\\end{pmatrix}}_{\\operatorname{vec}dA}.$$ In order to understand why this is the case, however, we must first build up some understanding of the algebra of Kronecker products. To start with, a good exercise is to convince yourself of a few simpler properties of Kronecker products:\n\nFrom the definition of the Kronecker product, derive the following identities:\n\n(A⊗B)T=AT⊗BT(A\\otimes B)^{T}=A^{T}\\otimes B^{T}.\n(A⊗B)(C⊗D)=(AC)⊗(BD)(A\\otimes B)(C\\otimes D)=(AC)\\otimes(BD).\n(A⊗B)−1=A−1⊗B−1(A\\otimes B)^{-1}=A^{-1}\\otimes B^{-1}. (Follows from property 2.)\nA⊗BA\\otimes B is orthogonal (its transpose is its inverse) if AA and BB are orthogonal. (From properties 1 & 3.)\ndet(A⊗B)=det(A)mdet(B)n\\det(A\\otimes B)=\\det(A)^{m}\\det(B)^{n}, where $A\\in\\R^{n,n}$ and $B\\in\\R^{m,m}$.\n$\\tr(A\\otimes B)=(\\tr A)(\\tr B)$.\nGiven eigenvectors/values Au=λuAu=\\lambda u and Bv=μvBv=\\mu v of AA and BB, then λμ\\lambda\\mu is an eigenvalue of A⊗BA\\otimes B with eigenvector u⊗vu\\otimes v. (Also, since u⊗v=vecXu\\otimes v=\\operatorname{vec}X where X=vuTX=vu^{T}, you can relate this via Prop. [prop5000] below to the identity BXAT=Bv(Au)T=λμXBXA^{T}=Bv(Au)^T=\\lambda\\mu X.)\n\n\n\nKey Kronecker-product identity\nIn order to convert linear operations like AX+XAAX+XA into Kronecker products via vectorization, the key identity is:\n\n Given (compatibly sized) matrices A,B,CA,B,C, we have (A⊗B)vec(C)=vec(BCAT).(A\\otimes B)\\operatorname{vec}(C)=\\operatorname{vec}(BCA^{T}). We can thus view A⊗BA\\otimes B as a vectorized equivalent of the linear operation C↦BCATC\\mapsto BCA^{T}. We are tempted to introduce a parallel notation (A⊗B)[C]=BCAT(A\\otimes B)[C]=BCA^{T} for the “non-vectorized” version of this operation, although this notation is not standard.\nOne possible mnemonic for this identity is that the BB is just to the left of the CC while the AA “circles around” to the right and gets transposed.\n\nWhere does this identity come from? We can break it into simpler pieces by first considering the cases where either AA or BB is an identity matrix $\\Id$ (of the appropriate size). To start with, suppose that $A=\\Id$, so that BCAT=BCBCA^{T}=BC. What is vec(BC)\\operatorname{vec}(BC)? If we let c→1,c→2,…\\vec{c}_{1},\\vec{c}_{2},\\ldots denote the columns of CC, then recall that BCBC simply multiples BB on the left with each of the columns of CC: BC=B(c→1c→2⋯)=(Bc→1Bc→2⋯)⇒vec(BC)=(Bc→1Bc→2⋮).BC=B\\left(\\begin{array}{ccc}\n\\vec{c}_{1} & \\vec{c}_{2} & \\cdots\\end{array}\\right)=\\left(\\begin{array}{ccc}\nB\\vec{c}_{1} & B\\vec{c}_{2} & \\cdots\\end{array}\\right)\\Longrightarrow\\operatorname{vec}(BC)=\\left(\\begin{array}{c}\nB\\vec{c}_{1}\\\\\nB\\vec{c}_{2}\\\\\n\\vdots\n\\end{array}\\right). Now, how can we get this vec(BC)\\operatorname{vec}(BC) vector as something multiplying vecC\\operatorname{vec}C? It should be immediately apparent that $$\\operatorname{vec}(BC)=\\left(\\begin{array}{c}\nB\\vec{c}_{1}\\\\\nB\\vec{c}_{2}\\\\\n\\vdots\n\\end{array}\\right)=\\underbrace{\\left(\\begin{array}{ccc}\nB\\\\\n& B\\\\\n&  & \\ddots\n\\end{array}\\right)}_{\\Id\\otimes B}\\underbrace{\\left(\\begin{array}{c}\n\\vec{c}_{1}\\\\\n\\vec{c}_{2}\\\\\n\\vdots\n\\end{array}\\right)}_{\\operatorname{vec}C},$$ but this matrix is exactly the Kronecker product I⊗BI\\otimes B! Hence, we have derived that $$(\\Id\\otimes B)\\operatorname{vec}C=\\operatorname{vec}(BC).$$ What about the ATA^{T} term? This is a little trickier, but again let’s simplify to the case where $B=\\Id$, in which case BCAT=CATBCA^{T}=CA^{T}. To vectorize this, we need to look at the columns of CATCA^{T}. What is the first column of CATCA^{T}? It is a linear combination of the columns of CC whose coefficients are given by the first column of ATA^{T} (= first row of AA): column 1 of CAT=∑ja1jc→j.\\text{column 1 of }CA^{T}=\\sum_{j}a_{1j}\\vec{c}_{j}\\:. Similarly for column 2, etc, and we then “stack” these columns to get vec(CAT)\\operatorname{vec}(CA^{T}). But this is exactly the formula for multipling a matrix AA by a vector, if the “elements” of the vector were the columns c→j\\vec{c}_{j}. Written out explicitly, this becomes: $$\\operatorname{vec}(CA^{T})=\\left(\\begin{array}{c}\n\\sum_{j}a_{1j}\\vec{c}_{j}\\\\\n\\sum_{j}a_{2j}\\vec{c}_{j}\\\\\n\\vdots\n\\end{array}\\right)=\\underbrace{\\left(\\begin{array}{ccc}\na_{11}\\Id & a_{12}\\Id & \\cdots\\\\\na_{21}\\Id & a_{22}\\Id & \\cdots\\\\\n\\vdots & \\vdots & \\ddots\n\\end{array}\\right)}_{A\\otimes\\Id}\\underbrace{\\left(\\begin{array}{c}\n\\vec{c}_{1}\\\\\n\\vec{c}_{2}\\\\\n\\vdots\n\\end{array}\\right)}_{\\operatorname{vec}C},$$ and hence we have derived $$(A\\otimes\\Id)\\operatorname{vec}C=\\operatorname{vec}(CA^{T}).$$ The full identity (A⊗B)vec(C)=vec(BCAT)(A\\otimes B)\\operatorname{vec}(C)=\\operatorname{vec}(BCA^{T}) can then be obtained by straightforwardly combining these two derivations: replace CATCA^T with BCATBCA^T in the second derivation, which replaces c→j\\vec{c}_j with Bc→jB\\vec{c}_j and hence $\\Id$ with BB.\n\n\nThe Jacobian in Kronecker-product notation\nSo now we want to use Prop. [prop5000] to calculate the Jacobian of f(A)=A2f(A)=A^{2} in terms of the Kronecker product. Let dAdA be our CC in Prop. [prop5000]. We can now immediately see that $$\\operatorname{vec}(A\\,dA+dA\\,A)=\\underbrace{(\\Id\\otimes A+A^{T}\\otimes\\Id)}_{\\mbox{Jacobian }\\tilde{f}'(\\operatorname{vec}A)}\\operatorname{vec}(dA) \\, ,$$ where $\\Id$ is the identity matrix of the same size as AA. We can also write this in our “non-vectorized” linear-operator notation: $$A\\,dA+dA\\,A=(\\Id\\otimes A+A^{T}\\otimes\\Id)[dA] \\, .$$ In the 2×22\\times2 example, these Kronecker products can be computed explicitly: $$\\begin{aligned}\n\\underbrace{\\begin{pmatrix}1 & \\\\\n& 1\n\\end{pmatrix}}_{\\Id}\\otimes\\underbrace{\\begin{pmatrix}p & r\\\\\nq & s\n\\end{pmatrix}}_{A}+\\underbrace{\\begin{pmatrix}p & q\\\\\nr & s\n\\end{pmatrix}}_{A^{T}}\\otimes\\underbrace{\\begin{pmatrix}1 & \\\\\n& 1\n\\end{pmatrix}}_{\\Id} & =\\underbrace{\\left(\\begin{array}{cccc}\np & r &  & \\\\\nq & s &  & \\\\\n&  & p & r\\\\\n&  & q & s\n\\end{array}\\right)}_{\\Id\\otimes A}+\\underbrace{\\left(\\begin{array}{cccc}\np &  & q & \\\\\n& p &  & q\\\\\nr &  & s & \\\\\n& r &  & s\n\\end{array}\\right)}_{A^{T}\\otimes\\Id}\\\\\n& =\\left(\\begin{array}{cccc}\n2p & r & q & 0\\\\\nq & p+s & 0 & q\\\\\nr & 0 & p+s & r\\\\\n0 & r & q & 2s\n\\end{array}\\right)=\\tilde{f}'\\,,\n\\end{aligned}$$ which exactly matches our laboriously computed Jacobian f̃′\\tilde{f}' from earlier!\n\nFor the matrix-cube function A3A^{3}, where AA is an m×mm\\times m square matrix, compute the m2×m2m^{2}\\times m^{2} Jacobian of the vectorized function vec(A3)\\operatorname{vec}(A^{3}).\n\nLet’s use the same trick for the matrix-cube function. Sure, we could laboriously compute the Jacobian via element-by-element partial derivatives (which is done nicely by symbolic computing in the notebook), but it’s much easier and more elegant to use Kronecker products. Recall that our “non-vectorized” matrix-calculus derivative is the linear operator: (A3)′[dA]=dAA2+AdAA+A2dA,(A^{3})'[dA]=dA\\,A^{2}+A\\,dA\\,A+A^{2}\\,dA, which now vectorizes by three applications of the Kronecker identity: $$\\operatorname{vec}(dA\\,A^{2}+A\\,dA\\,A+A^{2}\\,dA)=\\underbrace{\\left((A^{2})^{T}\\otimes\\Id+A^{T}\\otimes A+\\Id\\otimes A^{2}\\right)}_{\\text{vectorized Jacobian}}\\operatorname{vec}(dX)\\,.$$ You could go on to find the Jacobians of A4A^{4}, A5A^{5}, and so on, or any linear combination of matrix powers. Indeed, you could imagine applying a similar process to the Taylor series of any (analytic) matrix function f(A)f(A), but it starts to become awkward. Later on (and in homework), we will discuss more elegant ways to differentiate other matrix functions, not as vectorized Jacobians but as linear operators on matrices.\n\n\nThe computational cost of Kronecker products\nOne must be cautious about using Kronecker products as a computational tool, rather than as more of a conceptual tool, because they can easily cause the computational cost of matrix problems to explode far beyond what is necessary.\nSuppose that AA, BB, and CC are all m×mm\\times m matrices. The cost of multiplying two m×mm\\times m matrices (by the usual methods) scales proportional to ∼m3\\sim m^{3}, what the computer scientists call Θ(m3)\\Theta(m^{3}) “complexity.” Hence, the cost of the linear operation C↦BCATC\\mapsto BCA^{T} scales as ∼m3\\sim m^{3} (two m×mm\\times m multiplications). However, if we instead compute the same answer via vec(BCAT)=(A⊗B)vecC\\operatorname{vec}(BCA^{T})=(A\\otimes B)\\operatorname{vec}C, then we must:\n\nForm the m2×m2m^{2}\\times m^{2} matrix A⊗BA\\otimes B. This requires m4m^{4} multiplications (all entries of AA by all entries of BB), and ∼m4\\sim m^{4} memory storage. (Compare to ∼m2\\sim m^{2} memory to store AA or BB. If mm is 1000, this is a million times more storage, terabytes instead of megabytes!)\nMultiply A⊗BA\\otimes B by the vector vecC\\operatorname{vec}C of m2m^{2} entries. Multiplying an n×nn\\times n matrix by a vector requires ∼n2\\sim n^{2} operations, and here n=m2n=m^{2}, so this is again ∼m4\\sim m^{4} arithmetic operations.\n\nSo, instead of ∼m3\\sim m^{3} operations and ∼m2\\sim m^{2} storage to compute BCATBCA^{T}, using (A⊗B)vecC(A\\otimes B)\\operatorname{vec}C requires ∼m4\\sim m^{4} operations and ∼m4\\sim m^{4} storage, vastly worse! Essentially, this is because A⊗BA\\otimes B has a lot of structure that we are not exploiting (it is a very special m2×m2m^{2}\\times m^{2} matrix).\nThere are many examples of this nature. Another famous one involves solving the linear matrix equations AX+XB=CAX+XB=C for an unknown matrix XX, given A,B,CA,B,C, where all of these are m×mm\\times m matrices. This is called a “Sylvester equation.” These are linear equations in our unknown XX, and we can convert them to an ordinary system of m2m^{2} linear equations by Kronecker products: $$\\operatorname{vec}(AX+XB)=(\\Id\\otimes A+B^{T}\\otimes\\Id)\\operatorname{vec}X=\\operatorname{vec}C,$$ which you can then solve for the m2m^{2} unknowns vecX\\operatorname{vec}X using Gaussian elimination. But the cost of solving an m2×m2m^{2}\\times m^{2} system of equations by Gaussian elimination is ∼(m2)3=m6\\sim (m^2)^3 = m^{6}. It turns out, however, that there are clever algorithms to solve AX+XB=CAX+XB=C in only ∼m3\\sim m^{3} operations (with ∼m2\\sim m^{2} memory)—for m=1000m=1000, this saves a factor of ∼m3=109\\sim m^3 = {10}^9 (a billion) in computational effort.\n(Kronecker products can be a more practical computational tool for sparse matrices: matrices that are mostly zero, e.g. having only a few nonzero entries per row. That’s because the Kronecker product of two sparse matrices is also sparse, avoiding the huge storage requirements for Kronecker products of non-sparse “dense” matrices. This can be a convenient way to assemble large sparse systems of equations for things like multidimensional PDEs.)"
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#sec:approximation",
    "href": "matrix-calculus/en-US/index.html#sec:approximation",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Why compute derivatives approximately instead of exactly?",
    "text": "Why compute derivatives approximately instead of exactly?\nWorking out derivatives by hand is a notoriously error-prone procedure for complicated functions. Even if every individual step is straightforward, there are so many opportunities to make a mistake, either in the derivation or in its implementation on a computer. Whenever you implement a derivatives, you should always double-check for mistakes by comparing it to an independent calculation. The simplest such check is a finite-difference approximation, in which we estimate the derivative(s) by comparing f(x)f(x) and f(x+δx)f(x + \\delta x) for one or more “finite” (non-infinitesimal) perturbations δx\\delta x.\nThere are many finite-difference techniques at varying levels of sophistication, as we will discuss below. They all incur an intrinsic truncation error due to the fact that δx\\delta x is not infinitesimal. (And we will also see that you can’t make δx\\delta x too small, either, or roundoff errors start exploding!) Moreover, finite differences become expensive for higher-dimensional xx (in which you need a separate finite difference for each input dimension to compute the full Jacobian). This makes them an approach of last resort for computing derivatives accurately. On the other hand, they are the first method you generally employ in order to check derivatives: if you have a bug in your analytical derivative calculation, usually the answer is completely wrong, so even a crude finite-difference approximation for a single small δx\\delta x (chosen at random in higher dimensions) will typically reveal the problem.\nAnother alternative is automatic differentiation (AD), software/compilers perform analytical derivatives for you. This is extremely reliable and, with modern AD software, can be very efficient. Unfortunately, there is still lots of code, e.g. code calling external libraries in other languages, that AD tools can’t comprehend. And there are other cases where AD is inefficient, typically because it misses some mathematical structure of the problem. Even in such cases, you can often fix AD by defining the derivative of one small piece of your program by hand,3 which is much easier than differentiating the whole thing. In such cases, you still will typically want a finite-difference check to ensure that you have not made a mistake.\nIt turns out that finite-difference approximations are a surprisingly complicated subject, with rich connections to many areas of numerical analysis; in this lecture we will just scratch the surface."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#finite-difference-approximations-easy-version",
    "href": "matrix-calculus/en-US/index.html#finite-difference-approximations-easy-version",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Finite-Difference Approximations: Easy Version",
    "text": "Finite-Difference Approximations: Easy Version\nThe simplest way to check a derivative is to recall that the definition of a differential: df=f(x+dx)−f(x)=f′(x)dxdf = f(x+dx) - f(x) = f'(x) dx came from dropping higher-order terms from a small but finite difference: δf=f(x+δx)−f(x)=f′(x)δx+o(‖δx‖).\\delta f = f(x+\\delta x) - f(x) = f'(x) \\delta x + o(\\Vert \\delta x \\Vert) \\, . So, we can just compare the finite difference f(x+δx)−f(x)\\boxed{f(x+\\delta x) - f(x)} to our (directional) derivative operator f′(x)δxf'(x) \\delta x (i.e. the derivative in the direction δx\\delta x). f(x+δx)−f(x)f(x+\\delta x) - f(x) is also called a forward difference approximation. The antonym of a forward difference is a backward difference approximation f(x)−f(x−δx)≈f′(x)δxf(x) - f(x - \\delta x) \\approx f'(x) \\delta x. If you just want to compute a derivative, there is not much practical distinction between forward and backward differences. The distinction becomes more important when discretizing (approximating) differential equations. We’ll look at other possibilities below.\n\nRemark. Note that this definition of forward and backward difference is not the same as forward- and backward-mode differentiation—these are unrelated concepts.\n\nIf xx is a scalar, we can also divide both sides by δx\\delta x to get an approximation for f′(x)f'(x) instead of for dfdf: f′(x)≈f(x+δx)−f(x)δx+(higher-order corrections).f'(x) \\approx \\frac{f(x+\\delta x) - f(x)}{\\delta x} + \\text{(higher-order corrections)} \\, . This is a more common way to write the forward-difference approximation, but it only works for scalar xx, whereas in this class we want to think of xx as perhaps belonging to some other vector space.\nFinite-difference approximations come in many forms, but they are generally a last resort in cases where it’s too much effort to work out an analytical derivative and AD fails. But they are also useful to check your analytical derivatives and to quickly explore."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#example-matrix-squaring",
    "href": "matrix-calculus/en-US/index.html#example-matrix-squaring",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Example: Matrix squaring",
    "text": "Example: Matrix squaring\nLet’s try the finite-difference approximation for the square function f(A)=A2f(A) = A^2, where here AA is a square matrix in $\\R^{m,m}$. By hand, we obtain the product rule df=AdA+dAA,df = A \\,dA + dA \\, A, i.e. f′(A)f'(A) is the linear operator f′(A)[δA]=AδA+δAA.\\boxed{f'(A)[\\delta A] = A \\,\\delta A + \\delta A \\,A.} This is not equal to 2AδA2A \\,\\delta A because in general AA and δA\\delta A do not commute. So let’s check this difference against a finite difference. We’ll try it for a random input A and a random small perturbation δA\\delta A.\nUsing a random matrix AA, let dA=A⋅10−8dA = A \\cdot 10^{-8}. Then, you can compare f(A+dA)−f(A)f(A+dA) - f(A) to AdA+dAAA \\,dA + dA \\, A. If the matrix you chose was really random, you would get that the approximation minus the exact equality from the product rule has entries with order of magnitude around 10−16!10^{-16}! However, compared to 2AdA2A dA, you’d obtain entries of order 10−810^{-8}.\nTo be more quantitative, we might compute that \"norm\" ‖approx−exact‖\\lVert \\text{approx} - \\text{exact} \\rVert which we want to be small. But small compared to what? The natural answer is small compared to the correct answer. This is called the relative error (or \"fractional error\") and is computed via relative error=‖approx−exact‖‖exact‖.\\text{relative error} = \\frac{\\lVert \\text{approx} - \\text{exact} \\rVert}{\\lVert \\text{exact} \\rVert}. Here, ‖⋅‖\\lVert \\cdot \\rVert is a norm, like the length of a vector. This allows us to understand the size of the error in the finite difference approximation, i.e. it allows us to answer how accurate this approximation is (recall Sec. 4.1).\nSo, as above, you can compute that the relative error between the approximation and the exact answer is about 10−810^{-8}, where as the relative error between 2AdA2A dA and the exact answer is about 10010^{0}. This shows that our exact answer is likely correct! Getting a good match up between a random input and small displacement isn’t a proof of correctness of course, but it is always a good thing to check. This kind of randomized comparison will almost always catch major bugs where you have calculated the symbolic derivative incorrectly, like in our 2AdA2A dA example.\n\nNote that the norm of a matrix that we are using, computed by norm(A) in Julia, is just the direct analogue of the familiar Euclidean norm for the case of vectors. It is simply the square root of the sum of the matrix entries squared: $$\\lVert A \\rVert := \\sqrt{\\sum_{i,j} |A_{ij}|^2} = \\sqrt{\\tr (A^T A)} \\, .$$ This is called the Frobenius norm."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#accuracy-of-finite-differences",
    "href": "matrix-calculus/en-US/index.html#accuracy-of-finite-differences",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Accuracy of Finite Differences",
    "text": "Accuracy of Finite Differences\nNow how accurate is our finite-difference approximation above? How should we choose the size of δx\\delta x?\nLet’s again consider the example f(A)=A2f(A) = A^2, and plot the relative error as a function of ‖δA‖\\lVert \\delta A \\rVert. This plot will be done logarithmically (on a log–log scale) so that we can see power-law relationships as straight lines.\n\n\n\nForward-difference accuracy for f(A)=A2f(A) = A^2, showing the relative error in δf=f(A+δA)−f(A)\\delta f = f(A + \\delta A) - f(A) versus the linearization f′(A)δAf'(A) \\delta A, as a function of the magnitude ‖δA‖\\Vert \\delta A\\Vert. AA is a 4×44 \\times 4 matrix with unit-variance Gaussian random entries, and δA\\delta A is similarly a unit-variance Gaussian random perturbation scaled by a factor ss ranging from 11 to 10−1610^{-16}.\n\n\nWe notice two main features as we decrease δA\\delta A:\n\nThe relative error at first decreases linearly with ‖δA‖\\lVert \\delta A\\rVert. This is called first-order accuracy. Why?\nWhen δA\\delta A gets too small, the error increases. Why?"
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#order-of-accuracy",
    "href": "matrix-calculus/en-US/index.html#order-of-accuracy",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Order of accuracy",
    "text": "Order of accuracy\nThe truncation error is the inaccuracy arising from the fact that the input perturbation δx\\delta x is not infinitesimal: we are computing a difference, not a derivative. If the truncation error in the derivative scales proportional ‖δx‖n\\Vert \\delta x \\Vert^n, we call the approximation n-th order accurate. For forward differences, here, the order is n=1. Why?\nFor any f(x)f(x) with a nonzero second derivative (think of the Taylor series), we have f(x+δx)=f(x)+f′(x)δx+(terms proportional to ‖δx‖2)+o(‖δx‖2)⏟i.e. higher-order termsf(x + \\delta x) = f(x) + f'(x) \\delta x + (\\text{terms proportional to }\\Vert \\delta x \\Vert^2) + \\underbrace{o(\\Vert \\delta x \\Vert^2)}_\\text{i.e. higher-order terms} That is, the terms we dropped in our forward-difference approximations are proportional to ‖δx‖2\\Vert \\delta x\\Vert^2. But that means that the relative error is linear: relative error=‖f(x+δx)−f(x)−f′(x)δx‖‖f′(x)δx‖=(terms proportional to ‖δx‖2)+o(‖δx‖2)proportional to ‖δx‖=(terms proportional to ‖δx‖)+o(‖δx‖)\\begin{aligned}\n    \\text{relative error} &= \\frac{\\Vert f(x+\\delta x) - f(x) - f'(x) \\delta x \\Vert}{\\Vert f'(x) \\delta x \\Vert} \\\\\n    &= \\frac{(\\text{terms proportional to }\\Vert \\delta x \\Vert^2) + o(\\Vert \\delta x \\Vert^2)}{\\text{proportional to }\\Vert \\delta x \\Vert} = (\\text{terms proportional to }\\Vert \\delta x \\Vert) + o(\\Vert \\delta x \\Vert)\n\\end{aligned} This is first-order accuracy. Truncation error in a finite-difference approximation is the inherent error in the formula for non-infinitesimal δx\\delta x. Does that mean we should just make δx\\delta x as small as we possibly can?"
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#roundoff-error",
    "href": "matrix-calculus/en-US/index.html#roundoff-error",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Roundoff error",
    "text": "Roundoff error\nThe reason why the error increased for very small δA\\delta A was due to roundoff errors. The computer only stores a finite number of significant digits (about 15 decimal digits) for each real number and rounds off the rest on each operation — this is called floating-point arithmetic. If δx\\delta x is too small, then the difference f(x+δx)−f(x)f(x+\\delta x) - f(x) gets rounded off to zero (some or all of the significant digits cancel). This is called catastrophic cancellation.\nFloating-point arithmetic is much like scientific notation *.*****×10e{*}.{*}{*}{*}{*}{*} \\times 10^e: a finite-precision coefficient *.*****{*}.{*}{*}{*}{*}{*} scaled by a power of 1010 (or, on a computer, a power of 22). The number of digits in the coefficient (the “significant digits”) is the “precision,” which in the usual 64-bit floating-point arithmetic is charactized by a quantity ϵ=2−52≈2.22×10−16\\epsilon = 2^{-52} \\approx 2.22 \\times 10^{-16}, called the machine epsilon. When an arbitrary real number y∈ℝy \\in \\mathbb{R} is rounded to the closest floating-point value ỹ\\tilde{y}, the roundoff error is bounded by |ỹ−y|≤ϵ|y||\\tilde{y} - y| \\le \\epsilon |y|. Equivalently, the computer keeps only about 15–16 ≈−log10ϵ\\approx -\\log_{10} \\epsilon decimal digits, or really 53=1−log2ϵ53 = 1 - \\log_2 \\epsilon binary digits, for each number.\nIn our finite-difference example, for ‖δA‖/‖A‖\\Vert \\delta A \\Vert / \\Vert A \\Vert of roughly 10−8≈ϵ‖A‖10^{-8} \\approx \\sqrt{\\epsilon} \\lVert A\\rVert or larger, the approximation for f′(A)f'(A) is dominated by the truncation error, but if we go smaller than that the relative error starts increasing due to roundoff. Experience has shown that ‖δx‖≈ϵ‖x‖\\Vert \\delta x \\Vert \\approx \\sqrt{\\epsilon} \\Vert x \\Vert is often a good rule of thumb—about half the significant digits is the most that is reasonably safe to rely on—but the precise crossover point of minimum error depends on the function ff and the finite-difference method. But, like all rules of thumb, this may not always be completely reliable."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#other-finite-difference-methods",
    "href": "matrix-calculus/en-US/index.html#other-finite-difference-methods",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Other finite-difference methods",
    "text": "Other finite-difference methods\nThere are more sophisticated finite-difference methods, such as Richardson extrapolation, which consider a sequence of progressively smaller δx\\delta x values in order to adaptively determine the best possible estimate for f′f' (extrapolating to δx→0\\delta x \\to 0 using progressively higher degree polynomials). One can also use higher-order difference formulas than the simple forward-difference method here, so that the truncation error decreases faster than than linearly with δx\\delta x. The most famous higher-order formula is the “centered difference” f′(x)δx≈[f(x+δx)−f(x−δx)]/2f'(x) \\delta x \\approx [f(x + \\delta x) - f(x - \\delta x)]/2, which has second-order accuracy (relative truncation error proportional to ‖δx‖2\\Vert \\delta x\\Vert^2).\nHigher-dimensional inputs xx pose a fundamental computational challenge for finite-difference techniques, because if you want to know what happens for every possible direction δx\\delta x then you need many finite differences: one for each dimension of δx\\delta x. For example, suppose x∈ℝnx \\in \\mathbb{R}^n and f(x)∈ℝf(x) \\in \\mathbb{R}, so that you are computing ∇f∈ℝn\\nabla f \\in \\mathbb{R}^n; if you want to know the whole gradient, you need nn separate finite differences. The net result is that finite differences in higher dimensions are expensive, quickly becoming impractical for high-dimensional optimization (e.g. neural networks) where nn might be huge. On the other hand, if you are just using finite differences as a check for bugs in your code, it is usually sufficient to compare f(x+δx)−f(x)f(x+\\delta x) - f(x) to f′(x)[δx]f'(x)[\\delta x] in a few random directions, i.e. for a few random small δx\\delta x."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#a-simple-matrix-dot-product-and-norm",
    "href": "matrix-calculus/en-US/index.html#a-simple-matrix-dot-product-and-norm",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "A Simple Matrix Dot Product and Norm",
    "text": "A Simple Matrix Dot Product and Norm\nRecall that for scalar-valued functions $f(x) \\in \\R$ with vector inputs $x\\in \\R^n$ (i.e. nn-component “column vectors\") we have that $$df = f(x + dx) - f(x) = f'(x) [dx] \\in \\R.$$ Therefore, f′(x)f'(x) is a linear operator taking in the vector dxdx in and giving a scalar value out. Another way to view this is that f′(x)f'(x) is the row vector4 (∇f)T(\\nabla f)^T. Under this viewpoint, it follows that dfdf is the dot product (or”inner product”): df=∇f⋅dxdf = \\nabla f \\cdot dx\nWe can generalize this to any vector space VV with inner products! Given x∈Vx\\in V, and a scalar-valued function ff, we obtain the linear operator $f'(x) [dx] \\in \\R$, called a “linear form.” In order to define the gradient ∇f\\nabla f, we need an inner product for VV, the vector-space generalization of the familiar dot product!\nGiven x,y∈Vx,y \\in V, the inner product ⟨x,y⟩\\langle x, y \\rangle is a map (⋅\\cdot) such that $\\langle x, y \\rangle \\in \\R$. This is also commonly denoted x⋅yx \\cdot y or ⟨x∣y⟩\\langle x \\mid y \\rangle. More technically, an inner product is a map that is\n\nSymmetric: i.e. ⟨x,y⟩=⟨y,x⟩\\langle x, y \\rangle = \\langle y, x \\rangle (or conjugate-symmetric,5 ⟨x,y⟩=⟨y,x⟩¯\\langle x, y \\rangle = \\overline{\\langle y, x \\rangle}, if we were using complex numbers),\nLinear: i.e. ⟨x,αy+βz⟩=α⟨x,y⟩+β⟨x,z⟩\\langle x, \\alpha y + \\beta z\\rangle = \\alpha \\langle x, y \\rangle + \\beta \\langle x, z \\rangle, and\nNon-negative: i.e. ⟨x,x⟩:=‖x‖2≥0\\langle x, x \\rangle := \\lVert x \\rVert^2 \\geq 0, and =0=0 if and only if x=0x = 0.\n\nNote that the combination of the first two properties means that it must also be linear in the left vector (or conjugate-linear, if we were using complex numbers). Another useful consequence of these three properties, which is a bit trickier to derive, is the Cauchy–Schwarz inequality |⟨x,y⟩|≤‖x‖‖y‖|\\langle x, y \\rangle| \\le \\Vert x \\Vert \\, \\Vert y \\Vert.\n\nA (complete) vector space with an inner product is called a Hilbert space. (The technical requirement of “completeness” essentially means that you can take limits in the space, and is important for rigorous proofs.6)\n\nOnce we have a Hilbert space, we can define the gradient for scalar-valued functions. Given x∈Vx\\in V a Hilbert space, and f(x)f(x) scalar, then we have the linear form $f'(x) [dx] \\in \\R$. Then, under these assumptions, there is a theorem known as the “Riesz representation theorem” stating that any linear form (including f′f') must be an inner product with something: f′(x)[dx]=⟨(some vector)⏟gradient ∇f|x,dx⟩=df.f'(x) [dx] = \\big\\langle \\underbrace{\\text{(some vector)}}_{\\text{gradient } \\nabla f\\bigr|_x} , dx \\big\\rangle = df. That is, the gradient ∇f\\nabla f is defined as the thing you take the inner product of dxdx with to get dfdf. Note that ∇f\\nabla f always has the “same shape” as xx.\nThe first few examples we look at involve the usual Hilbert space $V = \\R^n$ with different inner products.\n\nGiven $V = \\R^n$ with nn-column vectors, we have the familiar Euclidean dot product ⟨x,y⟩=xTy\\langle x, y \\rangle = x^T y. This leads to the usual ∇f\\nabla f.\n\n\nWe can have different inner products on $\\R^n$. For instance, ⟨x,y⟩W=w1x1y1+w2x2y2+…wnxnyn=xT(w1⋱wn)⏟Wy\\langle x, y\\rangle_W = w_1 x_1 y_1 + w_2 x_2 y_2 + \\dots w_n x_n y_n = x^T \\underbrace{\\begin{pmatrix}\n        w_1 & & \\\\\n         & \\ddots & \\\\\n         & & w_n\n    \\end{pmatrix}}_{W} y for weights w1,…,wn&gt;0w_1,\\dots, w_n &gt;0.\nMore generally we can define a weighted dot product ⟨x,y⟩W=xTWy\\langle x, y\\rangle_W= x^T W y for any symmetric-positive-definite matrix WW (W=WTW = W^T and WW is positive definite, which is sufficient for this to be a valid inner product).\nIf we change the definition of the inner product, then we change the definition of the gradient! For example, with f(x)=xTAxf(x) = x^T A x we previously found that df=xT(A+AT)dxdf = x^T (A + A^T) dx. With the ordinary Euclidean inner product, this gave a gradient ∇f=(A+AT)x\\nabla f = (A+A^T)x. However, if we use the weighted inner product xTWyx^T W y, then we would obtain a different “gradient” ∇(W)f=W−1(A+AT)x\\nabla^{(W)} f = W^{-1} (A+A^T)x so that df=⟨∇(W)f,dx⟩df = \\langle \\nabla^{(W)}  f , dx \\rangle.\nIn these notes, we will employ the Euclidean inner product for x∈ℝnx \\in \\mathbb{R}^n, and hence the usual ∇f\\nabla f, unless noted otherwise. However, weighted inner products are useful in lots of cases, especially when the components of xx have different scales/units.\n\nWe can also consider the space of m×nm\\times n matrices $V = \\R^{m \\times n}$. There, is of course, a vector-space isomorphism from $V \\ni A \\to \\mathrm{vec}(A) \\in \\R^{mn}$. Thus, in this space we have the analogue of the familiar (“Frobenius\") Euclidean inner product, which is convenient to rewrite in terms of matrix operations via the trace:\n\nThe Frobenius inner product of two m×nm \\times n matrices AA and BB is: $$\\langle A, B \\rangle_F = \\sum_{ij} A_{ij} B_{ij} = \\mathrm{vec}(A)^T \\mathrm{vec}(B) = \\tr(A^T B) \\, .$$ Given this inner product, we also have the corresponding Frobenius norm: $$\\lVert A \\rVert_F = \\sqrt{\\langle A,A \\rangle_F} = \\sqrt{\\tr(A^TA)} = \\lVert \\mathrm{vec} A\\rVert = \\sqrt{\\sum_{i,j} |A_{ij}|^2} \\, .$$ Using this, we can now define the gradient of scalar functions with matrix inputs! This will be our default matrix inner product (hence defining our default matrix gradient) in these notes (sometimes dropping the FF subscript).\n\n\nConsider the function $$f(A) = \\lVert A \\rVert_F = \\sqrt{\\tr(A^T A)}.$$ What is dfdf?\n\nFirstly, by the familiar scalar-differentiation chain and power rules we have that $$df = \\frac{1}{2 \\sqrt{\\tr(A^T A)}} d(\\tr A^T A).$$ Then, note that (by linearity of the trace) $$d( \\tr B) = \\tr(B+dB) - \\tr(B) = \\tr(B) + \\tr(dB) - \\tr(B) = \\tr(dB).$$ Hence, $$\\begin{aligned}\n    df &= \\frac{1}{2\\lVert A\\rVert_F} \\tr(d(A^T A)) \\\\\n    &= \\frac{1}{2\\lVert A\\rVert_F} \\tr( dA^T\\, A + A^T\\, dA) \\\\\n    &= \\frac{1}{2 \\lVert A\\rVert_F} (\\tr(dA^T\\, A) + \\tr(A^T\\, dA)) \\\\\n    &= \\frac{1}{\\lVert A\\rVert_F} \\tr(A^T \\, dA) = \\big\\langle \\frac{A}{\\lVert A\\rVert_F} , dA \\big\\rangle.\n\\end{aligned}$$ Here, we used the fact that $\\tr B = \\tr B^T$, and in the last step we connected dfdf with a Frobenius inner product. In other words, ∇f=∇‖A‖F=A‖A‖F.\\nabla f = \\nabla \\lVert A \\rVert_F = \\frac{A}{\\lVert A \\rVert_F}. Note that one obtains exactly the same result for column vectors xx, i.e. ∇‖x‖=x/‖x‖\\nabla \\Vert x\\Vert = x/\\Vert x \\Vert (and in fact this is equivalent via x=vecAx = \\operatorname{vec}A).\nLet’s consider another simple example:\n\nFix some constant $x \\in \\R^m$, $y\\in \\R^n$, and consider the function $f:\\R^{m\\times n} \\to \\R$ given by f(A)=xTAy.f(A) = x^T A y. What is ∇f\\nabla f?\n\nWe have that $$\\begin{aligned}\n    df &= x^T \\,dA \\,y \\\\\n    &= \\tr( x^T \\,dA\\, y) \\\\\n    &= \\tr(y x^T \\, dA) \\\\\n    &= \\big\\langle \\underbrace{x y^T}_{\\nabla f} , \\, dA \\big\\rangle.\n\\end{aligned}$$\nMore generally, for any scalar-valued function f(A)f(A), from the definition of Frobenius inner product it follows that: df=f(A+dA)−f(A)=⟨∇f,dA⟩=∑i,j(∇f)i,jdAi,j,df = f(A+dA)-f(A) = \\langle \\nabla f , \\, dA \\rangle = \\sum_{i,j} (\\nabla f)_{i,j} \\, dA_{i,j} \\, , and hence the components of the gradient are exactly the elementwise derivatives (∇f)i,j=∂f∂Ai,j,(\\nabla f)_{i,j} = \\frac{\\partial f}{\\partial A_{i,j}} \\, , similar to the component-wise definition of the gradient vector from multivariable calculus! But for non-trivial matrix-input functions f(A)f(A) it can be extremely awkward to take the derivative with respect to each entry of AA individually. Using the “holistic” matrix inner-product definition, we will soon be able to compute even more complicated matrix-valued gradients, including ∇(detA)\\nabla (\\det A)!"
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#sec:banach",
    "href": "matrix-calculus/en-US/index.html#sec:banach",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Derivatives, Norms, and Banach spaces",
    "text": "Derivatives, Norms, and Banach spaces\nWe have been using the term “norm” throughout this class, but what technically is a norm? Of course, there are familiar examples such as the Euclidean (“ℓ2\\ell^2”) norm ‖x‖=∑kxk2\\Vert x \\Vert = \\sqrt{\\sum_k x_k^2} for x∈ℝnx\\in \\mathbb{R}^n, but it is useful to consider how this concept generalizes to other vector spaces. It turns out, in fact, that norms are crucial to the definition of a derivative!\nGiven a vector space VV, a norm ‖⋅‖\\lVert \\cdot \\rVert on VV is a map $\\lVert \\cdot \\rVert: V\\to \\R$ satisfying the following three properties:\n\nNon-negative: i.e. ‖v‖≥0\\lVert v \\rVert \\geq 0 and ‖v‖=0⇔v=0\\lVert v \\rVert = 0 \\iff v = 0,\nHomogeneity: ‖αv‖=|α|‖v‖\\lVert \\alpha v \\rVert = |\\alpha |\\lVert v \\rVert for any $\\alpha \\in \\R$, and\nTriangle inequality: ‖u+v‖≤‖u‖+‖v‖\\lVert u + v\\rVert \\leq \\lVert u \\rVert + \\lVert v \\rVert.\n\nA vector space that has a norm is called an normed vector space. Often, mathematicians technically want a slightly more precise type of normed vector space with a less obvious name: a Banach space.\n\nA (complete) vector space with a norm is called a Banach space. (As with Hilbert spaces, “completeness” is a technical requirement for some types of rigorous analysis, essentially allowing you to take limits.)\n\nFor example, given any inner product ⟨u,v⟩\\langle u , v \\rangle, there is a corresponding norm ‖u‖=⟨u,u⟩\\lVert u \\rVert = \\sqrt{\\langle u , u \\rangle}. (Thus, every Hilbert space is also a Banach space.7)\nTo define derivatives, we technically need both the input and the output to be Banach spaces. To see this, recall our formalism f(x+δx)−f(x)=f′(x)[δx]⏟linear+o(δx)⏟smaller.f(x + \\delta x) - f(x) = \\underbrace{f'(x) [\\delta x]}_{\\mbox{linear}} \\; + \\underbrace{o(\\delta x)}_{\\mbox{smaller}}\\, . To precisely define the sense in which the o(δx)o(\\delta x) terms are “smaller” or “higher-order,” we need norms. In particular, the “little-oo” notation o(δx)o(\\delta x) denotes any function such that limδx→0‖o(δx)‖‖δx‖=0,\\lim_{\\delta x\\to 0} \\frac{\\lVert o (\\delta x) \\rVert}{\\lVert \\delta x\\rVert} = 0 \\, , i.e. which goes to zero faster than linearly in δx\\delta x. This requires both the input δx\\delta x and the output (the function) to have norms. This extension of differentiation to arbitrary normed/Banach spaces is sometimes called the Fréchet derivative."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#sec:newton-roots",
    "href": "matrix-calculus/en-US/index.html#sec:newton-roots",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Newton’s Method",
    "text": "Newton’s Method\nOne common application of derivatives is to solve nonlinear equations via linearization.\n\nScalar Functions\nFor instance, suppose we have a scalar function $f: \\R \\to \\R$ and we wanted to solve f(x)=0f(x) = 0 for a root xx. Of course, we could solve such an equation explicitly in simple cases, such as when ff is linear or quadratic, but if the function is something more arbitrary like f(x)=x3−sin(cosx)f(x) = x^3 - \\sin (\\cos x) you might not be able to obtain closed-form solutions. However, there is a nice way to obtain the solution approximately to any accuracy you want, as long if you know approximately where the root is. The method we are talking about is known as Newton’s method, which is really a linear-algebra technique. It takes in the function and a guess for the root, approximates it by a straight line (whose root is easy to find), which is then an approximate root that we can use as a new guess. In particular, the method (depicted in Fig. 4) is as follows:\n\nLinearize f(x)f(x) near some xx using the approximation f(x+δx)≈f(x)+f′(x)δx,f(x + \\delta x) \\approx f(x) + f'(x) \\delta x,\nsolve the linear equation f(x)+f′(x)δx=0⟹δx=−f(x)f′(x)f(x) + f'(x) \\delta x = 0 \\implies \\delta x = -\\frac{f(x)}{f'(x)},\nand then use this to update the value of xx we linearized near—i.e., letting the new xx be xnew=x−δx=x+f(x)f′(x).x_\\text{new} = x - \\delta x = x + \\frac{f(x)}{f'(x)} \\, .\n\nOnce you are close to the root, Newton’s method converges amazingly quickly. As discussed below, it asymptotically doubles the number of correct digits on every step!\nOne may ask what happens when f′(x)f'(x) is not invertible, for instance here if f′(x)=0f'(x) = 0. If this happens, then Newton’s method may break down! See here for examples of when Newton’s method breaks down.\n\n\n\nSingle step of the scalar Newton’s method to solve f(x)=0f(x)=0 for an example nonlinear function f(x)=2cos(x)−x+x2/10f(x) = 2\\cos(x) - x + x^2/10. Given a starting guess (x=2.3x = 2.3 in this example), we use f(x)f(x) and f′(x)f'(x) to form a linear (affine) approximation of ff, and then our next step xnewx_\\mathrm{new} is the root of this approximation. As long as the initial guess is not too far from the root, Newton’s method converges extremely rapidly to the exact root (black dot).\n\n\n\n\nMultidimensional Functions\nWe can generalize Newton’s method to multidimensional functions! Let $f: \\R^n \\to \\R^n$ be a function which takes in a vector and spits out a vector of the same size nn. We can then apply a Newton approach in higher dimensions:\n\nLinearize f(x)f(x) near some xx using the first-derivative approximation f(x+δx)≈f(x)+f′(x)⏟Jacobianδx,f(x + \\delta x) \\approx f(x) + \\underbrace{f'(x)}_\\text{Jacobian} \\delta x,\nsolve the linear equation f(x)+f′(x)δx=0⟹δx=−f′(x)−1⏟inverse Jacobianf(x)f(x) + f'(x) \\delta x = 0 \\implies \\delta x = -\\underbrace{f'(x)^{-1}}_\\text{inverse Jacobian} f(x),\nand then use this to update the value of xx we linearized near—i.e., letting the new xx be xnew=xold−f′(x)−1f(x).x_\\text{new} = x_\\text{old} - f'(x)^{-1}f(x)\\, .\n\nThat’s it! Once we have the Jacobian, we can just solve a linear system on each step. This again converges amazingly fast, doubling the number of digits of accuracy in each step. (This is known as “quadratic convergence.”) However, there is a caveat: we need some starting guess for xx, and the guess needs to be sufficiently close to the root for the algorithm to make reliable progress. (If you start with an initial xx far from a root, Newton’s method can fail to converge and/or it can jump around in intricate and surprising ways—google “Newton fractal” for some fascinating examples.) This is a widely used and very practical application of Jacobians and derivatives!"
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#optimization",
    "href": "matrix-calculus/en-US/index.html#optimization",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Optimization",
    "text": "Optimization\n\nNonlinear Optimization\nA perhaps even more famous application of large-scale differentiation is to nonlinear optimization. Suppose we have a scalar-valued function $f: \\R^n \\to \\R$, and suppose we want to minimize (or maximize) ff. For instance, in machine learning, we could have a big neural network (NN) with a vector xx of a million parameters, and one tries to minimize a “loss” function ff that compares the NN output to the desired results on “training” data. The most basic idea in optimization is to go “downhill” (see diagram) to make ff as small as possible. If we can take the gradient of this function ff, to go “downhill” we consider −∇f- \\nabla f, the direction of steepest descent, as depicted in Fig. 5.\n\n\n\nA steepest-descent algorithm minimizes a function f(x)f(x) by taking successive “downhill” steps in the direction −∇f-\\nabla f. (In the example shown here, we are minimizing a quadratic function in two dimensions x∈ℝ2x \\in \\mathbb{R}^2, performing an exact 1d minimization in the downhill direction for each step.) Steepest-descent algorithms can sometimes “zig-zag” along narrow valleys, slowing convergence (which can be counteracted in more sophisticated algorithms by “momentum” terms, second-derivative information, and so on).\n\n\nThen, even if we have a million parameters, we can evolve all of them simultaneously in the downhill direction. It turns out that calculating all million derivatives costs about the same as evaluating the function at a point once (using reverse-mode/adjoint/left-to-right/backpropagation methods). Ultimately, this makes large-scale optimization practical for training neural nets, optimizing shapes of airplane wings, optimizing portfolios, etc.\nOf course, there are many practical complications that make nonlinear optimization tricky (far more than can be covered in a single lecture, or even in a whole course!), but we give some examples here.\n\nFor instance, even though we can compute the “downhill direction”, how far do we need to step in that direction? (In machine learning, this is sometimes called the “learning rate.”) Often, you want to take “as big of a step as you can” to speed convergence, but you don’t want the step to be too big because ∇f\\nabla f only tells you a local approximation of ff. There are many different ideas of how to determine this:\n\nLine search: using a 1D minimization to determine how far to step.\nA “trust region” bounding the step size (where we trust the derivative-based approximation of ff). There are many techniques to evolve the size of the trust region as optimization progresses.\n\nWe may also need to consider constraints, for instance minimizing f(x)f(x) subject to gk(x)≤0g_k(x) \\leq 0 or hk(x)=0h_k(x)=0, known as inequality/equality constraints. Points xx satisfying the constraints are called “feasible”. One typically uses a combination of ∇f\\nabla f and ∇gk\\nabla g_k to approximate (e.g. linearize) the problem and make progress towards the best feasible point.\nIf you just go straight downhill, you might “zig-zag” along narrow valleys, making convergence very slow. There are a few options to combat this, such as “momentum” terms and conjugate gradients. Even fancier than these techniques, one might estimate second-derivative “Hessian matrices” from a sequence of ∇f\\nabla f values—a famous version of this is known as the BFGS algorithm—and use the Hessian to take approximate Newton steps (for the root ∇f=0\\nabla f = 0). (We’ll return to Hessians in a later lecture.)\nUltimately, there are a lot of techniques and a zoo of competing algorithms that you might need to experiment with to find the best approach for a given problem. (There are many books on optimization algorithms, and even a whole book can only cover a small slice of what is out there!)\n\nSome parting advice: Often the main trick is less about the choice of algorithms than it is about finding the right mathematical formulation of your problem—e.g. what function, what constraints, and what parameters should you be considering—to match your problem to a good algorithm. However, if you have many (≫10\\gg 10) parameters, try hard to use an analytical gradient (not finite differences), computed efficiently in reverse mode.\n\n\nEngineering/Physical Optimization\nThere are many, many applications of optimization besides machine learning (fitting models to data). It is interesting to also consider engineering/physical optimization. (For instance, suppose you want to make an airplane wing that is as strong as possible.) The general outline of such problems is typically:\n\nYou start with some design parameters 𝐩\\mathbf{p}, e.g. describing the geometry, materials, forces, or other degrees of freedom.\nThese 𝐩\\mathbf{p} are then used in some physical model(s), such as solid mechanics, chemical reactions, heat transport, electromagnetism, acoustics, etc. For example, you might have a linear model of the form A(𝐩)x=b(𝐩)A(\\mathbf{p}) x = b(\\mathbf{p}) for some matrix AA (typically very large and sparse).\nThe solution of the physical model is a solution x(𝐩)x(\\mathbf{p}). For example, this could be the mechanical stresses, chemical concentrations, temperatures, electromagnetic fields, etc.\nThe physical solution x(𝐩)x(\\mathbf{p}) is the input into some design objective f(x(𝐩))f(x(\\mathbf{p})) that you want to improve/optimize. For instance, strength, speed power, efficiency, etc.\nTo maximize/minimize f(x(𝐩))f(x(\\mathbf{p})), one uses the gradient ∇𝐩f\\nabla_{\\mathbf{p}}f, computed using reverse-mode/“adjoint” methods, to update the parameters 𝐩\\mathbf{p} and improve the design.\n\nAs a fun example, researchers have even applied “topology optimization” to design a chair, optimizing every voxel of the design—the parameters 𝐩\\mathbf{p} represent the material present (or not) in every voxel, so that the optimization discovers not just an optimal shape but an optimal topology (how materials are connected in space, how many holes there are, and so forth)—to support a given weight with minimal material. To see it in action, watch this chair-optimization video. (People have applied such techniques to much more practical problems as well, from airplane wings to optical communications.)"
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#sec:adjoint-method",
    "href": "matrix-calculus/en-US/index.html#sec:adjoint-method",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Reverse-mode “Adjoint” Differentiation",
    "text": "Reverse-mode “Adjoint” Differentiation\nBut what is adjoint differentiation—the method of differentiating that makes these applications actually feasible to solve? Ultimately, it is yet another example of left-to-right/reverse-mode differentiation, essentially applying the chain rule from outputs to inputs. Consider, for example, trying to compute the gradient ∇g\\nabla g of the scalar-valued function g(p)=f(A(p)−1b⏟x).g(p) = f(\\underbrace{A(p)^{-1} b}_x) \\, . where xx solves A(p)x=bA(p) x  = b (e.g. a parameterized physical model as in the previous section) and f(x)f(x) is a scalar-valued function of xx (e.g. an optimization objective depending on our physics solution). For example, this could arise in an optimization problem $$\\min_p g(p) \\Longleftrightarrow \\substack{  \\text{ \\normalsize  $\\displaystyle \\min_p f(x)$} \\\\ \\text{ subject to } A(p)x = b }\\; ,$$ for which the gradient ∇g\\nabla g would be helpful to search for a local minimum. The chain rule for gg corresponds to the following conceptual chain of dependencies: change dg in g←change dx in x=A−1b←change d(A−1) in A−1←change dA in A(p)←change dp in p\\begin{aligned}\n\\text{change $dg$ in $g$} &\\longleftarrow \n\\text{change $dx$ in $x = A^{-1} b$}  \\\\\n&\\longleftarrow \\text{change $d(A^{-1})$ in $A^{-1}$} \\\\\n&\\longleftarrow \n\\text{change $dA$ in $A(p)$}  \\\\\n&\\longleftarrow \\text{change $dp$ in $p$}\n\\end{aligned} which is expressed by the equations: dg=f′(x)[dx] dg←dx=f′(x)[d(A−1)b] dx←d(A−1)=−f′(x)A−1⏟vTdAA−1b dA−1←dA=−vTA′(p)[dp]⏟dAA−1b dA←dp.\\begin{aligned}\ndg &= f'(x) [dx] &\\text{ } & dg \\longleftarrow dx    \\\\\n& = f'(x) [d(A^{-1}) b] &\\text{ } & dx \\longleftarrow d(A^{-1})   \\\\\n& = - \\underbrace{f'(x) A^{-1}}_{v^T} dA \\, A^{-1} b &\\text{ } & dA^{-1} \\longleftarrow dA \\\\\n& = - v^T \\underbrace{A'(p)[dp]}_{dA} \\, A^{-1} b &\\text{ } &dA \\longleftarrow dp \\, .\n\\end{aligned} Here, we are defining the row vector vT=f′(x)A−1v^T = f'(x) A^{-1}, and we have used the differential of a matrix inverse d(A−1)=−A−1dAA−1d(A^{-1})=-A^{-1}\\,dA\\,A^{-1} from Sec. 7.3.\nGrouping the terms left-to-right, we first solve the “adjoint” (transposed) equation ATv=f′(x)T=∇xfA^T v = f'(x)^T = \\nabla_x f for vv, and then we obtain dg=−vTdAxdg = - v^T dA \\, x. Because the derivative A′(p)A'(p) of a matrix with respect to a vector is awkward to write explicitly, it is convenient to examine this object one parameter at a time. For any given parameter pkp_k, ∂g/∂pk=−vT(∂A/∂pk)x\\partial g/\\partial p_k = -v^T (\\partial A/\\partial p_k) x (and in many applications ∂A/∂pk\\partial A /\\partial p_k is very sparse); here, “dividing by” ∂pk\\partial p_k works because this is a scalar factor that commutes with the other linear operations. That is, it takes only two solves to get both gg and ∇g\\nabla g: one for solving Ax=bA x = b to find g(p)=f(x)g(p)=f(x), and another with ATA^T for vv, after which all of the derivatives ∂g/∂pk\\partial g/\\partial p_k are just some cheap dot products.\nNote that you should not use right-to-left “forward-mode” derivatives with lots of parameters, because ∂g∂pk=−f′(x)(A−1∂A∂pkx)\\frac{\\partial g}{\\partial p_k} = - f'(x) \\left(A^{-1} \\frac{\\partial A}{\\partial p_k}x\\right) represents one solve per parameter pkp_k! As discussed in Sec. 8.4, right-to-left (a.k.a. forward mode) is better when there is one (or few) input parameters pkp_k and many outputs, while left-to-right “adjoint” differentiation (a.k.a. reverse mode) is better when there is one (or few) output values and many input parameters. (In Sec. 8.1, we will discuss using dual numbers for differentiation, and this also corresponds to forward mode.)\nAnother possibility that might come to mind is to use finite differences (as in Sec. 4), but you should not use this if you have lots of parameters! Finite differences would involve a calculation of something like ∂g∂pk≈[g(p+ϵek)−g(p)]/ϵ,\\frac{\\partial g}{\\partial p_k} \\approx [g(p + \\epsilon e_k) - g(p)]/\\epsilon, where eke_k is a unit vector in the kk-th direction and ϵ\\epsilon is a small number. This, however, requires one solve for each parameter pkp_k, just like forward-mode differentiation. (It becomes even more expensive if you use fancier higher-order finite-difference approximations in order to obtain higher accuracy.)\n\nNonlinear equations\nYou can also apply adjoint/reverse differentiation to nonlinear equations. For instance, consider the gradient of the scalar function g(p)=f(x(p))g(p) = f(x(p)), where $x(p)\\in \\R^n$ solves some system of nn equations $h(p,x) = 0 \\in \\R^n$. By the chain rule, h(p,x)=0⟹∂h∂pdp+∂h∂xdx=0⟹dx=−(∂h∂x)−1∂h∂pdp.h(p,x) = 0 \\implies \\frac{\\partial h}{\\partial p} dp + \\frac{\\partial  h}{\\partial x} dx = 0 \\implies dx = -\\left(\\frac{\\partial  h}{\\partial x}\\right)^{-1}  \\frac{\\partial h}{\\partial p}   dp \\,. (This is an instance of the Implicit Function Theorem: as long as ∂h∂x\\frac{\\partial  h}{\\partial x} is nonsingular, we can locally define a function x(p)x(p) from an implicit equation h=0h=0, here by linearization.) Hence, dg=f′(x)dx=−f′(x)(∂h∂x)−1⏟vT∂h∂pdp.dg = f'(x) dx = - \\underbrace{ f'(x) \\left(\\frac{\\partial  h}{\\partial x}\\right)^{-1} }_{v^T} \\frac{\\partial h}{\\partial p} dp \\, . Associating left-to-right again leads to a single “adjoint” equation: (∂h/∂x)Tv=f′(x)T=∇xf(\\partial h/\\partial x)^T v = f'(x)^T = \\nabla_x f. In other words, it again only takes two solves to get both gg and ∇g\\nabla g—one nonlinear “forward” solve for xx and one linear “adjoint” solve for vv! Thereafter, all derivatives ∂g/∂pk\\partial g/\\partial p_k are cheap dot products. (Note that the linear “adjoint” solve involves the transposed Jacobian ∂h/∂x\\partial h/\\partial x. Except for the transpose, this is very similar to the cost of a single Newton step to solve h=0h=0 for xx. So the adjoint problem should be cheaper than the forward problem.)\n\n\nAdjoint methods and AD\nIf you use automatic differentiation (AD) systems, why do you need to learn this stuff? Doesn’t the AD do everything for you? In practice, however, it is often helpful to understand adjoint methods even if you use automatic differentiation. Firstly, it helps you understand when to use forward- vs. reverse-mode automatic differentiation. Secondly, many physical models call large software packages written over the decades in various languages that cannot be differentiated automatically by AD. You can typically correct this by just supplying a “vector–Jacobian product” yTdxy^T dx for this physics, or even just part of the physics, and then AD will differentiate the rest and apply the chain rule for you. Lastly, often models involve approximate calculations (e.g. for the iterative solution of linear or nonlinear equations, numerical integration, and so forth), but AD tools often don’t “know” this and spend extra effort trying to differentiate the error in your approximation; in such cases, manually written derivative rules can sometimes be much more efficient. (For example, suppose your model involves solving a nonlinear system h(x,p)=0h(x,p) = 0 by an iterative approach like Newton’s method. Naive AD will be very inefficient because it will attempt to differentiate through all your Newton steps. Assuming that you converge your Newton solver to enough accuracy that the error is negligible, it is much more efficient to perform differentiation via the implicit-function theorem as described above, leading to a single linear adjoint solve.)\n\n\nAdjoint-method example\nTo finish off this section of the notes, we conclude with an example of how to use this “adjoint method” to compute a derivative efficiently. Before working through the example, we first state the problem and highly recommend trying it out before reading the solution.\n\n Suppose that A(p)A(p) takes a vector p∈ℝn−1p\\in\\mathbb{R}^{n-1} and returns the n×nn\\times n tridiagonal real-symmetric matrix A(p)=(a1p1p1a2p2p2⋱⋱⋱an−1pn−1pn−1an),A(p)=\\left(\\begin{array}{ccccc}\na_{1} & p_{1}\\\\\np_{1} & a_{2} & p_{2}\\\\\n & p_{2} & \\ddots & \\ddots\\\\\n &  & \\ddots & a_{n-1} & p_{n-1}\\\\\n &  &  & p_{n-1} & a_{n}\n\\end{array}\\right), where a∈ℝna\\in\\mathbb{R}^{n} is some constant vector. Now, define a scalar-valued function f(p)f(p) by g(p)=(cTA(p)−1b)2g(p)=\\left(c^{T}A(p)^{-1}b\\right)^{2} for some constant vectors b,c∈ℝnb,c\\in\\mathbb{R}^{n} (assuming we choose pp and aa so that AA is invertible). Note that, in practice, A(p)−1bA(p)^{-1}b is not computed by explicitly inverting the matrix AA—instead, it can be computed in Θ(n)\\Theta(n) (i.e., roughly proportional to nn) arithmetic operations using Gaussian elimination that takes advantage of the “sparsity” of AA (the pattern of zero entries), a “tridiagonal solve.”\n\nWrite down a formula for computing ∂g/∂p1\\partial g/\\partial p_{1} (in terms of matrix–vector products and matrix inverses). (Hint: once you know dgdg in terms of dAdA, you can get ∂g/∂p1\\partial g/\\partial p_{1} by “dividing” both sides by ∂p1\\partial p_{1}, so that dAdA becomes ∂A/∂p1\\partial A/\\partial p_{1}.)\nOutline a sequence of steps to compute both gg and ∇g\\nabla g (with respect to pp) using only two tridiagonal solves x=A−1bx=A^{-1}b and an “adjoint” solve v=A−1(something)v=A^{-1}\\text{(something)}, plus Θ(n)\\Theta(n) (i.e., roughly proportional to nn) additional arithmetic operations.\nWrite a program implementing your ∇g\\nabla g procedure (in Julia, Python, Matlab, or any language you want) from the previous part. (You don’t need to use a fancy tridiagonal solve if you don’t know how to do this in your language; you can solve A−1(vector)A^{-1}\\text{(vector)} inefficiently if needed using your favorite matrix libraries.) Implement a finite-difference test: Choose a,b,c,pa,b,c,p at random, and check that ∇g⋅δp≈g(p+δp)−g(p)\\nabla g\\cdot\\delta p\\approx g(p+\\delta p)-g(p) (to a few digits) for a randomly chosen small δp\\delta p.\n\n\n[PSETexample](a) Solution: From the chain rule and the formula for the differential of a matrix inverse, we have dg=−2(cTA−1b)cTA−1dAA−1bdg = -2(c^T A^{-1} b) c^T A^{-1} dA\\,A^{-1} b (noting that cTA−1bc^T A^{-1} b is a scalar so we can commute it as needed). Hence ∂g∂p1=−2(cTA−1b)cTA−1⏟vT∂A∂p1A−1b⏟x=vT(011000⋱⋱⋱0000)⏟∂A∂p1x=v1x2+v2x1,\\begin{aligned}\n\\frac{\\partial g}{\\partial p_1} &= \\underbrace{-2(c^T A^{-1} b) c^T A^{-1}}_{v^T} \\frac{\\partial A}{\\partial p_1} \\underbrace{A^{-1} b}_x \\\\\n&=  v^T \\underbrace{\\left(\\begin{array}{ccccc}\n0 & 1\\\\\n1 & 0 & 0\\\\\n & 0 & \\ddots & \\ddots\\\\\n &  & \\ddots & 0 & 0\\\\\n &  &  & 0 & 0\n\\end{array}\\right)}_{\\frac{\\partial A}{\\partial p_1}} x = \\boxed{v_1 x_2 + v_2 x_1} \\, ,\n\\end{aligned} where we have simplified the result in terms of xx and vv for the next part.\n[PSETexample](b) Solution: Using the notation from the previous part, exploiting the fact that AT=AA^T = A, we can choose v=A−1[−2(cTx)c]\\boxed{v = A^{-1} [-2(c^T x) c]}, which is a single tridiagonal solve. Given xx and vv, the results of our two Θ(n)\\Theta(n) tridiagonal solves, we can compute each component of the gradient similar to above by ∂g/∂pk=vkxk+1+vk+1xk\\boxed{\\partial g/\\partial p_k = v_k x_{k+1} + v_{k+1} x_k} for k=1,…,n−1k=1,\\ldots,n-1, which costs Θ(1)\\Theta(1) arithmetic per kk and hence Θ(n)\\Theta(n) arithmetic to obtain all of ∇g\\nabla g.\n[PSETexample](c) Solution: See the Julia solution notebook (Problem 1) from our IAP 2023 course (which calls the function ff rather than gg)."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#two-derivations",
    "href": "matrix-calculus/en-US/index.html#two-derivations",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Two Derivations",
    "text": "Two Derivations\nThis section of notes follows this Julia notebook. This notebook is a little bit short, but is an important and useful calculation.\n\nGiven AA is a square matrix, we have ∇(detA)=cofactor(A)=(detA)A−T:=adj(AT)=adj(A)T\\nabla (\\det A) = \\mathrm{cofactor}(A) = (\\det A)A^{-T} := \\operatorname{adj}(A^T) = \\operatorname{adj}(A)^T where adj\\operatorname{adj} is the “adjugate”. (You may not have heard of the matrix adjugate, but this formula tells us that it is simply adj(A)=det(A)A−1\\operatorname{adj}(A) = \\det(A) A^{-1}, or cofactor(A)=adj(AT)\\mathrm{cofactor}(A) = \\operatorname{adj}(A^T).) Furthermore, $$d(\\det A) = \\tr(\\det(A) A^{-1} dA) = \\tr (\\operatorname{adj}(A) dA) = \\tr(\\mathrm{cofactor}(A)^T dA).$$\n\nYou may remember that each entry (i,j)(i,j) of the cofactor matrix is (−1)i+j(-1)^{i + j} times the determinant obtained by deleting row ii and column jj from AA. Here are some 2×22 \\times 2 calculuations to obtain some intuition about these functions: M=(acbd)⟹cofactor(M)=(d−c−ba)adj(M)=(d−b−ca)(M)−1=1ad−bc(d−b−ca).\\begin{aligned}\n    M &= \\begin{pmatrix}\n        a & c \\\\ b & d    \n    \\end{pmatrix} \\\\\n    \\implies \\mathrm{cofactor}(M) &= \\begin{pmatrix}\n         d & -c \\\\ -b & a\n    \\end{pmatrix}  \\\\\n    \\operatorname{adj}(M) &= \\begin{pmatrix}\nd & -b \\\\ -c & a\n\\end{pmatrix} \\\\\n(M)^{-1}  &= \\frac{1}{ad-bc} \\begin{pmatrix}\n    d & -b \\\\ -c & a\n\\end{pmatrix}.\n\\end{aligned}\nNumerically, as is done in the notebook, you can construct a random n×nn \\times n matrix AA (say, 9×99 \\times 9), consider e.g. dA=.00001AdA = .00001 A, and see numerically that $$\\det(A + dA) - \\det (A) \\approx \\tr(\\operatorname{adj}(A) dA),$$ which numerically supports our claim for the theorem.\nWe now prove the theorem in two ways. Firstly, there is a direct proof where you just differentiate the scalar with respect to every input using the cofactor expansion of the determinant based on the ii-th row. Recall that det(A)=Ai1Ci1+Ai2Ci2+…+AinCin.\\det (A) = A_{i1} C_{i1} +A_{i2} C_{i2} + \\dots + A_{in} C_{in}. Thus, ∂detA∂Aij=Cij⟹∇(detA)=C,\\frac{\\partial \\det A}{ \\partial A_{ij}} = C_{ij} \\implies \\nabla (\\det A) = C, the cofactor matrix. (In computing these partial derivatives, it’s important to remember that the cofactor CijC_{ij} contains no elements of AA from row ii or column jj. So, for example, Ai1A_{i1} only appears explicitly in the first term, and not hidden in any of the CC terms in this expansion.)\nThere is also a fancier proof of the theorem using linearization near the identity. Firstly, note that it is easy to see from the properties of determinants that $$\\det(I + dA) - 1 = \\tr(dA),$$ and thus $$\\begin{aligned}\n    \\det(A + A(A^{-1} dA)) - \\det (A) &= \\det(A) (\\det (I + A^{-1} dA) - 1) \\\\\n    &= \\det(A) \\tr(A^{-1} dA) = \\tr(\\det (A) A^{-1} dA) \\\\\n    &= \\tr(\\operatorname{adj}(A) dA).\n\\end{aligned}$$ This also implies the theorem."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#applications-1",
    "href": "matrix-calculus/en-US/index.html#applications-1",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Applications",
    "text": "Applications\n\nCharacteristic Polynomial\nWe now use this as an application to find the derivative of a characteristic polynomial evaluated at xx. Let p(x)=det(xI−A)p(x) = \\det(xI -A), a scalar function of xx. Recall that through factorization, p(x)p(x) may be written in terms of eigenvalues λi\\lambda_i. So we may ask: what is the derivative of p(x)p(x), the characteristic polynomial at xx? Using freshman calculus, we could simply compute ddx∏i(x−λi)=∑i∏j≠i(x−λj)=∏(x−λi){∑i(x−λi)−1},\\frac{d}{dx} \\prod_i (x-\\lambda_i) = \\sum_i \\prod_{j\\neq i} (x-\\lambda_j)  = \\prod (x-\\lambda_i) \\{\\sum_i (x- \\lambda_i)^{-1}\\}, as long as x≠λix \\neq \\lambda_i.\nThis is a perfectly good simply proof, but with our new technology we have a new proof: $$\\begin{aligned}\n    d(\\det (x I - A)) &= \\det(x I - A) \\tr((xI - A)^{-1} d(x I - A)) \\\\\n    &= \\det(xI - A) \\tr(x I - A)^{-1} dx.\n\\end{aligned}$$ Note that here we used that d(xI−A)=dxId(x I - A) = dx \\, I when AA is constant and $\\tr (A dx) = \\tr(A) dx$ since dxdx is a scalar.\nWe may again check this computationally as we do in the notebook.\n\n\nThe Logarithmic Derivative\nWe can similarly compute using the chain rule that $$d(\\log (\\det (A))) = \\frac{d(\\det A)}{ \\det A} = \\det (A^{-1}) d(\\det (A)) = \\tr(A^{-1} dA).$$ The logarithmic derivative shows up a lot in applied mathematics. Note that here we use that 1detA=det(A−1)\\frac{1}{\\det A} = \\det(A^{-1}) as 1=det(I)=det(AA−1)=det(A)det(A−1).1 = \\det(I) = \\det(AA^{-1}) = \\det (A) \\det(A^{-1}).\nFor instance, recall Newton’s method to find roots f(x)=0f(x)=0 of single-variable real-valued functions f(x)f(x) by taking a sequence of steps x→x+δxx \\to x + \\delta x. The key formula in Newton’s method is δx=f′(x)−1f(x)\\delta x = f'(x)^{-1}f(x), but this is the same as 1(logf(x))′\\frac{1}{(\\log f(x))'}. So, derivatives of log determinants show up in finding roots of determinants, i.e. for f(x)=detM(x)f(x) = \\det M(x). When M(x)=A−xIM(x) = A - x I, roots of the determinant are eigenvalues of AA. For more general functions M(x)M(x), solving detM(x)=0\\det M(x) = 0 is therefore called a nonlinear eigenproblem."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#sec:jacobian-inverse",
    "href": "matrix-calculus/en-US/index.html#sec:jacobian-inverse",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Jacobian of the Inverse",
    "text": "Jacobian of the Inverse\nLastly, we compute the derivative (as both a linear operator and an explicit Jacobian matrix) of the inverse of a matrix. There is a neat trick to obtain this derivative, simply from the property A−1A=IA^{-1}A = I of the inverse. By the product rule, this implies that d(A−1A)=d(I)=0=d(A−1)A+A−1dA⟹d(A−1)=(A−1)′[dA]=−A−1dAA−1.\\begin{aligned}\nd(A^{-1} A) &= d(I) = 0 = d(A^{-1}) A + A^{-1} dA \\\\\n& \\implies \\boxed{d(A^{-1}) = (A^{-1})'[dA] = - A^{-1} \\, dA \\, A^{-1} }\\, .\n\\end{aligned} Here, the second line defines a perfectly good linear operator for the derivative (A−1)′(A^{-1})', but if we want we can rewrite this as an explicit Jacobian matrix by using Kronecker products acting on the “vectorized” matrices as we did in Sec. 3: vec(d(A−1))=vec(−A−1(dA)A−1)=−(A−T⊗A−1)⏟Jacobianvec(dA),\\operatorname{vec}\\left(d(A^{-1})\\right) = \\operatorname{vec}\\left(-A^{-1} (dA) A^{-1}\\right) = \\underbrace{- (A^{-T} \\otimes A^{-1})}_\\mathrm{Jacobian} \\operatorname{vec}(dA) \\, , where A−TA^{-T} denotes (A−1)T=(AT)−1(A^{-1})^T = (A^T)^{-1}. One can check this formula numerically, as is done in the notebook.\nIn practice, however, you will probably find that the operator expression −A−1dAA−1- A^{-1} \\, dA \\, A^{-1} is more useful than explicit Jacobian matrix for taking derivatives involving matrix inverses. For example, if you have a matrix-valued function A(t)A(t) of a scalar parameter t∈ℝt \\in \\mathbb{R}, you immediately obtain d(A−1)dt=−A−1dAdtA−1\\frac{d(A^{-1})}{dt} = -A^{-1} \\frac{dA}{dt} A^{-1}. A more sophisticated application is discussed in Sec. 6.3."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#sec:dual-AD",
    "href": "matrix-calculus/en-US/index.html#sec:dual-AD",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Automatic Differentiation via Dual Numbers",
    "text": "Automatic Differentiation via Dual Numbers\n(This lecture is accompanied by a Julia “notebook” showing the results of various computational experiments, which can be found on the course web page. Excerpts from those experiments are included below.)\nOne AD approach that can be explained relatively simply is “forward-mode” AD, which is implemented by carrying out the computation of f′f' in tandem with the computation of ff. One augments every intermediate value aa in the computer program with another value bb that represents its derivative, along with chain rules to propagate these derivatives through computations on values in the program. It turns out that this can be thought of as replacing real numbers (values aa) with a new kind of “dual number” D(a,b)D(a,b) (values & derivatives) and corresponding arithmetic rules, as explained below.\n\nExample: Babylonian square root\nWe start with a simple example, an algorithm for the square-root function, where a practical method of automatic differentiation came as both a mathematical surprise and a computing wonder for Professor Edelman. In particular, we consider the “Babylonian” algorithm to compute x\\sqrt{x}, known for millennia (and later revealed as a special case of Newton’s method applied to t2−x=0t^2 - x = 0): simply repeat t←(t+x/t)/2t \\leftarrow (t + x/t)/2 until tt converges to x\\sqrt{x} to any desired accuracy. Each iteration has one addition and two divisions. For illustration purposes, 10 iterations suffice. Here is a short program in Julia that implements this algorithm, starting with a guess of 11 and then performing NN steps (defaulting to N=10N=10):\njulia&gt; function Babylonian(x; N = 10) \n           t = (1+x)/2   # one step from t=1\n           for i = 2:N   # remaining N-1 steps\n               t = (t + x/t) / 2\n           end    \n           return t\n       end\nIf we run this function to compute the square root of x=4x=4, we will see that it converges very quickly: for only N=3N=3 steps, it obtains the correct answer (22) to nearly 3 decimal places, and well before N=10N=10 steps it has converged to 22 within the limits of the accuracy of computer arithmetic (about 16 digits). In fact, it roughly doubles the number of correct digits on every step:\njulia&gt; Babylonian(4, N=1)\n2.5\n\njulia&gt; Babylonian(4, N=2)\n2.05\n\njulia&gt; Babylonian(4, N=3)\n2.000609756097561\n\njulia&gt; Babylonian(4, N=4)\n2.0000000929222947\n\njulia&gt; Babylonian(4, N=10)\n2.0\nOf course, any first-year calculus student knows the derivative of the square root, (x)′=0.5/x(\\sqrt{x})' = 0.5/\\sqrt{x}, which we could compute here via 0.5 / Babylonian(x), but we want to know how we can obtain this derivative automatically, directly from the Babylonian algorithm itself. If we can figure out how to easily and efficiently pass the chain rule through this algorithm, then we will begin to understand how AD can also differentiate much more complicated computer programs for which no simple derivative formula is known.\n\n\nEasy forward-mode AD\nThe basic idea of carrying the chain rule through a computer program is very simple: replace every number with two numbers, one which keeps track of the value and one which tracks the derivative of that value. The values are computed the same way as before, and the derivatives are computed by carrying out the chain rule for elementary operations like ++ and //.\nIn Julia, we can implement this idea by defining a new type of number, which we’ll call D, that encapsulates a value val and a derivative deriv.\njulia&gt; struct D &lt;: Number\n           val::Float64\n           deriv::Float64\n       end\n(A detailed explanation of Julia syntax can be found elsewhere, but hopefully you can follow the basic ideas even if you don’t understand every punctuation mark.) A quantity x = D(a,b) of this new type has two components x.val = a and x.deriv = b, which we will use to represent values and derivatives, respectively. The Babylonian code only uses two arithmetic operations, ++ and //, so we just need to overload the built-in (“Base”) definitions of these in Julia to include new rules for our D type:\njulia&gt; Base.:+(x::D, y::D) = D(x.val+y.val, x.deriv+y.deriv)\n       Base.:/(x::D, y::D) = D(x.val/y.val, (y.val*x.deriv - x.val*y.deriv)/y.val^2)\nIf you look closely, you’ll see that the values are just added and divided in the ordinary way, while the derivatives are computed using the sum rule (adding the derivatives of the inputs) and the quotient rule, respectively. We also need one other technical trick: we need to define “conversion” and “promotion” rules that tell Julia how to combine D values with ordinary real numbers, as in expressions like x+1x+1 or x/2x/2:\njulia&gt; Base.convert(::Type{D}, r::Real) = D(r,0)\n       Base.promote_rule(::Type{D}, ::Type{&lt;:Real}) = D\nThis just says that an ordinary real number rr is combined with a D value by first converting rr to D(r,0): the value is rr and the derivative is 00 (the derivative of any constant is zero).\nGiven these definitions, we can now plug a D value into our unmodified Babylonian function, and it will “magically” compute the derivative of the square root. Let’s try it for x=49=72x = 49 = 7^2:\njulia&gt; x = 49\n49\n\njulia&gt; Babylonian(D(x,1))\nD(7.0, 0.07142857142857142)\nWe can see that it correctly returned a value of 7.0 and a derivative of 0.07142857142857142, which indeed matches the square root 49\\sqrt{49} and its derivative 0.5/490.5/\\sqrt{49}:\njulia&gt; (√x, 0.5/√x)\n(7.0, 0.07142857142857142)\nWhy did we input D(x,1)? Where did the 11 come from? That’s simply the fact that the derivative of the input xx with respect to itself is (x)′=1(x)' = 1, so this is the starting point for the chain rule.\nIn practice, all this (and more) has already been implemented in the ForwardDiff.jl package in Julia (and in many similar software packages in a variety of languages). That package hides the implementation details under the hood and explicitly provides a function to compute the derivative. For example:\njulia&gt; using ForwardDiff\n\njulia&gt; ForwardDiff.derivative(Babylonian, 49)\n0.07142857142857142\nEssentially, however, this is the same as our little D implementation, but implemented with greater generality and sophistication (e.g. chain rules for more operations, support for more numeric types, partial derivatives with respect to multiple variables, etc.): just as we did, ForwardDiff augments every value with a second number that tracks the derivative, and propagates both quantities through the calculation.\nWe could have also implemented the same idea specifically for the Bablylonian algorithm, by writing a new function dBabylonian that tracks both the variable tt and its derivative t′=dt/dxt' = dt/dx through the course of the calculation:\njulia&gt; function dBabylonian(x; N = 10) \n           t = (1+x)/2\n           t′ = 1/2\n           for i = 1:N\n               t = (t+x/t)/2\n               t′= (t′+(t-x*t′)/t^2)/2\n           end    \n           return t′\n       end\n\njulia&gt; dBabylonian(49)\n0.07142857142857142\nThis is doing exactly the same calculations as calling Babylonian(D(x,1)) or ForwardDiff.derivative(Babylonian, 49), but needs a lot more human effort—we’d have to do this for every computer program we write, rather than implementing a new number type once.\n\n\nDual numbers\nThere is a pleasing algebraic way to think about our new number type D(a,b)D(a,b) instead of the “value & derivative” viewpoint above. Remember how a complex number a+bia + bi is formed from two real numbers (a,b)(a,b) by defining a special new quantity ii (the imaginary unit) that satisfies i2=−1i^2 = -1, and all the other complex-arithmetic rules follow from this? Similarly, we can think of D(a,b)D(a,b) as a+bϵa + b \\epsilon, where ϵ\\epsilon is a new “infinitesimal unit” quantity that satisfies ϵ2=0\\epsilon^2 = 0. This viewpoint is called a dual number.\nGiven the elementary rule ϵ2=0\\epsilon^2 = 0, the other algebraic rules for dual numbers immediately follow: (a+bϵ)±(c+dϵ)=(a±c)+(b±d)ϵ(a+bϵ)⋅(c+dϵ)=(ac)+(bc+ad)ϵa+bϵc+dϵ=a+bϵc+dϵ⋅c−dϵc−dϵ=(a+bϵ)(c−dϵ)c2=ac+bc−adc2ϵ.\\begin{aligned}\n    (a + b \\epsilon) \\pm (c + d \\epsilon) &= (a \\pm c) + (b \\pm d) \\epsilon \\\\\n    (a + b \\epsilon) \\cdot (c + d\\epsilon) &= (ac) + (bc + ad) \\epsilon \\\\\n    \\frac{a + b \\epsilon}{c + d \\epsilon} &= \\frac{a + b \\epsilon}{c + d \\epsilon} \\cdot \\frac{c - d \\epsilon}{c - d \\epsilon} = \\frac{(a + b \\epsilon)(c - d \\epsilon)}{c^2}  =     \n    \\frac{a}{c} + \\frac{bc - ad}{c^2}\\epsilon.\n\\end{aligned} The ϵ\\epsilon coefficients of these rules correspond to the sum/difference, product, and quotient rules of differential calculus!\nIn fact, these are exactly the rules we implemented above for our D type. We were only missing the rules for subtraction and multiplication, which we can now include:\njulia&gt; Base.:-(x::D, y::D) = D(x.val - y.val, x.deriv - y.deriv)\n       Base.:*(x::D, y::D) = D(x.val*y.val, x.deriv*y.val + x.val*y.deriv)\nIt’s also nice to add a “pretty printing” rule to make Julia display dual numbers as a + bϵ rather than as D(a,b):\njulia&gt; Base.show(io::IO, x::D) = print(io, x.val, \" + \", x.deriv, \"ϵ\")\nOnce we implement the multiplication rule for dual numbers in Julia, then ϵ2=0\\epsilon^2 = 0 follows from the special case a=c=0a = c=0 and b=d=1b=d=1:\njulia&gt; ϵ = D(0,1)\n0.0 + 1.0ϵ\n\njulia&gt; ϵ * ϵ \n0.0 + 0.0ϵ\n\njulia&gt; ϵ^2\n0.0 + 0.0ϵ\n(We didn’t define a rule for powers D(a,b)nD(a,b)^n, so how did it compute ϵ2^2? The answer is that Julia implements xnx^n via repeated multiplication by default, so it sufficed to define the ** rule.) Now, we can compute the derivative of the Babylonian algorithm at x=49x = 49 as above by:\njulia&gt; Babylonian(x + ϵ)\n7.0 + 0.07142857142857142ϵ\nwith the “infinitesimal part” being the derivative 0.5/49=0.0714⋯0.5/\\sqrt{49} = 0.0714\\cdots.\nA nice thing about this dual-number viewpoint is that it corresponds directly to our notion of a derivative as linearization: f(x+ϵ)=f(x)+f′(x)ϵ+(higher-order terms),f(x + \\epsilon) = f(x) + f'(x) \\epsilon + \\mbox{(higher-order terms)} \\, , with the dual-number rule ϵ2=0\\epsilon^2 = 0 corresponding to dropping the higher-order terms."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#naive-symbolic-differentiation",
    "href": "matrix-calculus/en-US/index.html#naive-symbolic-differentiation",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Naive symbolic differentiation",
    "text": "Naive symbolic differentiation\nForward-mode AD implements the exact analytical derivative by propagating chain rules, but it is completely different from what many people imagine AD might be: evaluating a program symbolically to obtain a giant symbolic expression, and then differentiating this giant expression to obtain the derivative. A basic issue with this approach is that the size of these symbolic expressions can quickly explode as the program runs. Let’s see what it would look like for the Babylonian algorithm.\nImagine inputting a “symbolic variable” xx into our Babylonian code, running the algorithm, and writing a big algebraic expression for the result. After only one step, for example, we would get (x+1)/2(x + 1)/2. After two steps, we would get ((x+1)/2+2x/(x+1))/2((x+1)/2 + 2x/(x+1))/2, which simplifies to a ratio of two polynomials (a “rational function”): x2+6x+14(x+1).\\frac{x^2 + 6x + 1}{4(x+1)} \\, . Continuing this process by hand is quite tedious, but fortunately the computer can do it for us (as shown in the accompanying Julia notebook). Three Babylonian iterations yields: x4+28x3+70x2+28x+18(x3+7x2+7x+1),\\frac{x^{4} + 28 x^{3} + 70 x^{2} + 28 x + 1}{8 \\left(x^{3} + 7 x^{2} + 7 x + 1\\right)} \\, , four iterations gives x8+120x7+1820x6+8008x5+12870x4+8008x3+1820x2+120x+116(x7+35x6+273x5+715x4+715x3+273x2+35x+1),\\frac{x^{8} + 120 x^{7} + 1820 x^{6} + 8008 x^{5} + 12870 x^{4} + 8008 x^{3} + 1820 x^{2} + 120 x + 1}{16 \\left(x^{7} + 35 x^{6} + 273 x^{5} + 715 x^{4} + 715 x^{3} + 273 x^{2} + 35 x + 1\\right)} \\, , and five iterations produces the enormous expression: $$\\resizebox{1.0\\hsize}{!}{$\\frac{x^{16} + 496 x^{15} + 35960 x^{14} + 906192 x^{13} + 10518300 x^{12} + 64512240 x^{11} + 225792840 x^{10} + 471435600 x^{9} + 601080390 x^{8} + 471435600 x^{7} + 225792840 x^{6} + 64512240 x^{5} + 10518300 x^{4} + 906192 x^{3} + 35960 x^{2} + 496 x + 1}{32 \\left(x^{15} + 155 x^{14} + 6293 x^{13} + 105183 x^{12} + 876525 x^{11} + 4032015 x^{10} + 10855425 x^{9} + 17678835 x^{8} + 17678835 x^{7} + 10855425 x^{6} + 4032015 x^{5} + 876525 x^{4} + 105183 x^{3} + 6293 x^{2} + 155 x + 1\\right)}$} \\, .$$ Notice how quickly these grow—in fact, the degree of the polynomials doubles on every iteration! Now, if we take the symbolic derivatives of these functions using our ordinary calculus rules, and simplify (with the help of the computer), the derivative of one iteration is 12\\frac{1}{2}, of two iterations is x2+2x+54(x2+2x+1),\\frac{x^{2} + 2 x + 5}{4 \\left(x^{2} + 2 x + 1\\right)} \\, , of three iterations is x6+14x5+147x4+340x3+375x2+126x+218(x6+14x5+63x4+100x3+63x2+14x+1),\\frac{x^{6} + 14 x^{5} + 147 x^{4} + 340 x^{3} + 375 x^{2} + 126 x + 21}{8 \\left(x^{6} + 14 x^{5} + 63 x^{4} + 100 x^{3} + 63 x^{2} + 14 x + 1\\right)} \\, , of four iterations is $$\\resizebox{1.0\\hsize}{!}{$\\frac{x^{14} + 70 x^{13} + 3199 x^{12} + 52364 x^{11} + 438945 x^{10} + 2014506 x^{9} + 5430215 x^{8} + 8836200 x^{7} + 8842635 x^{6} + 5425210 x^{5} + 2017509 x^{4} + 437580 x^{3} + 52819 x^{2} + 3094 x + 85}{16 \\left(x^{14} + 70 x^{13} + 1771 x^{12} + 20540 x^{11} + 126009 x^{10} + 440986 x^{9} + 920795 x^{8} + 1173960 x^{7} + 920795 x^{6} + 440986 x^{5} + 126009 x^{4} + 20540 x^{3} + 1771 x^{2} + 70 x + 1\\right)}$} \\, ,$$ and of five iterations is a monstrosity you can only read by zooming in: $$\\resizebox{1.0\\hsize}{!}{$\\frac{x^{30} + 310 x^{29} + 59799 x^{28} + 4851004 x^{27} + 215176549 x^{26} + 5809257090 x^{25} + 102632077611 x^{24} + 1246240871640 x^{23} + 10776333438765 x^{22} + 68124037776390 x^{21} + 321156247784955 x^{20} + 1146261110726340 x^{19} + 3133113888931089 x^{18} + 6614351291211874 x^{17} + 10850143060249839 x^{16} + 13883516068991952 x^{15} + 13883516369532147 x^{14} + 10850142795067314 x^{13} + 6614351497464949 x^{12} + 3133113747810564 x^{11} + 1146261195398655 x^{10} + 321156203432790 x^{9} + 68124057936465 x^{8} + 10776325550040 x^{7} + 1246243501215 x^{6} + 102631341330 x^{5} + 5809427001 x^{4} + 215145084 x^{3} + 4855499 x^{2} + 59334 x + 341}{32 \\left(x^{30} + 310 x^{29} + 36611 x^{28} + 2161196 x^{27} + 73961629 x^{26} + 1603620018 x^{25} + 23367042639 x^{24} + 238538538360 x^{23} + 1758637118685 x^{22} + 9579944198310 x^{21} + 39232152623175 x^{20} + 122387258419860 x^{19} + 293729420641881 x^{18} + 546274556891506 x^{17} + 791156255418003 x^{16} + 894836006026128 x^{15} + 791156255418003 x^{14} + 546274556891506 x^{13} + 293729420641881 x^{12} + 122387258419860 x^{11} + 39232152623175 x^{10} + 9579944198310 x^{9} + 1758637118685 x^{8} + 238538538360 x^{7} + 23367042639 x^{6} + 1603620018 x^{5} + 73961629 x^{4} + 2161196 x^{3} + 36611 x^{2} + 310 x + 1\\right)}$} \\, .$$ This is a terrible way to compute derivatives! (However, more sophisticated approaches to efficient symbolic differentiation exist, such as the “D*D^*” algorithm, that avoid explicit giant formulas by exploiting repeated subexpressions.)\nTo be clear, the dual number approach (absent rounding errors) computes an answer exactly as if it evaluated these crazy expressions at some particular xx, but the words “as if” are very important here. As you can see, we do not form these expressions, let alone evaluate them. We merely compute results that are equal to the values we would have gotten if we had."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#automatic-differentiation-via-computational-graphs",
    "href": "matrix-calculus/en-US/index.html#automatic-differentiation-via-computational-graphs",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Automatic Differentiation via Computational Graphs",
    "text": "Automatic Differentiation via Computational Graphs\nLet’s now get into automatic differentiation via computational graphs. For this section, we consider the following simple motivating example.\n\n Define the following functions: {a(x,y)=sinxb(x,y)=1y⋅a(x,y)z(x,y)=b(x,y)+x.\\begin{cases}\n        a(x,y) = \\sin x \\\\\n        b(x,y) = \\frac{1}{y}\\cdot a(x,y) \\\\\n        z(x,y) = b(x,y) + x.\n    \\end{cases} Compute ∂z∂x\\frac{\\partial z}{\\partial x} and ∂z∂y.\\frac{\\partial z}{\\partial y}.\n\nThere are a few ways to solve this problem. Firstly, of course, one can compute this symbolically, noting that z(x,y)=b(x,y)+x=1ya(x,y)+x=sinxy+x,z(x,y) = b(x,y) + x = \\frac{1}{y} a(x,y) + x = \\frac{\\sin x}{y} + x, which implies $$\\frac{\\partial z}{\\partial x} = \\frac{\\cos x}{y} + 1 \\hspace{.25cm} \\text{and} \\hspace{.25cm} \\frac{\\partial z}{\\partial y} = -\\frac{\\sin x}{y^2}.$$\nHowever, one can also use a Computational Graph (see Figure of Computational Graph below) where the edge from node AA to node BB is labelled with ∂B∂A\\frac{\\partial B}{\\partial A}.\n\n\n\n\n\nA computational graph corresponding to example [ex:compute-graph], representing the computation of an output z(x, y) from two inputs x, y, with intermediate quantities a(x, y) and b(x, y). The nodes are labelled by values, and edges are labelled with the derivatives of the values with respect to the preceding values.\n\n\nNow how do we use this directed acyclic graph (DAG) to find the derivatives? Well one view (called the “forward view”) is given by following the paths from the inputs to the outputs and (left) multiplying as you go, adding together multiple paths. For instance, following this procedure for paths from xx to z(x,y)z(x,y), we have ∂z∂x=1⋅1y⋅cosx+1=cosxy+1.\\frac{\\partial z}{\\partial x} = 1 \\cdot \\frac{1}{y} \\cdot \\cos x + 1 = \\frac{\\cos x}{y} + 1. Similarly, for paths from yy to z(x,y)z(x,y), we have ∂z∂y=1⋅−a(x,y)y2=−sinxy2,\\frac{\\partial z}{\\partial y} = 1 \\cdot \\frac{-a(x,y)}{ y^2} = \\frac{- \\sin x}{y^2}, and if you have numerical derivatives on the edges, this algorithm works. Alternatively, you could follow a reverse view and follow the paths backwards (multiplying right to left), and obtain the same result. Note that there is nothing magic about these being scalar here– you could imagine these functions are the type that we are seeing in this class and do the same computations! The only thing that matters here fundamentally is the associativity. However, when considering vector-valued functions, the order in which you multiply the edge weights is vitally important (as vector/matrix valued functions are not generally commutative).\nThe graph-theoretic way of thinking about this is to consider “path products”. A path product is the product of edge weights as you traverse a path. In this way, we are interested in the sum of path products from inputs to outputs to compute derivatives using computational graphs. Clearly, we don’t particularly care which order we traverse the paths as long as the order we take the product in is correct. In this way, forward and reverse-mode automatic differentiation is not so mysterious.\nLet’s take a closer view of the implementation of forward-mode automatic differentiation. Suppose we are at a node AA during the process of computing the derivative of a computational graph, as shown in the figure below:\n\n\n\n\n\nSuppose we know the path product PP of all the edges up to and including the one from B2B_2? to AA. Then what is the new path product as we move to the right from AA? It is f′(A)⋅Pf'(A)\\cdot P! So we need a data structure that maps in the following way: (value,path product)↦(f(value),f′⋅path product).(\\text{value}, \\text{path product}) \\mapsto (f(\\text{value}), f'\\cdot \\text{path product}). In some sense, this is another way to look at the Dual Numbers– taking in our path products and spitting out values. In any case, we overload our program which can easily calculate f(value)f(\\text{value}) and tack-on f′⋅(path product)f'\\cdot \\text{(path product)}.\nOne might ask how our program starts– this is how the program works in the “middle”, but what should our starting value be? Well the only thing it can be for this method to work is (x,1)(x, 1). Then, at every step you do the following map listed above: (value,path product)↦(f(value),f′⋅path product),(\\text{value}, \\text{path product}) \\mapsto (f(\\text{value}), f'\\cdot \\text{path product}), and at the end we obtain our derivatives.\nNow how do we combine arrows? In other words, suppose at the two notes on the LHS we have the values (a,p)(a,p) and (b,q)(b,q), as seen in the diagram below:\n\n\n\n\n\nSo here, we aren’t thinking of a,ba,b as numbers, but as variables. What should the new output value be? We want to add the two path products together, obtaining (f(a,b),∂z∂ap+∂z∂bq).\\left(f(a,b), \\frac{\\partial z}{\\partial a} p + \\frac{\\partial z}{\\partial b} q\\right). So really, our overloaded data structure looks like this:\n\n\n\n\n\nThis diagram of course generalizes if we may many different nodes on the left side of the graph.\nIf we come up with such a data structure for all of the simple computations (addition/subtraction, multiplication, and division), and if this is all we need for our computer program, then we are set! Here is how we define the structure for addition/subtraction, multiplication, and division.\nAddition/Subtraction: See figure.\n\n\n\n\n\nFigure of Addition/Subtraction Computational Graph\n\n\nMultiplication: See figure.\n\n\n\n\n\nFigure of Multiplication Computational Graph\n\n\nDivision: See figure.\n\n\n\n\n\nFigure of Division Computational Graph\n\n\nIn theory, these three graphs are all we need, and we can use Taylor series expansions for more complicated functions. But in practice, we throw in what the derivatives of more complicated functions are so that we don’t waste our time trying to compute something we already know, like the derivative of sine or of a logarithm.\n\nReverse Mode Automatic Differentiation on Graphs\nWhen we do reverse mode, we have arrows going the other direction, which we will understand in this section of the notes. In forward mode it was all about “what do we depend on,” i.e. computing the derivative on the right hand side of the above diagram using the functions in the nodes on the left. In reverse mode, the question is really “what are we influenced by?” or “what do we influence later?”\nWhen going “backwards,” we need know what nodes a given node influences. For instance, given a node A, we want to know the nodes BiB_i that is influenced by, or depends on, node AA. So now our diagram looks like this:\n\n\n\n\n\nSo now, we eventually have a final node (z,1)(z,1) (far on the right hand side) where everything starts. This time, all of our multiplications take place from right to left as we are in reverse mode. Our goal is to be able to calculate the node (x,∂z/∂x)(x,\\partial z/\\partial x). So if we know how to fill in the ∂z∂a\\frac{\\partial z}{\\partial a} term, we will be able to go from right to left in these computational graphs (i.e., in reverse mode). In fact, the formula for getting ∂z∂a\\frac{\\partial z}{\\partial a} is given by ∂z∂a=∑i=1s∂bi∂a∂z∂bi\\frac{\\partial z}{\\partial a} = \\sum_{i=1}^s \\frac{\\partial b_i}{\\partial a} \\frac{\\partial z}{\\partial b_i} where the bib_is come from the nodes that are influenced by the node AA. This is again just another chain rule like from calculus, but you can also view this as multiplying the sums of all the weights in the graph influenced by AA.\n\n\n\n\n\nWhy can reverse mode be more efficient than forward mode? One reason it because it can save data and use it later. Take, for instance, the following sink/source computational graph.\nIf x,yx,y here are our sources, and zz is our sink, we want to compute the sum of products of weights on paths from sources to sinks. If we were using forward mode, we would need to compute the paths dcadca and dcbdcb, which requires four multiplications (and then you would add them together). If we were using reverse mode, we would only need compute acd_a\\underline{cd} and bcd_b\\underline{cd} and sum them; notice reverse mode (since we need only compute cdcd once), only takes 3 multiplications. In general, this can more efficiently resolve certain types of problems, such as the source/sink one."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#sec:forward-vs-reverse",
    "href": "matrix-calculus/en-US/index.html#sec:forward-vs-reverse",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Forward- vs. Reverse-mode Differentiation",
    "text": "Forward- vs. Reverse-mode Differentiation\nIn this section, we briefly summarize the relative benefits and drawbacks of these two approaches to computation of derivatives (whether worked out by hand or using AD software). From a mathematical point of view, the two approaches are mirror images, but from a computational point of view they are quite different, because computer programs normally proceed “forwards” in time from inputs to outputs.\nSuppose we are differentiating a function f:ℝn↦ℝmf: \\mathbb{R}^n \\mapsto \\mathbb{R}^m, mapping nn scalar inputs (an nn-dimensional input) to mm scalar outputs (an mm-dimensional output). The first key distinction of forward- vs. reverse-mode is how the computational cost scales with the number/dimension of inputs and outputs:\n\nThe cost of forward-mode differentiation (inputs-to-outputs) scales proportional to nn, the number of inputs. This is ideal for functions where n≪mn \\ll m (few inputs, many outputs).\nThe cost of reverse-mode differentiation (outputs-to-inputs) scales proportional to mm, the number of outputs. This is ideal for functions where m≪nm \\ll n (few outputs, many inputs).\n\nBefore this chapter, we first saw these scalings in Sec. 2.5.1, and again in Sec. 6.3; in a future lecture, we’ll see it yet again in Sec. 9.2. The case of few outputs is extremely common in large-scale optimization (whether for machine learning, engineering design, or other applications), because then one has many optimization parameters (n≫1n\\gg 1) but only a single output (m=1m=1) corresponding to the objective (or “loss”) function, or sometimes a few outputs corresponding to objective and constraint functions. Hence, reverse-mode differentiation (“backpropagation”) is the dominant approach for large-scale optimization and applications such as training neural networks.\nThere are other practical issues worth considering, however:\n\nForward-mode differentiation proceeds in the same order as the computation of the function itself, from inputs to outputs. This seems to make forward-mode AD easier to implement (e.g. our sample implementation in Sec. 8.1) and efficient.\nReverse-mode differentiation proceeds in the opposite direction to ordinary computation. This makes reverse-mode AD much more complicated to implement, and adds a lot of storage overhead to the function computation. First you evaluate the function from inputs to outputs, but you (or the AD system) keep a record (a “tape”) of all the intermediate steps of the computation; then, you run the computation in reverse (“play the tape backwards”) to backpropagate the derivatives.\n\nAs a result of these practical advantages, even for the case of many (n&gt;1n &gt;1) inputs and a single (m=1m = 1) output, practitioners tell us that they’ve found forward mode to be more efficient until nn becomes sufficiently large (perhaps even until n&gt;100n &gt; 100, depending on the function being differentiated and the AD implementation). (You may also be interested in the blog post Engineering Trade-offs in AD by Chris Rackauckas, which is mainly about reverse-mode implementations.)\nIf n=mn = m, where neither approach has a scaling advantage, one typically prefers the lower overhead and simplicity of forward-mode differentiation. This case arises in computing explicit Jacobian matrices for nonlinear root-finding (Sec. 6.1), or Hessian matrices of second derivatives (Sec. 12), for which one often uses forward mode…or even a combination of forward and reverse modes, as discussed below.\nOf course, forward and reverse are not the only options. The chain rule is associative, so there are many possible orderings (e.g. starting from both ends and meeting in the middle, or vice versa). A difficult8 problem that may often require hybrid schemes is to compute Jacobians (or Hessians) in a minimal number of operations, exploiting any problem-specific structure (e.g. sparsity: many entries may be zero). Discussion of this and other AD topics can be found, in vastly greater detail than in these notes, in the book Evaluating Derivatives (2nd ed.) by Griewank and Walther (2008).\n\nForward-over-reverse mode: Second derivatives\nOften, a combination of forward- and reverse-mode differentiation is advantageous when computing second derivatives, which arise in many practical applications.\nHessian computation: For example, let us consider a function f(x):ℝn→ℝf(x): \\mathbb{R}^n \\to \\mathbb{R} mapping nn inputs xx to a single scalar. The first derivative f′(x)=(∇f)Tf'(x) = (\\nabla f)^T is best computed by reverse mode if n≫1n \\gg 1 (many inputs). Now, however, consider the second derivative, which is the derivative of g(x)=∇fg(x) = \\nabla f, mapping nn inputs xx to nn outputs ∇f\\nabla f. It should be clear that g′(x)g'(x) is therefore an n×nn \\times n Jacobian matrix, called the Hessian of ff, which we will discuss much more generally in Sec. 12. Since g(x)g(x) has the same number of inputs and outputs, neither forward nor reverse mode has an inherent scaling advantage, so typically forward mode is chosen for g′g' thanks to its practical simplicity, while still computing ∇f\\nabla f in reverse-mode. That is, we compute ∇f\\nabla f by reverse mode, but then compute g′=(∇f)′g' = (\\nabla f)' by applying forward-mode differentiation to the ∇f\\nabla f algorithm. This is called a forward-over-reverse algorithm.\nAn even more clear-cut application of forward-over-reverse differentiation is to Hessian–vector products. In many applications, it turns out that what is required is only the product (∇f)′v(\\nabla f)' v of the Hessian (∇f)′(\\nabla f)' with an arbitrary vector vv. In this case, one can completely avoid computing (or storing) the Hessian matrix explicitly, and incur computational cost proportional only to that of a single function evaluation f(x)f(x). The trick is to recall (from Sec. 2.2.1) that, for any function gg, the linear operation g′(x)[v]g'(x)[v] is a directional derivative, equivalent to a single-variable derivative ∂∂αg(x+αv)\\frac{\\partial}{\\partial\\alpha} g(x+\\alpha v) evaluated at α=0\\alpha = 0. Here, we simply apply that rule to the function g(x)=∇fg(x) = \\nabla f, and obtain the following formula for a Hessian–vector product: (∇f)′v=∂∂α(∇f|x+αv)|α=0.(\\nabla f)' v = \\left. \\frac{\\partial}{\\partial\\alpha} \\left( \\left. \\nabla f \\right|_{x + \\alpha v} \\right) \\right|_{\\alpha=0} \\, . Computationally, the inner evaluation of the gradient ∇f\\nabla f at an arbitrary point x+αvx + \\alpha v can be accomplished efficiently by a reverse/adjoint/backpropagation algorithm. In contrast, the outer derivative with respect to a single input α\\alpha is best performed by forward-mode differentiation.9 Since the Hessian matrix is symmetric (as discussed in great generality by Sec. 12), the same algorithm works for vector–Hessian products vT(∇f)′=[(∇f)′v]Tv^T (\\nabla f)' = [(\\nabla f)' v]^T, a fact that we employ in the next example.\nScalar-valued functions of gradients: There is another common circumstance in which one often combines forward and reverse differentiation, but which can appear somewhat more subtle, and that is in differentiating a scalar-valued function of a gradient of another scalar-valued function. Consider the following example:\n\nLet f(x):ℝn↦ℝf(x): \\mathbb{R}^n \\mapsto \\mathbb{R} be a scalar-valued function of n≫1n \\gg 1 inputs with gradient ∇f|x=f′(x)T\\left. \\nabla f \\right|_{x} = f'(x)^T, and let g(z):ℝn↦ℝg(z): \\mathbb{R}^n \\mapsto \\mathbb{R} be another such function with gradient ∇g|z=g′(z)T\\left. \\nabla g \\right|_{z} = g'(z)^T. Now, consider the scalar-valued function h(x)=g(∇f|x):ℝn↦ℝh(x) = g(\\left. \\nabla f \\right|_{x}): \\mathbb{R}^n \\mapsto \\mathbb{R} and compute ∇h|x=h′(x)T\\left. \\nabla h \\right|_{x} = h'(x)^T.\nDenote z=∇f|xz = \\left. \\nabla f \\right|_{x}. By the chain rule, h′(x)=g′(z)(∇f)′(x)h'(x) = g'(z)(\\nabla f)'(x), but we want to avoid explicitly computing the large n×nn \\times n Hessian matrix (∇f)′(\\nabla f)'. Instead, as discussed above, we use the fact that such a vector–Hessian product is equivalent (by symmetry of the Hessian) to the transpose of a Hessian–vector product multiplying the Hessian (∇f)′(\\nabla f)' with the vector ∇g=g′(z)T\\nabla g = g'(z)^T, which is equivalent to a directional derivative: ∇h|x=h′(x)T=∂∂α(∇f|x+α∇g|z)|α=0,\\left. \\nabla h \\right|_{x} = h'(x)^T =\n\\left. \\frac{\\partial}{\\partial\\alpha} \\left( \\left. \\nabla f \\right|_{x + \\alpha \\left. \\nabla g \\right|_{z}}\\right) \\right|_{\\alpha = 0} \\, , involving differentiation with respect to a single scalar α∈ℝ\\alpha \\in \\mathbb{R}. As for any Hessian–vector product, therefore, we can evaluate hh and ∇h\\nabla h by:\n\nEvaluate h(x)h(x): evaluate z=∇f|xz = \\left. \\nabla f \\right|_{x} by reverse mode, and plug it into g(z)g(z).\nEvaluate ∇h\\nabla h:\n\nEvaluate ∇g|z\\left. \\nabla g \\right|_{z} by reverse mode.\nImplement ∇f|x+α∇g|z\\left. \\nabla f \\right|_{x + \\alpha \\left. \\nabla g \\right|_{z} } by reverse mode, and then differentiate with respect to α\\alpha by forward mode, evaluated at α=0\\alpha = 0.\n\n\nThis is a “forward-over-reverse” algorithm, where forward mode is used efficiently for the single-input derivative with respect to α∈ℝ\\alpha \\in \\mathbb{R}, combined with reverse mode to differentate with respect to x,z∈ℝnx,z \\in \\mathbb{R}^n.\n\nExample Julia code implementing the above “forward-over-reverse” process for just such a h(x)=g(∇f)h(x)=g(\\nabla f) function is given below. Here, the forward-mode differentiation with respect to α\\alpha is implemented by the ForwardDiff.jl package discussed in Sec. 8.1, while the reverse-mode differentiation with respect to xx or zz is performed by the Zygote.jl package. First, let’s import the packages and define simple example functions f(x)=1/‖x‖f(x) = 1/\\Vert x \\Vert and g(z)=(∑kzk)3g(z) = (\\sum_k z_k)^3, along with the computation of hh via Zygote:\njulia&gt; using ForwardDiff, Zygote, LinearAlgebra\njulia&gt; f(x) = 1/norm(x)\njulia&gt; g(z) = sum(z)^3\njulia&gt; h(x) = g(Zygote.gradient(f, x)[1])\nNow, we’ll compute ∇h\\nabla h by forward-over-reverse:\njulia&gt; function ∇h(x)\n           ∇f(y) = Zygote.gradient(f, y)[1]\n           ∇g = Zygote.gradient(g, ∇f(x))[1]\n           return ForwardDiff.derivative(α -&gt; ∇f(x + α*∇g), 0)\n       end\nWe can now plug in some random numbers and compare to a finite-difference check:\njulia&gt; x = randn(5); δx = randn(5) * 1e-8;\n\njulia&gt; h(x)\n-0.005284687528953334\n\njulia&gt; ∇h(x)\n5-element Vector{Float64}:\n -0.006779692698531759\n  0.007176439898271982\n -0.006610264199241697\n -0.0012162087082746558\n  0.007663756720005014\n\njulia&gt; ∇h(x)' * δx    # directional derivative\n-3.0273434457397667e-10\n\njulia&gt; h(x+δx) - h(x)  # finite-difference check\n-3.0273433933303284e-10\nThe finite-difference check matches to about 7 significant digits, which is as much as we can hope for—the forward-over-reverse code works!\n\nA common variation on the above procedure, which often appears in machine learning, involves a function f(x,p)∈ℝf(x,p) \\in \\mathbb{R} that maps input “data” x∈ℝnx \\in \\mathbb{R}^n and “parameters” p∈ℝNp \\in \\mathbb{R}^N to a scalar. Let ∇xf\\nabla_x f and ∇pf\\nabla_p f denote the gradients with respect to xx and pp.\nNow, suppose we have a function g(z):ℝn↦ℝg(z): \\mathbb{R}^n \\mapsto \\mathbb{R} as before, and define h(x,p)=g(∇xf|x,p)h(x,p) = g(\\left. \\nabla_x f \\right|_{x,p}). We want to compute ∇ph=(∂h/∂p)T\\nabla_p h = (\\partial h / \\partial p)^T, which will involve “mixed” derivatives of ff with respect to both xx and pp.\nShow that you can compute ∇ph\\nabla_p h by: ∇ph|x,p=∂∂α(∇pf|x+α∇g|z,p)|α=0,\\left. \\nabla_p h \\right|_{x,p} =\n\\left. \\frac{\\partial}{\\partial\\alpha} \\left( \\left. \\nabla_p f \\right|_{x + \\alpha \\left. \\nabla g \\right|_{z},p}\\right) \\right|_{\\alpha = 0} \\, , where z=∇xf|x,pz = \\left. \\nabla_x f \\right|_{x,p}. (Crucially, this avoids ever computing an n×Nn \\times N mixed-derivative matrix of ff.)\nTry coming up with simple example functions ff and gg, implementing the above formula by forward-over-reverse in Julia similar to above (forward mode for ∂/∂α\\partial/\\partial \\alpha and reverse mode for the ∇\\nabla’s), and checking your result against a finite-difference approximation."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#ordinary-differential-equations-odes",
    "href": "matrix-calculus/en-US/index.html#ordinary-differential-equations-odes",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Ordinary differential equations (ODEs)",
    "text": "Ordinary differential equations (ODEs)\nAn ordinary differential equation (ODE) is an equation for a function u(t)u(t) of “time”10 t∈ℝt\\in\\mathbb{R} in terms of one or more derivatives, most commonly in the first-order form dudt=f(u,t)\\frac{du}{dt}=f(u,t) for some right-hand-side function ff. Note that u(t)u(t) need not be a scalar function—it could be a column vector u∈ℝnu\\in\\mathbb{R}^{n}, a matrix, or any other differentiable object. One could also write ODEs in terms of higher derivatives d2u/dt2d^{2}u/dt^{2} and so on, but it turns out that one can write any ODE in terms of first derivatives alone, simply by making uu a vector with more components.11 To uniquely determine a solution of a first-order ODE, we need some additional information, typically an initial value u(0)=u0u(0)=u_{0} (the value of uu at t=0t=0), in which case it is called an initial-value problem. These facts, and many other properties of ODEs, are reviewed in detail by many textbooks on differential equations, as well as in classes like 18.03 at MIT.\nODEs are important for a huge variety of applications, because the behavior of many realistic systems is defined in terms of rates of change (derivatives). For example, you may recall Newton’s laws of mechanics, in which acceleration (the derivative of velocity) is related to force (which may be a function of time, position, and/or velocity), and the solution u=[position,velocity]u=[\\text{position},\\text{velocity}] of the corresponding ODE tells us the trajectory of the system. In chemistry, uu might represent the concentrations of one or more reactant molecules, with the right-hand side ff providing reaction rates. In finance, there are ODE-like models of stock or option prices. Partial differential equations (PDEs) are more complicated versions of the same idea, for example in which u(x,t)u(x,t) is a function of space xx as well as time tt and one has ∂u∂t=f(u,x,t)\\frac{\\partial u}{\\partial t}=f(u,x,t) in which ff may involve some spatial derivatives of uu.\nIn linear algebra (e.g. 18.06 at MIT), we often consider initial-value problems for linear ODEs of the form du/dt=Audu/dt=Au where uu is a column vector and AA is a square matrix; if AA is a constant matrix (independent of tt or uu), then the solution u(t)=eAtu(0)u(t)=e^{At}u(0) can be described in terms of a matrix exponential eAte^{At}. More generally, there are many tricks to find explicit solutions of various sorts of ODEs (various functions ff). However, just as one cannot find explicit formulas for the integrals of most functions, there is no explicit formula for the solution of most ODEs, and in many practical applications one must resort to approximate numerical solutions. Fortunately, if you supply a computer program that can compute f(u,t)f(u,t), there are mature and sophisticated software libraries12 which can compute u(t)u(t) from u(0)u(0) for any desired set of times tt, to any desired level of accuracy (for example, to 8 significant digits).\nFor example, the most basic numerical ODE method computes the solution at a sequence of times tn=nΔtt_{n}=n\\Delta t for n=0,1,2,…n=0,1,2,\\ldots simply by approximating dudt=f(u,t)\\frac{du}{dt}=f(u,t) using the finite difference u(tn+1)−u(tn)Δt≈f(u(tn),tn)\\frac{u(t_{n+1})-u(t_{n})}{\\Delta t}\\approx f(u(t_{n}),t_{n}), giving us the “explicit” timestep algorithm: u(tn+1)≈u(tn)+Δtf(u(tn),tn).u(t_{n+1})\\approx u(t_{n})+\\Delta t\\,f(u(t_{n}),t_{n}). Using this technique, known as “Euler’s method,” we can march the solution forward in time: starting from our initial condition u0u_{0}, we compute u(t1)=u(Δt)u(t_1) = u(\\Delta t), then u(t2)=u(2Δt)u(t_2) = u(2\\Delta t) from u(Δt)u(\\Delta t), and so forth. Of course, this might be rather inaccurate unless we make Δt\\Delta t very small, necessitating many timesteps to reach a given time tt, and there can arise other subtleties like “instabilities” where the error may accumulate exponentially rapidly with each timestep. It turns out that Euler’s method is mostly obsolete: there are much more sophisticated algorithms that robustly produce accurate solutions with far less computational cost. However, they all resemble Euler’s method in the conceptual sense: they use evaluations of ff and uu at a few nearby times tt to “extrapolate” uu at a subsequent time somehow, and thus march the solution forwards through time.\nRelying on a computer to obtain numerical solutions to ODEs is practically essential, but it can also make ODEs a lot more fun to work with. If you ever took a class on ODEs, you may remember a lot of tedious labor (tricky integrals, polynomial roots, systems of equations, integrating factors, etc.) to obtain solutions by hand. Instead, we can focus here on simply setting up the correct ODEs and integrals and trust the computer to do the rest."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#sec:ODE-sensitivity",
    "href": "matrix-calculus/en-US/index.html#sec:ODE-sensitivity",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Sensitivity analysis of ODE solutions",
    "text": "Sensitivity analysis of ODE solutions\n\n\n\nIf we have an ordinary differential equation (ODE) ∂u∂t=f(u,p,t)\\frac{\\partial u}{\\partial t} = f(u, p, t) whose solution u(p,t)u(p, t) depends on parameters pp, we would like to know the change du=u(p+dp,t)−u(p,t)d u = u(p + d p, t) - u(p, t) in the solution due to changes in pp. Here, we show a simple example ∂u∂t=−pu\\frac{\\partial u}{\\partial t} = -pu, whose solution u(p,t)=e−ptu(p,0)u(p,t) = e^{-p t} u(p,0) is known analytically, and show the change δu\\delta u from changing p=1p=1 to by δp=0.1\\delta p = 0.1.\n\n\nOften, ODEs depend on some additional parameters p∈ℝNp\\in\\mathbb{R}^{N} (or some other vector space). For example, these might be reaction-rate coefficients in a chemistry problem, the masses of particles in a mechanics problem, the entries of the matrix AA in a linear ODE, and so on. So, you really have a problem of the form ∂u∂t=f(u,p,t),\\frac{\\partial u}{\\partial t}=f(u,p,t), where the solution u(p,t)u(p,t) depends both on time tt and the parameters pp, and in which the initial condition u(p,0)=u0(p)u(p,0)=u_{0}(p) may also depend on the parameters.\nThe question is, how can we compute the derivative ∂u/∂p\\partial u/\\partial p of the solution with respect to the parameters of the ODE? By this, as usual, we mean the linear operator that gives the first-order change in uu for a change in pp, as depicted in Fig. 6: u(p+dp,t)−u(p,t)=∂u∂p[dp](an n-component infinitesimal vector),u(p+dp,t)-u(p,t)=\\frac{\\partial u}{\\partial p}[dp] \\qquad \\mbox{(an }n\\mbox{-component infinitesimal vector)}, where of course ∂u/∂p\\partial u/\\partial p (which can be thought of as an n×Nn \\times N Jacobian matrix) depends on pp and tt. This kind of question is commonplace. For example, it is important in:\n\nUncertainty quantification (UQ): if you have some uncertainty in the parameters of your ODE (for example, you have a chemical reaction in which the reaction rates are only known experimentally ±\\pm some measurement errors), the derivative ∂u/∂p\\partial u/\\partial p tells you (to first order, at least) how sensitive your answer is to each of these uncertainties.\nOptimization and fitting: often, you want to choose the parameters pp to maximize or minimize some objective (or “loss” in machine learning). For example, if your ODE models some chemical reaction with unknown reaction rates or other parameters pp, you might want to fit the parameters pp to minimize the difference between u(p,t)u(p,t) and some experimentally observed concentrations.\n\nIn the latter case of optimization, you have a scalar objective function of the solution, since to minimize or maximize something you need a real number (and uu might be a vector). For example, this could take on one of the following two forms:\n\nA real-valued function g(u(p,T),T)∈ℝg(u(p,T),T) \\in \\mathbb{R} that depends on the solution u(p,T)u(p,T) at a particular time TT. For example, if you have an experimental solution u*(t)u_{*}(t) that you are are trying to match at t=Tt=T, you might minimize g(u(p,T),T)=‖u(p,T)−u*(T)‖2g(u(p,T),T)=\\Vert u(p,T)-u_{*}(T)\\Vert^{2}.\nA real-valued function G(p)=∫0Tg(u(p,t),t)dtG(p)=\\int_{0}^{T}g(u(p,t),t)dt that depends on an average (here scaled by TT) over many times t∈(0,T)t\\in(0,T) of our time-dependent gg. In the example of fitting experimental data u*(t)u_{*}(t), minimizing G(p)=∫0T‖u(p,t)−u*(t)‖2dtG(p)=\\int_{0}^{T}\\Vert u(p,t)-u_{*}(t)\\Vert^{2}dt corresponds to a least-square fit to minimize the error averaged over a time TT (e.g. the duration of your experiment).\n\nMore generally, you can give more weight to certain times than others by including a non-negative weight function w(t)w(t) in the integral: Gw(p)=∫0∞∥u(p,t)−u*(t)∥2⏟g(u(p,t),t)w(t)dt,.G_w(p)=\\int_0^\\infty \\underbrace{\\|u(p,t)-u_*(t)\\|^2}_{g(u(p,t),t)}  \\, w(t) \\, dt , . The two cases above are simply the choices w(t)=δ(t−T)w(t) = \\delta(t-T) (a Dirac delta function) and w(t)={1t≤T0otherwisew(t) = \\begin{cases} 1 & t \\le T \\\\ 0 & \\text{otherwise} \\end{cases} (a step function), respectively. As discussed in Problem [prob:discrete-data], you can let w(t)w(t) be a sum of delta functions to represent data at a sequence of discrete times.\nIn both cases, since these are scalar-valued functions, for optimization/fitting one would like to know the gradient ∇pg\\nabla_{p}g or ∇pG\\nabla_{p}G, such that, as usual, g(u(p+dp,t),t)−g(u(p,t),t)=(∇pg)Tdpg(u(p+dp,t),t)-g(u(p,t),t)=\\left(\\nabla_{p}g\\right)^{T}dp so that ±∇pg\\pm\\nabla_{p}g is the steepest ascent/descent direction for maximization/minimization of gg, respectively. It is worth emphasizing that gradients (which we only define for scalar-valued functions) have the same shape as their inputs pp, so ∇pg\\nabla_p g is a vector of length NN (the number of parameters) that depends on pp and tt.\nThese are “just derivatives,” but probably you can see the difficulty: if we don’t have a formula (explicit solution) for u(p,t)u(p,t), only some numerical software that can crank out numbers for u(p,t)u(p,t) given any parameters pp and tt, how do we apply differentiation rules to find ∂u/∂p\\partial u/\\partial p or ∇pg\\nabla_{p}g? Of course, we could use finite differences as in Sec. 4—just crank through numerical solutions for pp and p+δpp+\\delta p and subtract them—but that will be quite slow if we want to differentiate with respect to many parameters (N≫1N\\gg1), not to mention giving potentially poor accuracy. In fact, people often have huge numbers of parameters inside an ODE that they want to differentiate. Nowadays, our right-hand-side function f(u,p,t)f(u,p,t) can even contain a neural network (this is called a “neural ODE”) with thousands or millions (NN) of parameters pp, and we need all NN of these derivatives ∇pg\\nabla_{p}g or ∇pG\\nabla_{p}G to minimize the “loss” function gg or GG. So, not only do we need to find a way to differentiate our ODE solutions (or scalar functions thereof), but these derivatives must be obtained efficiently. It turns out that there are two ways to do this, and both of them hinge on the fact that the derivative is obtained by solving another ODE:\n\nForward mode: ∂u∂p\\frac{\\partial u}{\\partial p} turns out to solve another ODE that we can integrate with the same numerical solvers for uu. This gives us all of the derivatives we could want, but the drawback is that the ODE for ∂u∂p\\frac{\\partial u}{\\partial p} is larger by a factor of NN than the original ODE for uu, so it is only practical for small NN (few parameters).\nReverse (“adjoint”) mode: for scalar objectives, it turns out that ∇pg\\nabla_{p}g or ∇pG\\nabla_{p}G can be computed by solving a different ODE for an “adjoint” solution v(p,t)v(p,t) of the same size as uu, and then computing some simple integrals involving uu (the “forward” solution) and vv. This has the advantage of giving us all NN derivatives with only about twice the cost of solving for uu, regardless of the number NN of parameters. The disadvantage is that, since it turns out that vv must be integrated “backwards” in time (starting from an “initial” condition at t=Tt=T and working back to t=0t=0) and depends on uu, it is necessary to store u(p,t)u(p,t) for all t∈[0,T]t\\in[0,T] (rather than marching uu forwards in time and discarding values from previous times when they are no longer needed), which can require a vast amount of computer memory for large ODE systems integrated over long times.\n\nWe will now consider each of these approaches in more detail.\n\nForward sensitivity analysis of ODEs\nLet us start with our ODE ∂u∂t=f(u,p,t)\\frac{\\partial u}{\\partial t}=f(u,p,t), and consider what happens to uu for a small change dpdp in pp: d(∂u∂t)⏟=f(u,p,t)=∂∂t(du)=∂∂t(∂u∂p[dp])=∂∂t(∂u∂p)[dp]=d(f(u,p,t))=(∂f∂u∂u∂p+∂f∂p)[dp],\\begin{aligned}\nd\\underbrace{\\left(\\frac{\\partial u}{\\partial t}\\right)}_{=f(u,p,t)} &= \\frac{\\partial}{\\partial t} (du)\n=\n\\frac{\\partial}{\\partial t}\n\\left( \\frac{\\partial u}{\\partial p} [ dp ]\\right)=\\frac{\\partial}{\\partial t}\\left(\\frac{\\partial u}{\\partial p}\\right)[dp] \\\\\n&=d(f(u,p,t)) = \\left(\\frac{\\partial f}{\\partial u}\\frac{\\partial u}{\\partial p}+\\frac{\\partial f}{\\partial p}\\right)[dp],\n\\end{aligned} where we have used the familiar rule (from multivariable calculus) of interchanging the order of partial derivatives—a property that we will re-derive explicitly for our generalized linear-operator derivatives in our lecture on Hessians and second derivatives. Equating the right-hand sides of the two lines, we see that we have an ODE ∂∂t(∂u∂p)=∂f∂u∂u∂p+∂f∂p\\boxed{\\frac{\\partial}{\\partial t}\\left(\\frac{\\partial u}{\\partial p}\\right)=\\frac{\\partial f}{\\partial u}\\frac{\\partial u}{\\partial p}+\\frac{\\partial f}{\\partial p}} for the derivative ∂u∂p\\frac{\\partial u}{\\partial p}, whose initial condition is obtained simply by differentiating the initial condition u(p,0)=u0(p)u(p,0)=u_{0}(p) for uu: ∂u∂p|t=0=∂u0∂p.\\left.\\frac{\\partial u}{\\partial p}\\right|_{t=0}=\\frac{\\partial u_{0}}{\\partial p}. We can therefore plug this into any ODE solver technique (usually numerical methods, unless we are extremely lucky and can solve this ODE analytically for a particular ff) to find ∂u∂p\\frac{\\partial u}{\\partial p} at any desired time tt. Simple, right?\nThe only thing that might seem a little weird here is the shape of the solution: ∂u∂p\\frac{\\partial u}{\\partial p} is a linear operator, but how can the solution of an ODE be a linear operator? It turns out that there is nothing wrong with this, but it is helpful to think about a few examples:\n\nIf u,p∈ℝu,p\\in\\mathbb{R} are scalars (that is, we have a single scalar ODE with a single scalar parameter), then ∂u∂p\\frac{\\partial u}{\\partial p} is just a (time-dependent) number, and our ODE for ∂u∂p\\frac{\\partial u}{\\partial p} is an ordinary scalar ODE with an ordinary scalar initial condition.\nIf u∈ℝnu\\in\\mathbb{R}^{n} (a “system” of nn ODEs) and p∈ℝp\\in\\mathbb{R} is a scalar, then ∂u∂p∈ℝn\\frac{\\partial u}{\\partial p}\\in\\mathbb{R}^{n} is another column vector and our ODE for ∂u∂p\\frac{\\partial u}{\\partial p} is another system of nn ODEs. So, we solve two ODEs of the same size nn to obtain uu and ∂u∂p\\frac{\\partial u}{\\partial p}.\nIf u∈ℝnu\\in\\mathbb{R}^{n} (a “system” of nn ODEs) and p∈ℝNp\\in\\mathbb{R}^{N} is a vector of NN parameters, then ∂u∂p∈ℝn×N\\frac{\\partial u}{\\partial p}\\in\\mathbb{R}^{n\\times N} is an n×Nn\\times N Jacobian matrix. Our ODE for ∂u∂p\\frac{\\partial u}{\\partial p} is effectivly system of nNnN ODEs for all the components of this matrix, with a matrix ∂u0∂p\\frac{\\partial u_{0}}{\\partial p} of nNnN initial conditions! Solving this “matrix ODE” with numerical methods poses no conceptual difficulty, but will generally require about NN times the computational work of solving for uu, simply because there are NN times as many unknowns. This could be expensive if NN is large!\n\nThis reflects our general observation of forward-mode differentiation: it is expensive when the number NN of “input” parameters being differentiated is large. However, forward mode is straightforward and, especially for N≲100N\\lesssim100 or so, is often the first method to try when differentiating ODE solutions. Given ∂u∂p\\frac{\\partial u}{\\partial p} , one can then straightforwardly differentiate scalar objectives by the chain rule: ∇pg|t=T=∂u∂pT⏟JacobianT∂g∂uT⏟vector|t=T,∇pG=∫0T∇pgdt.\\begin{aligned}\n\\left.\\nabla_{p}g\\right|_{t=T} & =\\left.\\underbrace{\\frac{\\partial u}{\\partial p}^{T}}_{\\text{Jacobian}^{T}}\\underbrace{\\frac{\\partial g}{\\partial u}^{T}}_{\\text{vector}}\\right|_{t=T},\\\\\n\\nabla_{p}G & =\\int_{0}^{T}\\nabla_{p}g\\,dt.\n\\end{aligned} The left-hand side ∇pG\\nabla_p G is gradient of a scalar function of NN parameters, and hence the gradient is a vector of NN components. Correspondingly, the right-hand side is an integral of an NN-component gradient ∇pg\\nabla_p g as well, and the integral of a vector-valued function can be viewed as simply the elementwise integral (the vector of integrals of each component).\n\n\nReverse/adjoint sensitivity analysis of ODEs\nFor large N≫1N\\gg1 and scalar objectives gg or GG (etc.), we can in principle compute derivatives much more efficiently, with about the same cost as computing uu, by applying a “reverse-mode” or “adjoint” approach. In other lectures, we’ve obtained analogous reverse-mode methods simply by evaluating the chain rule left-to-right (outputs-to-inputs) instead of right-to-left. Conceptually, the process for ODEs is similar,13 but algebraically the derivation is rather trickier and less direct. The key thing is that, if possible, we want to avoid computing ∂u∂p\\frac{\\partial u}{\\partial p} explicitly, since this could be a prohibitively large Jacobian matrix if we have many parameters (pp is large), especially if we have many equations (uu is large).\nIn particular, let’s start with our forward-mode sensitivity analysis, and consider the derivative G′=(∇pG)TG'=(\\nabla_{p}G)^{T} where GG is the integral of a time-varying objective g(u,p,t)g(u,p,t) (which we allow to depend explicitly on pp for generality). By the chain rule, G′=∫0T(∂g∂p+∂g∂u∂u∂p)dt,G'=\\int_{0}^{T}\\left(\\frac{\\partial g}{\\partial p}+\\frac{\\partial g}{\\partial u}\\frac{\\partial u}{\\partial p}\\right)dt, which involves our unwanted factor ∂u∂p\\frac{\\partial u}{\\partial p}. To get rid of this, we’re going to use a “weird trick”\n(much like Lagrange multipliers) of adding zero to this equation: G′=∫0T[(∂g∂p+∂g∂u∂u∂p)+vT(∂∂t(∂u∂p)−∂f∂u∂u∂p−∂f∂p)⏟=0]dtG'=\\int_{0}^{T}\\left[\\left(\\frac{\\partial g}{\\partial p}+\\frac{\\partial g}{\\partial u}\\frac{\\partial u}{\\partial p}\\right)+v^{T}\\underbrace{\\left(\\frac{\\partial}{\\partial t}\\left(\\frac{\\partial u}{\\partial p}\\right)-\\frac{\\partial f}{\\partial u}\\frac{\\partial u}{\\partial p}-\\frac{\\partial f}{\\partial p}\\right)}_{=0}\\right]dt for some function v(t)v(t) of the same shape as u that multiplies our “forward-mode” equation for ∂u/∂p\\partial u/ \\partial p. (If u∈ℝnu\\in\\mathbb{R}^{n} then v∈ℝnv\\in\\mathbb{R}^{n}; more generally, for other vector spaces, read vTv^{T} as an inner product with vv.) The new term vT(⋯)v^T (\\cdots) is zero because the parenthesized expression is precisely the ODE satisfied by ∂u∂p\\frac{\\partial u}{\\partial p}, as obtained in our forward-mode analysis above, regardless of v(t)v(t). This is important because it allows us the freedom to choose v(t)v(t) to cancel the unwanted ∂u∂p\\frac{\\partial u}{\\partial p} term. In particular, if we first integrate by parts on the vT∂∂t(∂u∂p)v^{T}\\frac{\\partial}{\\partial t}\\left(\\frac{\\partial u}{\\partial p}\\right) term to change it to −(∂v∂t)T∂u∂p-\\left(\\frac{\\partial v}{\\partial t}\\right)^{T}\\frac{\\partial u}{\\partial p} plus a boundary term, then re-group the terms, we find: G′=vT∂u∂p|0T+∫0T[∂g∂p−vT∂f∂p+(∂g∂u−vT∂f∂u−(∂v∂t)T)⏟want to be zero!∂u∂p]dt.G'=\\left.v^{T}\\frac{\\partial u}{\\partial p}\\right|_{0}^{T}+\\int_{0}^{T}\\left[\\frac{\\partial g}{\\partial p}-v^{T}\\frac{\\partial f}{\\partial p}+\\underbrace{\\left(\\frac{\\partial g}{\\partial u}-v^{T}\\frac{\\partial f}{\\partial u}-\\left(\\frac{\\partial v}{\\partial t}\\right)^{T}\\right)}_{\\text{want to be zero!}}\\frac{\\partial u}{\\partial p}\\right]dt\\:. If we could now set the (⋯)(\\cdots) term to zero, then the unwanted ∂u∂p\\frac{\\partial u}{\\partial p} would vanish from the integral calculation in G′G'. We can accomplish this by choosing v(t)v(t) (which could be anything up to now) to satisfy the “adjoint” ODE: ∂v∂t=(∂g∂u)T−(∂f∂u)Tv.\\boxed{\\frac{\\partial v}{\\partial t}=\\left(\\frac{\\partial g}{\\partial u}\\right)^{T} - \\left(\\frac{\\partial f}{\\partial u}\\right)^{T}v}. What initial condition should we choose for v(t)v(t)? Well, we can use this choice to get rid of the boundary term we obtained above from integration by parts: vT∂u∂p|0T=v(T)T∂u∂p|T⏟unknown−v(0)T∂u0∂p⏟known.\\left.v^{T}\\frac{\\partial u}{\\partial p}\\right|_{0}^{T}=v(T)^{T}\\underbrace{\\left.\\frac{\\partial u}{\\partial p}\\right|_{T}}_{\\text{unknown}}-v(0)^{T}\\underbrace{\\frac{\\partial u_{0}}{\\partial p}}_{\\text{known}}. Here, the unknown ∂u∂p|T\\left.\\frac{\\partial u}{\\partial p}\\right|_{T} term is a problem—to compute that, we would be forced to go back to integrating our big ∂u∂p\\frac{\\partial u}{\\partial p} ODE from forward mode. The other term is okay: since the initial condition u0u_{0} is always given, we should know its dependence on pp explicitly (and we will simply have ∂u0∂p=0\\frac{\\partial u_{0}}{\\partial p}=0 in the common case where the initial conditions don’t depend on pp). To eliminate the ∂u∂p|T\\left.\\frac{\\partial u}{\\partial p}\\right|_{T} term, therefore, we make the choice v(T)=0.\\boxed{v(T)=0}. Instead of an initial condition, our adjoint ODE has a final condition. That’s no problem for a numerical solver: it just means that the adjoint ODE is integrated backwards in time, starting from t=Tt=T and working down to t=0t=0. Once we have solved the adjoint ODE for v(t)v(t), we can plug it into our equation for G′G' to obtain our gradient by a simple integral: ∇pG=(G′)T=−(∂u0∂p)Tv(0)+∫0T[(∂g∂p)T−(∂f∂p)Tv]dt.\\nabla_{p}G=\\left(G'\\right)^{T}=-\\left(\\frac{\\partial u_{0}}{\\partial p}\\right)^{T}v(0)+\\int_{0}^{T}\\left[\\left(\\frac{\\partial g}{\\partial p}\\right)^{T}-\\left(\\frac{\\partial f}{\\partial p}\\right)^{T}v\\right]dt\\:. (If you want to be fancy, you can compute this ∫0T\\int_{0}^{T} simultaneously with vv itself, by augmenting the adjoint ODE with an additional set of unknowns and equations representing the G′G' integrand. But that’s mainly just a computational convenience and doesn’t change anything fundamental about the process.)\nThe only remaining annoyance is that the adjoint ODE depends on u(p,t)u(p,t) for all t∈[0,T]t\\in[0,T]. Normally, if we are solving the “forward” ODE for u(p,t)u(p,t) numerically, we can “march” the solution uu forwards in time and only store the solution at a few of the most recent timesteps. Since the adjoint ODE starts at t=Tt=T, however, we can only start integrating vv after we have completed the calculation of uu. This requires us to save essentially all of our previously computed u(p,t)u(p,t) values, so that we can evaluate uu at arbitrary times t∈[0,T]t\\in[0,T] during the integration of vv (and G′G'). This can require a lot of computer memory if uu is large (e.g. it could represent millions of grid points from a spatially discretized PDE, such as in a heat-diffusion problem) and many timesteps tt were required. To ameliorate this challenge, a variety of strategies have been employed, typically centered around “checkpointing” techniques in which uu is only saved at a subset of times tt, and its value at other times is obtained during the vv integration by re-computing uu as needed (numerically integrating the ODE starting at the closest “checkpoint” time). A detailed discussion of such techniques lies outside the scope of these notes, however."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#example",
    "href": "matrix-calculus/en-US/index.html#example",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Example",
    "text": "Example\nLet us illustrate the above techniques with a simple example. Suppose that we are integrating the scalar ODE ∂u∂t=f(u,p,t)=p1+p2u+p3u2=pT(1uu2)\\frac{\\partial u}{\\partial t}=f(u,p,t)=p_{1}+p_{2}u+p_{3}u^{2}=p^{T}\\left(\\begin{array}{c}\n1\\\\\nu\\\\\nu^{2}\n\\end{array}\\right) for an initial condition u(p,0)=u0=0u(p,0)=u_{0}=0 and three parameters p∈ℝ3p \\in \\mathbb{R}^3. (This is probably simple enough to solve in closed form, but we won’t bother with that here.) We will also consider the scalar function G(p)=∫0T[u(p,t)−u*(t)]2⏟g(u,p,t)dtG(p)=\\int_{0}^{T}\\underbrace{\\left[u(p,t)-u_{*}(t)\\right]^{2}}_{g(u,p,t)}dt that (for example) we may want to minimize for some given u*(t)u_{*}(t) (e.g. experimental data or some given formula like u*=t3u_{*}=t^{3}), so we are hoping to compute ∇pG\\nabla_{p}G.\n\nForward mode\nThe Jacobian matrix ∂u∂p=(∂u∂p1∂u∂p2∂u∂p3)\\frac{\\partial u}{\\partial p}=\\left(\\begin{array}{ccc}\n\\frac{\\partial u}{\\partial p_{1}} & \\frac{\\partial u}{\\partial p_{2}} & \\frac{\\partial u}{\\partial p_{3}}\\end{array}\\right) is simply a row vector, and satisfies our “forward-mode” ODE: ∂∂t(∂u∂p)=∂f∂u∂u∂p+∂f∂p=(p2+2p3u)∂u∂p+(1uu2)\\frac{\\partial}{\\partial t}\\left(\\frac{\\partial u}{\\partial p}\\right)=\\frac{\\partial f}{\\partial u}\\frac{\\partial u}{\\partial p}+\\frac{\\partial f}{\\partial p}=\\left(p_{2}+2p_{3}u\\right)\\frac{\\partial u}{\\partial p}+\\left(\\begin{array}{ccc}\n1 & u & u^{2}\\end{array}\\right) for the initial condition ∂u∂p|t=0=∂u0∂p=0\\left.\\frac{\\partial u}{\\partial p}\\right|_{t=0}=\\frac{\\partial u_{0}}{\\partial p}=0. This is an inhomogeneous system of three coupled linear ODEs, which might look more conventional if we simply transpose both sides: ∂∂t(∂u∂p1∂u∂p2∂u∂p3)⏟(∂u/∂p)T=(p2+2p3u)(∂u∂p1∂u∂p2∂u∂p3)+(1uu2).\\frac{\\partial}{\\partial t}\\underbrace{\\left(\\begin{array}{c}\n\\frac{\\partial u}{\\partial p_{1}}\\\\\n\\frac{\\partial u}{\\partial p_{2}}\\\\\n\\frac{\\partial u}{\\partial p_{3}}\n\\end{array}\\right)}_{(\\partial u/\\partial p)^{T}}=\\left(p_{2}+2p_{3}u\\right)\\left(\\begin{array}{c}\n\\frac{\\partial u}{\\partial p_{1}}\\\\\n\\frac{\\partial u}{\\partial p_{2}}\\\\\n\\frac{\\partial u}{\\partial p_{3}}\n\\end{array}\\right)+\\left(\\begin{array}{c}\n1\\\\\nu\\\\\nu^{2}\n\\end{array}\\right). The fact that this depends on our “forward” solution u(p,t)u(p,t) makes it not so easy to solve by hand, but a computer can solve it numerically with no difficulty. On a computer, we would probably solve for uu and ∂u/∂p\\partial u/ \\partial psimultaneously by combining the two ODEs into a single ODE with 4 components: ∂∂t(u(∂u/∂p)T)=(p1+p2u+p3u2(p2+2p3u)(∂u/∂p)T+(1uu2)).\\frac{\\partial}{\\partial t}\\left(\\begin{array}{c}\nu \\\\\n(\\partial u/\\partial p)^{T}\n\\end{array}\\right) =\n\\left(\\begin{array}{c}\np_{1}+p_{2}u+p_{3}u^{2} \\\\\n\\left(p_{2}+2p_{3}u\\right) (\\partial u/\\partial p)^{T} +\\left(\\begin{array}{c}\n1\\\\\nu\\\\\nu^{2}\n\\end{array}\\right)\n\\end{array}\\right). Given ∂u/∂p\\partial u/\\partial p, we can then plug this into the chain rule for GG: ∇pG=2∫0T[u(p,t)−u*(t)]∂u∂pTdt\\nabla_{p}G=2\\int_{0}^{T}\\left[u(p,t)-u_{*}(t)\\right]\\frac{\\partial u}{\\partial p}^{T}\\,dt (again, an integral that a computer could evaluate numerically).\n\n\nReverse mode\nIn reverse mode, we have an adjoint solution v(t)∈ℝv(t)\\in\\mathbb{R} (the same shape as uu) which solves our adjoint equation ∂vdt=(∂g∂u)T−(∂f∂u)Tv=2[u(p,t)−u*(t)]−(p2+2p3u)v\\frac{\\partial v}{dt}=\\left(\\frac{\\partial g}{\\partial u}\\right)^{T} -\\left(\\frac{\\partial f}{\\partial u}\\right)^{T}v =2\\left[u(p,t)-u_{*}(t)\\right] - \\left(p_{2}+2p_{3}u\\right)v with a final condition v(T)=0.v(T)=0. Again, a computer can solve this numerically without difficulty (given the numerical “forward” solution uu) to find v(t)v(t) for t∈[0,T]t\\in[0,T]. Finally, our gradient is the integrated product: ∇pG=−∫0T(1uu2)vdt.\\nabla_{p}G = -\\int_{0}^{T}\n\\left(\n\\begin{array}{c} 1 \\\\ u \\\\ u^{2} \\end{array}\n\\right)\nv\\,dt\\:.\nAnother useful exercise is to consider a GG that takes the form of a summation:\n\n Suppose that G(p)G(p) takes the form of a sum of KK terms: G(p)=∑k=1Kgk(p,u(p,tk))G(p) = \\sum_{k=1}^{K} g_k(p,u(p,t_k)) for times tk∈(0,T)t_k \\in (0, T) and functions gk(p,u)g_k(p,u). For example, this could arise in least-square fitting of experimental data u*(tk)u_*(t_k) at KK discrete times, with gk(u(p,tk))=‖u*(tk)−u(p,tk)‖2g_k(u(p,t_k)) = \\Vert u_*(t_k) -u(p,t_k)\\Vert^2 measuring the squared difference between u(p,tk)u(p,t_k) and the measured data at time tkt_k.\n\nShow that such a G(p)G(p) can be expressed as a special case of our formulation in this chapter, by defining our function g(u,t)g(u,t) using a sum of Dirac delta functions δ(t−tk)\\delta(t - t_k).\nExplain how this affects the adjoint solution v(t)v(t): in particular, how the introduction of delta-function terms on the right-hand side of dv/dtdv/dt causes v(t)v(t) to have a sequence of discontinuous jumps. (In several popular numerical ODE solvers, such discontinuities can be incorporated via discrete-time “callbacks”.)\nExplain how these delta functions may also introduce a summation into the computation of ∇pG\\nabla_p G, but only if gkg_k depends explicitly on pp (not just via uu)."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#further-reading",
    "href": "matrix-calculus/en-US/index.html#further-reading",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Further reading",
    "text": "Further reading\nA classic reference on reverse/adjoint differentiation of ODEs (and generalizations thereof), using notation similar to that used today (except that the adjoint solution vv is denoted λ(t)\\lambda(t), in an homage to Lagrange multipliers), is Cao et al. (2003) (https://doi.org/10.1137/S1064827501380630), and a more recent review article is Sapienza et al. (2024) (https://arxiv.org/abs/2406.09699). See also the SciMLSensitivity.jl package (https://github.com/SciML/SciMLSensitivity.jl) for sensitivity analysis with Chris Rackauckas’s amazing DifferentialEquations.jl software suite for numerical solution of ODEs in Julia. There is a nice 2021 YouTube lecture on adjoint sensitivity of ODEs (https://youtu.be/k6s2G5MZv-I), again using a similar notation. A discrete version of this process arises for recurrence relations, in which case one obtains a reverse-order “adjoint” recurrence relation as described in MIT course notes by S. G. Johnson (https://math.mit.edu/~stevenj/18.336/recurrence2.pdf).\nThe differentiation methods in this chapter (e.g. for ∂u/∂p\\partial u/\\partial p or ∇pG\\nabla_p G) are derived assuming that the ODEs are solved exactly: given the exact ODE for uu, we derived an exact ODE for the derivative. On a computer, you will solve these forward and adjoint ODEs approximately, and in consequence the resulting derivatives will only be approximately correct (to the tolerance specified by your ODE solver). This is known as a differentiate-then-discretize approach, which has the advantage of simplicity (it is independent of the numerical solution scheme) at the expense of slight inaccuracy (your approximate derivative will not exactly predict the first-order change in your approximate solution uu). The alternative is a discretize-then-differentiate approach, in which you first approximate (“discretize”) your ODE into a discrete-time recurrence formula, and then exactly differentiate the recurrence. This has the advantage of exactly differentiating your approximate solution, at the expense of complexity (the derivation is specific to your discretization scheme). Various authors discuss these tradeoffs and their implications, e.g. in chapter 4 of M. D. Gunzburger’s Perspectives in Flow Control and Optimization (2002) or in papers like Jensen et al. (2014)."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#functionals-mapping-functions-to-scalars",
    "href": "matrix-calculus/en-US/index.html#functionals-mapping-functions-to-scalars",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Functionals: Mapping functions to scalars",
    "text": "Functionals: Mapping functions to scalars\n\nFor example, consider functions u(x)u(x) that map $x\\in [0,1] \\to u(x) \\in \\R$. We may then define the function ff: f(u)=∫01sin(u(x))dx.f(u) = \\int_0^1 \\sin (u(x)) \\,\\mathrm{d} x. Such a function, mapping an input function uu to an output number, is sometimes called a “functional.” What is f′f' or ∇f\\nabla f in this case?\n\nRecall that, given any function ff, we always define the derivative as a linear operator f′(u)f'(u) via the equation: df=f(u+du)−f(u)=f′(u)[du],df = f (u + du ) - f(u) = f'(u) [du] \\, , where now dudu denotes an arbitrary “small-valued” function du(x)du(x) that represents a small change in u(x)u(x), as depicted in Fig. 7 for the analogous case of a non-infinitesimal δu(x)\\delta u(x). Here, we may compute this via linearization of the integrand: df=f(u+du)−f(u)=∫01sin(u(x)+du(x))−sin(u(x))dx=∫01cos(u(x))du(x)dx=f′(u)[du],\\begin{aligned}\n    df &= f(u+ du) - f(u) \\\\ \n     &=\\int_0^1 \\sin (u(x) + du(x)) - \\sin (u(x)) \\, dx \\\\\n     &= \\int_0^1 \\cos (u(x)) \\, du(x) \\, dx = f'(u) [du] \\, ,\n\\end{aligned} where in the last step we took du(x)du(x) to be arbitrarily small15 so that we could linearize sin(u+du)\\sin(u + du) to first-order in du(x)du(x). That’s it, we have our derivative f′(u)f'(u) as a perfectly good linear operation acting on dudu!\n\n\n\nIf our f(u)f(u)’s inputs uu are functions u(x)u(x) (e.g., mapping [0,1]↦ℝ[0,1] \\mapsto \\mathbb{R}), then the essence of differentiation is linearizing ff for small perturbations δu(x)\\delta u(x) that are themselves functions, in the limit where δu(x)\\delta u(x) becomes arbitrarily small. Here, we show an example of a u(x)u(x) and a perturbation u(x)+δu(x)u(x)+\\delta u(x)."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#inner-products-of-functions",
    "href": "matrix-calculus/en-US/index.html#inner-products-of-functions",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Inner products of functions",
    "text": "Inner products of functions\nIn order to define a gradient ∇f\\nabla f when studying such “functionals” (maps from functions to $\\R$), it is natural to ask if there is an inner product on the input space. In fact, there are perfectly good ways to define inner products of functions! Given functions u(x),v(x)u(x), v(x) defined on x∈[0,1]x\\in [0,1], we could define a “Euclidean” inner product: ⟨u,v⟩=∫01u(x)v(x)dx.\\langle u, v \\rangle = \\int_0^1 u(x) v(x) \\,\\mathrm dx. Notice that this implies ‖u‖:=⟨u,u⟩=∫01u(x)2dx.\\lVert u \\rVert := \\sqrt{\\langle u, u \\rangle} = \\sqrt{\\int_0^1 u(x)^2 dx} \\, .\nRecall that the gradient ∇f\\nabla f is defined as whatever we take the inner product of dudu with to obtain dfdf. Therefore, we obtain the gradient as follows: df=f′(u)[du]=∫01cos(u(x))du(x)dx=⟨∇f,du⟩⟹∇f=cos(u(x)).df = f'(u)[du]  = \\int_0^1 \\cos(u(x)) \\,du(x)\\, dx  = \\langle \\nabla f, du \\rangle \\implies \\nabla f = \\cos (u(x)) \\, . The two infinitesimals dudu and dxdx may seem a bit disconcerting, but if this is confusing you can just think of the du(x)du(x) as a small non-infinitesimal function δu(x)\\delta u(x) (as in Fig. 7) for which we are dropping higher-order terms.\nThe gradient ∇f\\nabla f is just another function, cos(u(x))\\cos(u(x))! As usual, ∇f\\nabla f has the same “shape” as uu.\n\nRemark. It might be instructive here to compare the gradient of an integral, above, with a discretized version where the integral is replaced by a sum. If we have f(u)=∑k=1nsin(uk)Δxf(u) = \\sum_{k=1}^n \\sin(u_k) \\Delta x \\, where Δx=1/n\\Delta x = 1/n, for a vector u∈ℝnu \\in \\mathbb{R}^n, related to our previous u(x)u(x) by uk=u(kΔx)u_k = u(k\\Delta x), which can be thought of as a “rectangle rule” (or Riemann sum, or Euler) approximation for the integral. Then, ∇uf=(cos(u1)cos(u2)⋮)Δx.\\nabla_u f = \\begin{pmatrix} \\cos(u_1) \\\\ \\cos(u_2) \\\\ \\vdots \\end{pmatrix} \\Delta x  \\, . Why does this discrete version have a Δx\\Delta x multiplying the gradient, whereas our continuous version did not? The reason is that in the continuous version we effectively included the dxdx in the definition of the inner product ⟨u,v⟩\\langle u, v \\rangle (which was an integral). In discrete case, the ordinary inner product (used to define the conventional gradient) is just a sum without a Δx\\Delta x. However, if we define a weighted discrete inner product ⟨u,v⟩=∑k=1nukvkΔx\\langle u, v \\rangle = \\sum_{k=1}^n u_k v_k \\Delta x, then, according to Sec. 5, this changes the definition of the gradient, and in fact will remove the Δx\\Delta x term to correspond to the continuous version."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#example-minimizing-arc-length",
    "href": "matrix-calculus/en-US/index.html#example-minimizing-arc-length",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Example: Minimizing arc length",
    "text": "Example: Minimizing arc length\nWe now consider a more tricky example with an intuitive geometric interpretation.\n\nLet uu be a differentiable function on [0,1][0,1] and consider the functional f(u)=∫011+u′(x)2dx.f(u) = \\int_0^1 \\sqrt{1+ u'(x)^2}\\,dx. Solve for ∇f\\nabla f when u(0)=u(1)=0.u(0) = u(1) = 0.\n\nGeometrically, you learned in first-year calculus that this is simply the length of the curve u(x)u(x) from x=0x=0 to x=1x=1. To differentiate this, first notice that ordinary single-variable calculus gives us the linearization d(1+v2)=1+(v+dv)2−1+v2=(1+v2)′dv=v1+v2dv.d\\left( \\sqrt{1 + v^2} \\right) = \\sqrt{1+ (v + dv)^2} - \\sqrt{1 + v^2} = \\left( \\sqrt{1 + v^2} \\right)' dv =  \\frac{v}{\\sqrt{1+ v^2}} dv \\, . Therefore, df=f(u+du)−f(u)=∫01(1+(u+du)′2−1+u′2)dx=∫01u′1+u′2du′dx.\\begin{aligned}\n   df &= f(u+ du) - f(u) \\\\\n   &= \\int_0^1 \\left( \\sqrt{1 + (u+du)'^2} - \\sqrt{1 + u'^2} \\right) dx \\\\\n   &= \\int_0^1 \\frac{u'}{\\sqrt{1 + u'^2}} \\, du'  dx.\n\\end{aligned} However, this is a linear operator on du′du' and not (directly) on dudu. Abstractly, this is fine, because du′du' is itself a linear operation on dudu, so we have f′(u)[du]f'(u)[du] as the composition of two linear operations. However, it is more revealing to rewrite it explicitly in terms of dudu, for example in order to define ∇f\\nabla f. To accomplish this, we can apply integration by parts to obtain f′(u)[du]=∫01u′1+u′2du′dx=u′1+u′2du|01−∫01(u′1+u′2)′dudx.f'(u) [du] = \\int_0^1 \\frac{u'}{\\sqrt{1 + u'^2}} \\, du' dx = \\left. \\frac{u'}{\\sqrt{1 + u'^2}} du \\right|_0^1 - \\int_0^1 \\left(\\frac{u'}{\\sqrt{1 + u'^2}}\\right)' \\, du \\, dx \\, .\nNotice that up until now we did not need utilize the “boundary conditions” u(0)=u(1)=0u(0) = u(1) = 0 for this calculation. However, if we want to restrict ourselves to such functions u(x)u(x), then our perturbation dudu cannot change the endpoint values, i.e. we must have du(0)=du(1)=0du(0) = du(1) = 0. (Geometrically, suppose that we want to find the uu that minimizes arc length between (0,0)(0,0) and (1,0)(1,0), so that we need to fix the endpoints.) This implies that the boundary term in the above equation is zero. Hence, we have that df=−∫01(u′1+u′2)′⏟∇fdudx=⟨∇f,du⟩.df = -\\int_0^1 \\underbrace{\\left(\\frac{u'}{\\sqrt{1 + u'^2}}\\right)'}_{\\nabla f} \\, du \\, dx = \\langle \\nabla f , du \\rangle \\, .\nFurthermore, note that the uu that minimizes the functional ff has the property that ∇f|u=0\\left. \\nabla f \\right|_u = 0. Therefore, for a uu that minimizes the functional ff (the shortest curve), we must have the following result: 0=∇f=−(u′1+u′2)′=−u″1+u′2−u′u″u′1+u′21+u′2=−u″(1+u′2)−u″u′2(1+u′2)3/2=−u″(1+u′2)3/2.\\begin{aligned}\n    0 = \\nabla f &= -\\left(\\frac{u'}{\\sqrt{1 + u'^2}}\\right)' \\\\\n    &= -\\frac{u'' \\sqrt{1 + u'^2} - u' \\frac{u'' u '}{\\sqrt{1 + u'^2}}}{ 1 + u'^2} \\\\\n    &= -\\frac{u'' ( 1 + u'^2) - u'' u'^2}{(1 + u'^2)^{3/2}} \\\\\n    &= -\\frac{u''}{( 1+ u'^2)^{3/2}}.\n\\end{aligned} Hence, ∇f=0⟹u″(x)=0⟹u(x)=ax+b\\nabla f = 0 \\implies u''(x) = 0 \\implies u(x) = ax + b for constants a,ba,b; and for these boundary conditions a=b=0a=b=0. In other words, uu is the horizontal straight line segment!\nThus, we have recovered the familiar result that straight line segments in $\\R^2$ are the shortest curves between two points!\n\nRemark. Notice that the expression u″(1+u′2)3/2\\frac{u''}{(1+ u'^2)^{3/2}} is the formula from multivariable calculus for the curvature of the curve defined by y=u(x)y= u(x). It is not a coincidence that the gradient of arc length is the (negative) curvature, and the minimum arc length occurs for zero gradient = zero curvature."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#eulerlagrange-equations",
    "href": "matrix-calculus/en-US/index.html#eulerlagrange-equations",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Euler–Lagrange equations",
    "text": "Euler–Lagrange equations\nThis style of calculation is part of the subject known as the calculus of variations. Of course, the final answer in the example above (a straight line) may have been obvious, but a similar approach can be applied to many more interesting problems. We can generalize the approach as follows:\n\nLet f(u)=∫abF(u,u′,x)dxf(u) = \\int_a^b F(u, u', x) \\, dx where uu is a differentiable function on [a,b][a,b]. Suppose the endpoints of uu are fixed (i.e. its values at x=ax=a and x=bx=b are constants). Let us calculate dfdf and ∇f\\nabla f.\n\nWe find: df=f(u+du)−f(u)=∫ab(∂F∂udu+∂F∂u′du′)dx=∂F∂u′du|ab⏟=0+∫ab(∂F∂u−(∂F∂u′)′)dudx,\\begin{aligned}\n    df &= f(u + du) - f(u) \\\\\n    &= \\int_a^b \\left(\\frac{\\partial F}{\\partial u} du  + \\frac{\\partial F}{\\partial u'} du' \\right) dx \\\\\n    &= \\underbrace{\\frac{\\partial F}{\\partial u'} du \\bigr|_a^b}_{= 0} + \\int_a^b \\left( \\frac{\\partial F}{\\partial u} - \\left(\\frac{\\partial F}{\\partial u'}\\right)'\\right) \\, du \\, dx \\, ,\n\\end{aligned} where we used the fact that du=0du = 0 at aa or bb if the endpoints u(a)u(a) and u(b)u(b) are fixed. Hence, ∇f=∂F∂u−(∂F∂u′)′,\\nabla f = \\frac{\\partial F}{\\partial u} - \\left(\\frac{\\partial F}{\\partial u'}\\right)', which equals zero at extremum. Notice that ∇f=0\\nabla f = 0 yields a second-order differential equation in uu, known as the Euler–Lagrange equations!\n\nRemark. The notation ∂F/∂u′\\partial F / \\partial u' is a notoriously confusing aspect of the calculus of variations—what does it mean to take the derivative “with respect to u′u'” while holding uu fixed? A more explicit, albeit more verbose, way of expressing this is to think of F(u,v,x)F(u,v,x) as a function of three unrelated arguments, for which we only substitute v=u′v=u' after differentiating with respect to the second argument vv: ∂F∂u′=∂F∂v|v=u′.\\frac{\\partial F}{\\partial u'} = \\left. \\frac{\\partial F}{\\partial v} \\right|_{v=u'} \\, .\n\nThere are many wonderful applications of this idea. For example, search online for information about the “brachistochrone problem” (animated here) and/or the “principle of least action”. Another example is a catenary curve, which minimizes the potential energy of a hanging cable. A classic textbook on the topic is Calculus of Variations by Gelfand and Fomin."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#introduction",
    "href": "matrix-calculus/en-US/index.html#introduction",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Introduction",
    "text": "Introduction\nIn this class, we’ve learned how to take derivatives of all sorts of crazy functions. Recall one of our first examples: f(A)=A2,f(A) = A^2, where AA is a matrix. To differentiate this function, we had to go back to the drawing board, and ask:\n\nIf we perturb the input slightly, how does the output change? \n\nTo this end, we wrote down something like: δf=(A+δA)2−A2=A(δA)+(δA)A+(δA)2⏟neglected.\\delta f = (A+\\delta A)^2 - A^2 = A (\\delta A) + (\\delta A) A + \\underbrace{(\\delta A)^2}_{\\text{neglected}}. We called δf\\delta f and δA\\delta A differentials in the limit where δA\\delta A became arbitrarily small. We then had to ask:\n\nWhat terms in the differential can we neglect? \n\nWe decided that (δA)2(\\delta A)^2 should be neglected, justifying this by the fact that (δA)2(\\delta A)^2 is “higher-order”. We were left with the derivative operator δA↦A(δA)+(δA)A\\delta A \\mapsto\nA(\\delta A) + (\\delta A) A: the best possible linear approximation to ff in a neighbourhood of AA. At a high level, the main challenge here was dealing with complicated input and output spaces: ff was matrix-valued, and also matrix-accepting. We had to ask ourselves: in this case, what should the notion of a derivative even mean?\nIn this lecture, we will face a similar challenge, but with an even weirder type of function. This time, the output of our function will be random. Now, we need to revisit the same questions. If the output is random, how can we describe its response to a change in the input? And how can we form a useful notion of derivative?"
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#stochastic-programs",
    "href": "matrix-calculus/en-US/index.html#stochastic-programs",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Stochastic programs",
    "text": "Stochastic programs\nMore precisely, we will consider random, or stochastic, functions XX with real input p∈ℝp \\in \\mathbb{R} and real-valued random-variable output. As a map, we can write XX as p↦X(p),p \\mapsto X(p), where X(p)X(p) is a random variable. (To keep things simple, we’ll take p∈ℝp \\in \\mathbb{R} and X(p)∈ℝX(p) \\in \\mathbb{R} in this chapter, though of course they could be generalized to other vector spaces as in the other chapters. For now, the randomness is complicated enough to deal with.)\nThe idea is that we can only sample from X(p)X(p), according to some distribution of numbers with probabilities that depend upon pp. One simple example would be sampling real numbers uniformly (equal probabilities) from the interval [0,p][0,p]. As a more complicated example, suppose X(p)X(p) follows the exponential distribution with scale pp, corresponding to randomly sampled real numbers x≥0x \\ge 0 whose probability decreases proportional to e−x/pe^{-x/p}. This can be denoted X(p)∼Exp(p)X(p) \\sim \\operatorname{Exp}(p), and implemented in Julia by:\njulia&gt; using Distributions\n\njulia&gt; sample_X(p) = rand(Exponential(p))\nsample_X (generic function with 1 method)\nWe can take a few samples:\njulia&gt; sample_X(10.0)\n1.7849785709142214\n\njulia&gt; sample_X(10.0)\n4.435847397169775\n\njulia&gt; sample_X(10.0)\n0.6823343897949835\n\njulia&gt; mean(sample_X(10.0) for i = 1:10^9) # mean = p\n9.999930348291866\nIf our program gives a different output each time, what could a useful notion of derivative be? Before we try to answer this, let’s ask why we might want to take a derivative. The answer is that we may be very interested in statistical properties of random functions, i.e. values that can be expressed using averages. Even if a function is stochastic, its average (“expected value”), assuming the average exists, can be a deterministic function of its parameters that has a conventional derivative.\nSo, why not take the average first, and then take the ordinary derivative of this average? This simple approach works for very basic stochastic functions (e.g. the exponential distribution above has expected value pp, with derivative 11), but runs into practical difficulties for more complicated distributions (as are commonly implemented by large computer programs working with random numbers).\n\nRemark. It is often much easier to produce an “unbiased estimate” X(p)X(p) of a statistical quantity than to compute it exactly. (Here, an unbiased estimate means that X(p)X(p) averages out to our statistical quantity of interest.)\n\nFor example, in deep learning, the “variational autoencoder” (VAE) is a very common architecture that is inherently stochastic. It is easy to get a stochastic unbiased estimate of the loss function by running a random simulation X(p)X(p): the loss function L(p)L(p) is then the “average” value of X(p)X(p), denoted by the expected value $\\EE[X(p)]$. However, computing the loss L(p)L(p) exactly would require integrating over all possible outcomes, which usually is impractical. Now, to train the VAE, we also need to differentiate L(p)L(p), i.e. differentiate $\\EE[X(p)]$ with respect to pp!\nPerhaps more intuitive examples can be found in the physical sciences, where randomness may be baked into your model of a physical process. In this case, it’s hard to get around the fact that you need to deal with stochasticity! For example, you may have two particles that interact with an average rate of rr. But in reality, the times when these interactions actually occur follow a stochastic process. (In fact, the time until the first interaction might be exponentially distributed, with scale 1/r1/r.) And if you want to (e.g.) fit the parameters of your stochastic model to real-world data, it’s once again very useful to have derivatives.\nIf we can’t compute our statistical quantity of interest exactly, it seems unreasonable to assume we can compute its derivative exactly. However, we could hope to stochastically estimate its derivative. That is, if X(p)X(p) represents the full program that produces an unbiased estimate of our statistical quantity, here’s one property we’d definitely like our notion of derivative to have: we should be able to construct from it an unbiased gradient estimator16 X′(p)X'(p) satisfying $$\\EE[X'(p)] = \\EE[X(p)]' = \\frac{\\partial \\EE[X(p)]}{\\partial p}.$$ Of course, there are infinitely many such estimators. For example, given any estimator X′(p)X'(p) we can add any other random variable that has zero average without changing the expectation value. But in practice there are two additional considerations: (1) we want X′(p)X'(p) to be easy to compute/sample (about as easy as X(p)X(p)), and (2) we want the variance (the “spread”) of X′(p)X'(p) to be small enough that we don’t need too many samples to estimate its average accurately (hopefully no worse than estimating $\\EE[X(p)]$)."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#stochastic-differentials-and-the-reparameterization-trick",
    "href": "matrix-calculus/en-US/index.html#stochastic-differentials-and-the-reparameterization-trick",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Stochastic differentials and the reparameterization trick",
    "text": "Stochastic differentials and the reparameterization trick\nLet’s begin by answering our first question (Question [question:perturb]): how does X(p)X(p) respond to a change in pp? Let us consider a specific pp and write down a stochastic differential, taking a small but non-infinitesimal δp\\delta p to avoid thinking about infinitesimals for now: δX(p)=X(p+δp)−X(p),\\delta X(p) = X(p + \\delta p) - X(p), where δp\\delta p represents an arbitrary small change in pp. What sort of object is δX(p)\\delta X(p)?\nSince we’re subtracting two random variables, it ought to itself be a random variable. However, δX(p)\\delta X(p) is still not fully specified! We have only specified the marginal distributions of X(p)X(p) and X(p+δp)X(p+\\delta p): to be able to subtract the two, we need to know their joint distribution.\nOne possibility is to treat X(p)X(p) and X(p+δp)X(p + \\delta p) as independent. This means that δX(p)\\delta X(p) would be constructed as the difference of independent samples. Let’s see how samples from δX(p)\\delta X(p) would look like in this case!\njulia&gt; sample_X(p) = rand(Exponential(p))\nsample_X (generic function with 1 method)\n\njulia&gt; sample_δX(p, δp) = sample_X(p + δp) - sample_X(p)\nsample_δX (generic function with 1 method)\n\njulia&gt; p = 10; δp = 1e-5;\n\njulia&gt; sample_δX(p, δp)\n-26.000938718875904\n\njulia&gt; sample_δX(p, δp)\n-2.6157162001718092\n\njulia&gt; sample_δX(p, δp)\n6.352622554495474\n\njulia&gt; sample_δX(p, δp)\n-9.53215951927184\n\njulia&gt; sample_δX(p, δp)\n1.2232268930932104\nWe can observe something a bit worrying: even for a very tiny δp\\delta p (we chose δp=10−5\\delta p = 10^{-5}), δX(p)\\delta X(p) is still fairly large: essentially as large as the original random variables. This is not good news if we want to construct a derivative from δX(p)\\delta X(p): we would rather see its magnitude getting smaller and smaller with δp\\delta p, like in the non-stochastic case. Computationally, this will make it very difficult to determine $\\EE[X(p)]'$ by averaging sample_δX(p, δp) / δp over many samples: we’ll need a huge number of samples because the variance, the “spread” of random values, is huge for small δp\\delta p.\nLet’s try a different approach. It is natural to think of X(p)X(p) for all pp as forming a family of random variables, all defined on the same probability space. A probability space, with some simplification, is a sample space Ω\\Omega, with a probability distribution ℙ\\mathbb{P} defined on the sample space. From this point of view, each X(p)X(p) can be expressed as a function Ω→ℝ\\Omega \\to \\mathbb{R}. To sample from a particular X(p)X(p), we can imagine drawing a random ω\\omega from Ω\\Omega according to $\\PP$, and then plugging this into X(p)X(p), i.e. computing X(p)(ω)X(p)(\\omega). (Computationally, this is how most distributions are actually implemented: you start with a primitive pseudo-random number generator for a very simple distribution,17 e.g. drawing values ω\\omega uniformly from Ω=[0,1)\\Omega = [0,1), and then you build other distributions on top of this by transforming ω\\omega somehow.) Intuitively, all of the “randomness” resides in the probability space, and crucially ℙ\\mathbb{P} does not depend on pp: as pp varies, X(p)X(p) just becomes a different deterministic map on Ω\\Omega.\nThe crux here is that all the X(p)X(p) functions now depend on a shared source of randomness: the random draw of ω\\omega. This means that X(p)X(p) and X(p+δp)X(p+\\delta p) have a nontrivial joint distribution: what does it look like?\nFor concreteness, let’s study our exponential random variable X(p)∼Exp(p)X(p) \\sim \\operatorname{Exp}(p) from above. Using the “inversion sampling” parameterization, it is possible to choose Ω\\Omega to be [0,1)[0,1) and ℙ\\mathbb{P} to be the uniform distribution over Ω\\Omega; for any distribution, we can construct X(p)X(p) to be a corresponding nondecreasing function over Ω\\Omega (given by the inverse of X(p)X(p)’s cumulative probability distribution). Applied to X(p)∼Exp(p)X(p) \\sim \\operatorname{Exp}(p), the inversion method gives X(p)(ω)=−plog(1−ω)X(p)(\\omega) = -p \\log{(1-\\omega)}. This is implemented below, and is a theoretically equivalent way of sampling X(p)X(p) compared with the opaque rand(Exponential(p)) function we used above:\njulia&gt; sample_X2(p, ω) = -p * log(1 - ω)\nsample_X2 (generic function with 1 method)\n\njulia&gt; # rand() samples a uniform random number in [0,1)\njulia&gt; sample_X2(p) = sample_X2(p, rand()) \nsample_X2 (generic function with 2 methods)\n\njulia&gt; sample_X2(10.0)\n8.380816941818618\n\njulia&gt; sample_X2(10.0)\n2.073939134369733\n\njulia&gt; sample_X2(10.0)\n29.94586208847568\n\njulia&gt; sample_X2(10.0)\n23.91658360124792\nOkay, so what does our joint distribution look like?\n\n\n\nFor X(p)∼Exp(p)X(p) \\sim \\operatorname{Exp}(p) parameterized via the inversion method, we can write X(p)X(p), X(p+δp)X(p+\\delta p), and δX(p)\\delta X(p) as functions from Ω=[0,1]→ℝ\\Omega = [0,1] \\to \\mathbb{R}, defined on a probability space with ℙ=Unif(0,1)\\mathbb{P} = \\operatorname{Unif}(0,1).\n\n\nAs shown in Figure 8, we can plot X(p)X(p) and X(p+δp)X(p+\\delta p) as functions over Ω\\Omega. To sample the two of them jointly, we use the same choice of ω\\omega: thus, δX(p)\\delta X(p) can be formed by subtracting the two functions pointwise at each Ω\\Omega. Ultimately, δX(p)\\delta X(p) is itself a random variable over the same probability space, sampled in the same way: we pick a random ω\\omega according to ℙ\\mathbb{P}, and evaluate δX(p)(ω)\\delta X(p)(\\omega), using the function δX(p)\\delta X(p) depicted above. Our first approach with independent samples is depicted in red in Figure 8, while our second approach is in blue. We can now see the flaw of the independent-samples approach: the 𝒪(1)\\mathcal{O}(1)-sized “noise” from the independent samples washes out the 𝒪(δp)\\mathcal{O}(\\delta p)-sized “signal”.\nWhat about our second question (Question [question:neglect]): how can actually take the limit of δp→0\\delta p \\to 0 and compute the derivative? The idea is to differentiate δX(p)\\delta X(p) at each fixed sample ω∈Ω\\omega \\in \\Omega. In probability theory terms, we take the limit of random variables δX(p)/δp\\delta X(p) / \\delta p as δp→0\\delta p \\to 0: X′(p)=limδp→0δX(p)δp.X'(p) = \\lim_{\\delta p \\to 0} \\frac{\\delta X(p)}{\\delta p}. For X(p)∼Exp(p)X(p) \\sim \\operatorname{Exp}(p) parameterized via the inversion method, we get: X′(p)(ω)=limδp→0−δplog(1−ω)δp=−log(1−ω).X'(p)(\\omega) = \\lim_{\\delta p \\to 0} \\frac{-\\delta p\\log{(1-\\omega)}}{\\delta p} = -\\log{(1-\\omega)}. Once again, X′(p)X'(p) is a random variable over the same probability space. The claim is that X′(p)X'(p) is the notion of derivative we were looking for! Indeed, X′(p)X'(p) is itself in fact a valid gradient estimator: $$\\EE[X'(p)] = \\EE\\left[ \\lim_{\\delta p \\to 0} \\frac{\\delta X(p)}{\\delta p}\\right] \\stackrel{?}{=}\n\\lim_{\\delta p \\to 0} \\frac{\\EE[\\delta X(p)]}{\\delta p} =\n    \\frac{\\partial \\EE[X(p)]}{\\partial p}.\n    \\label{eq:exchange}$$ Rigorously, one needs to justify the interchange of limit and expectation in the above. In this chapter, however, we will be content with a crude empirical justification:\njulia&gt; X′(p, ω) = -log(1 - ω)\nX′ (generic function with 1 method)\n\njulia&gt; X′(p) = X′(p, rand())\nX′ (generic function with 2 methods)\n\njulia&gt; mean(X′(10.0) for i in 1:10000)\n1.011689946421105\nSo X′(p)X'(p) does indeed average to 1, which makes sense since the expectation of Exp(p)\\operatorname{Exp}(p) is pp, which has derivative 1 for any choice of pp. However, the crux is that this notion of derivative also works for more complicated random variables that can be formed via composition of simple ones such as an exponential random variable. In fact, it turns out to obey the same chain rule as usual!\nLet’s demonstrate this. Using the dual numbers introduced in Chapter 8, we can differentiate the expectation of the square of a sample from an exponential distribution without having an analytic expression for this quantity. (The expression for X′X' we derived is already implemented as a dual-number rule in Julia by the ForwardDiff.jl package.) The primal and dual values of the outputted dual number are samples from the joint distribution of (X(p),X′(p))(X(p), X'(p)).\njulia&gt; using Distributions, ForwardDiff: Dual\n\njulia&gt; sample_X(p) = rand(Exponential(p))^2\nsample_X (generic function with 1 method)\n\njulia&gt; sample_X(Dual(10.0, 1.0)) # sample a single dual number!\nDual{Nothing}(153.74964559529033,30.749929119058066)\n\njulia&gt; # obtain the derivative!\njulia&gt; mean(sample_X(Dual(10.0, 1.0)).partials[1] for i in 1:10000)\n40.016569793650525\nUsing the “reparameterization trick” to form a gradient estimator, as we have done here, is a fairly old idea. It is also called the “pathwise” gradient estimator. Recently, it has become very popular in machine learning due to its use in VAEs [e.g. Kingma & Welling (2013): https://arxiv.org/abs/1312.6114], and lots of resources can be found online on it. Since composition simply works by the usual chain rule, it also works in reverse mode, and can differentiate functions far more complicated than the one above!"
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#handling-discrete-randomness",
    "href": "matrix-calculus/en-US/index.html#handling-discrete-randomness",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Handling discrete randomness",
    "text": "Handling discrete randomness\nSo far we have only considered a continuous random variable. Let’s see how the picture changes for a discrete random variable! Let’s take a simple Bernoulli variable X(p)∼Ber(p)X(p) \\sim \\operatorname{Ber}(p), which is 1 with probability pp and 0 with probability 1−p1-p.\njulia&gt; sample_X(p) = rand(Bernoulli(p))\nsample_X (generic function with 1 method)\n\njulia&gt; p = 0.5\n0.6\n\njulia&gt; sample_X(δp) # produces false/true, equivalent to 0/1\ntrue\n\n\njulia&gt; sample_X(δp)\nfalse\n\njulia&gt; sample_X(δp)\ntrue\nThe parameterization of a Bernoulli variable is shown in Figure 2. Using the inversion method once again, the parameterization of a Bernoulli variable looks like a step function: for ω&lt;1−p\\omega &lt; 1-p, X(p)(ω)=0X(p)(\\omega) = 0, while for ω≥1−p\\omega \\geq 1-p, X(p)(ω)=1X(p)(\\omega) = 1.\nNow, what happens when we perturb pp? Let’s imagine perturbing pp by a positive amount δp\\delta p. As shown in Figure 2, something qualitatively very different has happened here. At nearly every ω\\omega except a small region of probability δp\\delta p, the output does not change. Thus, the quantity X′(p)X'(p) we defined in the previous subsection (which, strictly speaking, was defined by an \"almost-sure\" limit that neglects regions of probability 0) is 0 at every ω\\omega: after all, for every ω\\omega, there exists small enough δp\\delta p such that δX(p)(ω)=0\\delta X(p)(\\omega) = 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor X(p) ∼ Ber (p) parameterized via the inversion method, plots of X(p), X(p + δp), and δX(p) as functions Ω : [0, 1] → ℝ.\n\n\nHowever, there is certainly an important derivative contribution to consider here. The expectation of a Bernoulli is pp, so we would expect the derivative to be 1: but $\\EE[X'(p)] = \\EE[0] = 0$. What has gone wrong is that, although δX(p)\\delta X(p) is 0 with tiny probability, the value of δX(p)\\delta X(p) on this region of tiny probability is 1, which is large. In particular, it does not approach 0 as δp\\delta p approaches 0. Thus, to develop a notion of derivative of X(p)X(p), we need to somehow capture these large jumps with “infinitesimal” probability.\nA recent (2022) publication (https://arxiv.org/abs/2210.08572) by the author of this chapter (Gaurav Arya), together with Frank Schäfer, Moritz Schauer, and Chris Rackauckas, worked to extend the above ideas to develop a notion of “stochastic derivative” for discrete randomness, implemented by a software package called StochasticAD.jl that performs automatic differentiation of such stochastic processes. It generalizes the idea of dual numbers to stochastic triples, which include a third component to capture exactly these large jumps. For example, the stochastic triple of a Bernoulli variable might look like:\njulia&gt; using StochasticAD, Distributions\njulia&gt; f(p) = rand(Bernoulli(p)) # 1 with probability p, 0 otherwise\njulia&gt; stochastic_triple(f, 0.5) # Feeds 0.5 + δp into f\nStochasticTriple of Int64:\n0 + 0ε + (1 with probability 2.0ε)\nHere, δp\\delta p is denoted by ε\\upepsilon, imagined to be an “infinitesimal unit”, so that the above triple indicates a flip from 0 to 1 with probability that has derivative 22.\nHowever, many aspects of these problems are still difficult, and there are a lot of improvements awaiting future developments! If you’re interested in reading more, you may be interested in the paper and our package linked above, as well as the 2020 review article by Mohamed et al. (https://arxiv.org/abs/1906.10652), which is a great survey of the field of gradient estimation in general.\nAt the end of class, we considered a differentiable random walk example with StochasticAD.jl. Here it is!\njulia&gt; using Distributions, StochasticAD\n\njulia&gt; function X(p)\n           n = 0\n           for i in 1:100\n               n += rand(Bernoulli(p * (1 - (n+i)/200)))\n           end\n           return n\n       end\nX (generic function with 1 method)\n\njulia&gt; mean(X(0.5) for _ in 1:10000) # calculate E[X(p)] at p = 0.5\n32.6956\n\njulia&gt; st = stochastic_triple(X, 0.5) # sample a single stochastic triple at p = 0.5\nStochasticTriple of Int64:\n32 + 0δp + (1 with probability 74.17635818221052δp)\n\njulia&gt; derivative_contribution(st) # derivative estimate produced by this triple\n74.17635818221052\n\njulia&gt; # compute d/dp of E[X(p)] by taking many samples\njulia&gt; mean(derivative_contribution(stochastic_triple(f, 0.5)) for i in 1:10000)\n56.65142976168479"
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#sec:Hessian-scalar",
    "href": "matrix-calculus/en-US/index.html#sec:Hessian-scalar",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Hessian matrices of scalar-valued functions",
    "text": "Hessian matrices of scalar-valued functions\nRecall that for a function f(x)∈ℝf(x) \\in \\mathbb{R} that maps column vectors x∈ℝnx \\in \\mathbb{R}^n to scalars (f:ℝn↦ℝf: \\mathbb{R}^n \\mapsto \\mathbb{R}), the first derivative f′f' can be expressed in terms of the familiar gradient ∇f=(f′)T\\nabla f = (f')^T of multivariable calculus: ∇f=(∂f∂x1∂f∂x2⋮∂f∂xn).\\nabla f = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n}\n\\end{pmatrix} \\, . If we think of ∇f\\nabla f as a new (generally nonlinear) function mapping x∈ℝn↦∇f∈ℝnx \\in \\mathbb{R}^n \\mapsto \\nabla f \\in \\mathbb{R}^n, then its derivative is an n×nn \\times n Jacobian matrix (a linear operator mapping vectors to vectors), which we can write down explicitly in terms of second derivatives of ff: (∇f)′=(∂2f∂x12⋯∂2f∂xn∂x1⋮⋱⋮∂2f∂x1∂xn⋯∂2f∂xn∂xn)=H.(\\nabla f)' =\n\\begin{pmatrix}\n   \\frac{\\partial^2 f}{ \\partial x_1^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} \\\\\n        \\vdots  &\\ddots & \\vdots \\\\\n        \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n \\partial x_n}\n    \\end{pmatrix} = H \\, . This matrix, denoted here by HH, is known as the Hessian of ff, which has entries: Hi,j=∂2f∂xj∂xi=∂2f∂xi∂xj=Hj,i.H_{i,j} = \\frac{\\partial^2 f}{ \\partial x_j \\partial x_i} = \\frac{\\partial^2 f}{ \\partial x_i \\partial x_j} = H_{j,i}  \\, . The fact that you can take partial derivatives in either order is a familiar fact from multivariable calculus (sometimes called the “symmetry of mixed derivatives” or “equality of mixed partials”), and means that the Hessian is a symmetric matrix H=HTH = H^T. (We will later see that such symmetries arise very generally from the construction of second derivatives.)\n\nFor x∈ℝ2x \\in \\mathbb{R}^2 and the function f(x)=sin(x1)+x12x23f(x) = \\sin(x_1) + x_1^2 x_2^3, its gradient is ∇f=(cos(x1)+2x1x233x12x22),\\nabla f = \\begin{pmatrix} \\cos(x_1) + 2x_1 x_2^3 \\\\\n3 x_1^2 x_2^2\n\\end{pmatrix} \\, , and its Hessian is H=(∇f)′=(−sin(x1)+2x236x1x226x1x226x12x2)=HT.H = (\\nabla f)' = \\begin{pmatrix} -\\sin(x_1) + 2x_2^3 & 6 x_1 x_2^2 \\\\\n6 x_1 x_2^2 & 6 x_1^2 x_2 \\end{pmatrix} = H^T \\, .\n\nIf we think of the Hessian as the Jacobian of ∇f\\nabla f, this tells us that HdxH \\, dx predicts the change in ∇f\\nabla f to first order: d(∇f)=∇f|x+dx−∇f|x=Hdx.d(\\nabla f) = \\left. \\nabla f \\right|_{x+dx} - \\left. \\nabla f \\right|_{x} = H \\, dx \\, . Note that ∇f|x+dx\\left. \\nabla f \\right|_{x+dx} means ∇f\\nabla f evaluated at x+dxx+dx, which is very different from df=(∇f)Tdxdf = (\\nabla f)^T dx, where we act f′(x)=(∇f)Tf'(x)=(\\nabla f)^T on dxdx.\nInstead of thinking of HH of predicting the first-order change in ∇f\\nabla f, however, we can also think of it as predicting the second-order change in ff, a quadratic approximation (which could be viewed as the first three terms in a multidimensional Taylor series): f(x+δx)=f(x)+(∇f)Tδx+12δxTHδx+o(‖δx‖2),f(x+\\delta x) = f(x) + (\\nabla f)^T \\, \\delta x + \\frac{1}{2} \\delta x^T \\, H \\, \\delta x + o(\\Vert \\delta x \\Vert^2) \\, , where both ∇f\\nabla f and HH are evaluated at xx, and we have switched from an infinitesimal dxdx to a finite change δx\\delta x so that we emphasize the viewpoint of an approximation where terms higher than second-order in ‖δx‖\\Vert \\delta x \\Vert are dropped. You can derive this in a variety of ways, e.g. by taking the derivative of both sides with respect to δx\\delta x to reproduce ∇f|x+δx=∇f|x+Hδx+o(δx)\\left. \\nabla f \\right|_{x+\\delta x} = \\left. \\nabla f \\right|_{x} + H \\, \\delta x + o(\\delta x): a quadratic approximation for ff corresponds to a linear approximation for ∇f\\nabla f. Related to this equation, another useful (and arguably more fundamental) relation that we can derive (and will derive much more generally below) is: dxTHdx′=f(x+dx+dx′)+f(x)−f(x+dx)−f(x+dx′)=f″(x)[dx,dx′]dx^T H dx' = f(x + dx + dx') + f(x) - f(x + dx) - f(x + dx') = f''(x)[dx,dx'] \\, where dxdx and dx′dx' are two independent “infinitesimal” directions and we have dropped terms of higher than second order. This formula is very suggestive, because it uses HH to map two vectors into a scalar, which we will generalize below into the idea of a bilinear map f″(x)[dx,dx′]f''(x)[dx,dx']. This formula is also obviously symmetric with respect to interchange of dxdx and dx′dx' — f″(x)[dx,dx′]=f″(x)[dx′,dx]f''(x)[dx,dx'] = f''(x)[dx',dx] — which will lead us once again to the symmetry H=HTH=H^T below.\n\nRemark. Consider the Hessian matrix versus other Jacobian matrices. The Hessian matrix expresses the second derivative of a scalar-valued multivariate function, and is always square and symmetric. A Jacobian matrix, in general, expresses the first derivative of a vector-valued multivariate function, may be non-square, and is rarely symmetric. (However, the Hessian matrix is the Jacobian of the ∇f\\nabla f function!)"
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#general-second-derivatives-bilinear-maps",
    "href": "matrix-calculus/en-US/index.html#general-second-derivatives-bilinear-maps",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "General second derivatives: Bilinear maps",
    "text": "General second derivatives: Bilinear maps\nRecall, as we have been doing throughout this class, that we define the derivative of a function ff by a linearization of its change dfdf for a small (“infinitesimal”) change dxdx in the input: df=f(x+dx)−f(x)=f′(x)[dx],df = f(x + dx) - f(x) = f'(x) [dx] \\, , implicitly dropping higher-order terms. If we similarly consider the second derivative f″f'' as simply the same process applied to f′f' instead of ff, we obtain the following formula, which is easy to write down but will take some thought to interpret: df′=f′(x+dx′)−f′(x)=f″(x)[dx′].d f' = f'(x + dx') - f'(x) = f''(x) [dx']. (Notation: dx′dx' is not some kind of derivative of dxdx; the prime simply denotes a different arbitrary small change in xx.) What kind of “thing” is df′df'? Let’s consider a simple concrete example:\n\nConsider the following function f(x):ℝ2↦ℝ2f(x): \\mathbb{R}^2 \\mapsto \\mathbb{R}^2 mapping two-component vectors x∈ℝ2x \\in \\mathbb{R}^2 to two-component vectors f(x)∈ℝ2f(x) \\in \\mathbb{R}^2: f(x)=(x12sin(x2)5x1−x23).f(x) = \\begin{pmatrix} x_1^2 \\sin(x_2) \\\\ 5x_1 - x_2^3\n\\end{pmatrix} \\, . Its first derivative is described by a 2×22\\times 2 Jacobian matrix: f′(x)=(2x1sin(x2)x12cos(x2)5−3x22)f'(x) = \\begin{pmatrix}\n2x_1 \\sin(x_2) & x_1^2 \\cos(x_2) \\\\\n5 & -3x_2^2\n\\end{pmatrix} that maps a small change dxdx in the input vector xx to the corresponding small change df=f′(x)dxdf = f'(x)dx in the output vector ff.\nWhat is df′=f″(x)[dx′]df' = f''(x)[dx']? It must take a small change dx′=(dx1′,dx2′)dx' = (dx_1', dx_2') in xx and return the first-order change df′=f′(x+dx′)−f′(x)df' = f'(x+dx')-f'(x) in our Jacobian matrix f′f'. If we simply take the differential of each entry of our Jacobian (a function from vectors xx to matrices f′f'), we find: df′=(2dx1′sin(x2)+2x1cos(x2)dx2′2x1dx1′cos(x2)−x12sin(x2)dx2′0−6x2dx2′)=f″(x)[dx′]df' = \n\\begin{pmatrix}\n2\\,dx_1' \\sin(x_2) + 2x_1 \\cos(x_2) \\,dx_2' & 2 x_1 \\, dx_1' \\cos(x_2) - x_1^2 \\sin(x_2)\\, dx_2' \\\\\n0 & -6x_2\\,dx_2'\n\\end{pmatrix}\n= f''(x)[dx'] That is, df′df' is a 2×22\\times 2 matrix of “infinitesimal” entries, of the same shape as f′f'.\nFrom this viewpoint, f″(x)f''(x) is a linear operator acting on vectors dx′dx' and outputting 2×22\\times 2 matrices f″(x)[dx′]f''(x)[dx'], but this is one of the many cases where it is easier to write down the linear operator as a “rule” than as a “thing” like a matrix. The “thing” would have to either be some kind of “three-dimensional matrix” or we would have to “vectorize” f′f' into a “column vector” of 4 entries in order to write its 4×44\\times 4 Jacobian, as in Sec. 3 (which can obscure the underlying structure of the problem).\nFurthermore, since this df′df' is a linear operator (a matrix), we can act it on another vector dx=(dx1,dx2)dx = (dx_1, dx_2) to obtain: df′(dx1dx2)=(2sin(x2)dx1′dx1+2x1cos(x2)(dx2′dx1+dx1′dx2)−x12sin(x2)dx2′dx2−6x2dx2′dx2)=f″(x)[dx′][dx].df' \\begin{pmatrix} dx_1 \\\\ dx_2 \\end{pmatrix} =\n\\begin{pmatrix}\n2 \\sin(x_2) \\,dx_1'\\,dx_1 + 2x_1 \\cos(x_2) (\\,dx_2'\\,dx_1 + \\, dx_1' \\, dx_2) - x_1^2 \\sin(x_2)\\, dx_2' \\, dx_2 \\\\\n-6x_2\\,dx_2' \\,dx_2\n\\end{pmatrix} = f''(x)[dx'][dx] \\, . Notice that this result, which we will call f″(x)[dx′,dx]f''(x)[dx', dx] below, is the “same shape” as f(x)f(x) (a 2-component vector). Moreover, it doesn’t change if we swap dxdx and dx′dx': f″(x)[dx′,dx]=f″(x)[dx,dx′]f''(x)[dx', dx] = f''(x)[dx, dx'], a key symmetry of the second derivative that we will discuss further below.\n\ndf′df' is an (infinitesimal) object of the same “shape” as f′(x)f'(x), not f(x)f(x). Here, f′f' is a linear operator, so its change df′df' must also be an (infinitesimal) linear operator (a “small change” in a linear operator) that we can therefore act on an arbitrary dxdx (or δx\\delta x), in the form: df′[dx]=f″(x)[dx′][dx]:=f″(x)[dx′,dx],df'[dx] = f''(x) [dx'][dx] := f''(x) [dx', dx] \\, , where we combine the two brackets for brevity. This final result f″(x)[dx′,dx]f''(x) [dx', dx] is the same type of object (vector) as the original output f(x)f(x). This implies that f″(x)f''(x) is a bilinear map: acting on two vectors, and linear in either vector taken individually. (We will see shortly that the ordering of dxdx and dx′dx' doesn’t matter: f″(x)[dx′,dx]=f″(x)[dx,dx′]f''(x)[dx',dx]=f''(x)[dx,dx'].)\nMore precisely, we have the following.\n\nLet U,V,WU,V,W be a vector spaces, not necessarily the same. Then, a bilinear map is a function B:U×V→WB:U\\times V \\to W, mapping a u∈Uu \\in U and v∈Vv \\in V to B[u,v]∈WB[u,v] \\in W, such that we have linearity in both arguments: {B[u,αv1+βv2]=αB[u,v1]+βB[u,v2]B[αu1+βu2,v]=αB[u1,v]+βB[u2,v]\\begin{cases}\n        B[u, \\alpha v_1 + \\beta v_2] = \\alpha B[u,v_1] + \\beta B[u,v_2] \\\\\n        B[\\alpha u_1 + \\beta u_2, v] = \\alpha B[u_1,v] + \\beta B[u_2,v]\n    \\end{cases} for any scalars α,β\\alpha, \\beta,\nIf W=ℝW = \\mathbb{R}, i.e. the output is a scalar, then it is called a bilinear form.\n\nNote that in general, even if U=VU = V (the two inputs u,vu,v are the “same type” of vector) we may have B[u,v]≠B[v,u]B[u,v] \\neq B[v,u], but in the case of f″f'' we have something very special that happens. In particular, we can show that f″(x)f''(x) is a symmetric bilinear map, meaning f″(x)[dx′,dx]=f″(x)[dx,dx′]f''(x) [dx', dx] = f''(x) [dx, dx'] for any dxdx and dx′dx'. Why? Because, applying the definition of f″f'' as giving the change in f′f' from dx′dx', and then the definition of f′f' as giving the change in ff from dxdx, we can re-order terms to obtain: f″(x)[dx′,dx]=f′(x+dx′)[dx]−f′(x)[dx]=(f(x+dx′+dx⏟=dx+dx′)−f(x+dx′))−(f(x+dx)−f(x))=f(x+dx+dx′)+f(x)−f(x+dx)−f(x+dx′)=(f(x+dx+dx′)−f(x+dx))−(f(x+dx′)−f(x))=f′(x+dx)[dx′]−f′(x)[dx′]=f″(x)[dx,dx′]\\begin{aligned}\n    f''(x) [dx', dx] &= f'(x + \n    dx') [dx] - f'(x) [dx] \\\\\n    &= \\left(f(x+ \\underbrace{dx' + dx}_{= dx + dx'}) - f(x+ dx')\\right) - \\left(f(x+ dx) - f(x)\\right) \\\\ \n    &= \\boxed{f(x + dx + dx') + f(x) - f(x + dx) - f(x + dx')} \\\\\n    &= \\left(f(x + dx + dx')  - f(x + dx)\\right) - \\left(f(x + dx') - f(x)\\right) \\\\\n    &= f'(x + \n    dx) [dx'] - f'(x) [dx'] \\\\\n    &= f''(x) [dx, dx'] \\, \n\\end{aligned} where we’ve boxed the middle formula for f″f'' which emphasizes its symmetry in a natural way. (The basic reason why this works is that the “++” operation is always commutative for any vector space. A geometric interpretation is depicted in Fig. 12.)\n\n\n\n\n\nGeometric interpretation of f″(x)[dx, dx′]: To first order, a function f maps parallelograms to parallelograms. To second order, however it “opens” parallelograms: The deviation from point B (the image of A) from point C (the completion of the parallelogram) is the second derivative f″(x)[dx, dx′]. The symmetry of f″ as a bilinear form can be traced back geometrically to the mirror symmetry of the input parallelogram across its diagonal from x to point A. \n\n\n\nLet’s review the familiar example from multivariable calculus, $f: \\R^n \\to \\R$. That is, f(x)f(x) is a scalar-valued function of a column vector $x\\in \\R^n$. What is f″f''?\n\nRecall that f′(x)=(∇f)T⟹f′(x)[dx]=scalar df=(∇f)Tdx.f'(x) = (\\nabla f)^T \\implies f'(x) [dx] = \\text{scalar } df = (\\nabla f)^T dx. Similarly, f″(x)[dx′,dx]=scalar from two vectors, linear in both=dx′THdx,\\begin{aligned}\n    f''(x) [dx', dx] &= \\text{scalar from two vectors, linear in both} \\\\\n    &= dx'^T H dx \\, ,\n\\end{aligned} where HH must be exactly the n×nn \\times n matrix Hessian matrix introduced in Sec. 12.1, since an expression like dx′THdxdx'^T H dx is the most general possible bilinear form mapping two vectors to a scalar. Moreover, since we now know that f″f'' is always a symmetric bilinear form, we must have: f″(x)[dx′,dx]=dx′THdx=f″(x)[dx,dx′]=dxTHdx′=(dxTHdx′)T(scalar=scalarT)=dx′THTdx\\begin{aligned}\nf''(x) [dx', dx] &=dx'^T H dx \\\\\n&= f''(x) [dx, dx'] = dx^T H dx' =\n     (dx^T H dx')^T  \\qquad (\\mathrm{scalar} = \\mathrm{scalar}^T)\\\\\n    &= dx'^T H^T dx \n\\end{aligned} for all dxdx and dx′dx'. This implies that H=HTH = H^T: the Hessian matrix is symmetric. As discussed in Sec. 12.1, we already knew this from multi-variable calculus. Now, however, this “equality of mixed partial derivatives” is simply a special case of f″f'' being a symmetric bilinear map.\nAs an example, let’s consider a special case of the general formula above:\n\nLet f(x)=xTAxf(x) = x^T A x for $x \\in \\R^n$ and AA an n×nn \\times n matrix. As above, $f(x) \\in \\R$ (scalar outputs). Compute f″f''.\n\nThe computation is fairly straightforward. Firstly, we have that f′=(∇f)T=xT(A+AT).f' = (\\nabla f)^T = x^T(A + A^T). This implies that ∇f=(A+AT)x\\nabla f = (A + A^T) x, a linear function of xx. Hence, the Jacobian of ∇f\\nabla f is the Hessian f″=H=A+ATf'' = H = A + A^T. Furthermore, note that this implies f(x)=xTAx=(xTAx)T(scalar=scalarT)=xTATx=12(xTAx+xTATx)=12xT(A+AT)x=12xTHx=12f″[x,x],\\begin{aligned}\n    f(x) &= x^T A x = (x^T A x)^T  \\qquad (\\mathrm{scalar} = \\mathrm{scalar}^T)\\\\\n    &= x^T A^T x \\\\\n    &= \\frac{1}{2} (x^T A x + x^T A^T x) = \\frac{1}{2} x^T (A + A^T) x\\\\\n    &= \\frac{1}{2} x^T H x = \\frac{1}{2} f''[x,x] \\, ,\n\\end{aligned} which will turn out to be a special case of the quadratic approximations of Sec. 12.3 (exact in this example since f(x)=xTAxf(x)=x^T A x is quadratic to start with).\n\nLet f(A)=detAf(A) = \\det A for AA an n×nn\\times n matrix. Express f″(A)f''(A) as a rule for f″(A)[dA,dA′]f''(A)[dA,dA'] in terms of dAdA and dA′dA'.\n\nFrom lecture 3, we have the first derivative $$f'(A) [dA] = df = \\det (A) \\tr(A^{-1} dA).$$ Now, we want to compute the change d′(df)=d′(f′(A)[dA])=f′(A+dA′)[dA]−f′(A)[dA]d'(df) = d'(f'(A)[dA]) = f'(A+dA')[dA] - f'(A)[dA] in this formula, i.e. the differential (denoted d′d') where we change AA by dA′dA' while treating dAdA as a constant: $$\\begin{aligned}\n    f''(A) [dA, dA'] &= d' (\\det A \\tr (A^{-1} dA)) \\\\\n    &= \\det A \\tr(A^{-1} dA') \\tr(A^{-1} dA) - \\det A \\tr(A^{-1} \\,dA' A^{-1}\\, dA) \\\\\n    &= f''(A) [dA', dA]\n\\end{aligned}$$ where the last line (symmetry) can be derived explicitly by the cyclic property of the trace (although of course it must be true for any f″f''). Although f″f'' here is a perfectly good bilinear form acting on matrices dA,dA′dA,dA', it is not very natural to express f″f'' as a “Hessian matrix.”\nIf we really wanted to express f″f'' in terms of an explicit Hessian matrix, we could use the the “vectorization” approach of Sec. 3. Let us consider, for example, the term $\\tr(A^{-1} \\,dA' A^{-1}\\, dA)$ using Kronecker products (Sec. 3). In general, for matrices X,Y,B,CX,Y,B,C: $$(\\operatorname{vec}{X})^T (B \\otimes C) \\operatorname{vec}{Y} = (\\operatorname{vec}{X})^T \\operatorname{vec}{(CYB^T)} = \\tr(X^T CYB^T) = \\tr(B^T X^T CY) \\, ,$$ recalling that $(\\operatorname{vec}{X})^T \\operatorname{vec}{Y} = \\tr(X^T Y)$ is the Frobenius inner product (Sec. 5). Thus, $$\\tr(A^{-1} \\,dA' A^{-1}\\, dA)\n= \\operatorname{vec}{(dA'^T)}^T (A^{-T} \\otimes A^{-1}) \\operatorname{vec}{(dA)} \\, .$$ This is still not quite in the form we want for a Hessian matrix, however, because it involves vec(dA′T)\\operatorname{vec}{(dA'^T)} rather than vec(dA′)\\operatorname{vec}{(dA')} (the two vectors are related by a permutation matrix, sometimes called a “commutation” matrix). Completing this calculation would be a nice exercise in mastery of Kronecker products, but getting an explicit Hessian seems like a lot of algebra for a result of dubious utility!"
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#sec:Hessian-quadratic",
    "href": "matrix-calculus/en-US/index.html#sec:Hessian-quadratic",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Generalized quadratic approximation",
    "text": "Generalized quadratic approximation\nSo how do we ultimately think about f″f''? We know that f′f' is the linearization/linear approximation of f(x)f(x), i.e. f(x+δx)=f(x)+f′(x)[δx]+o(‖δx‖).f(x + \\delta x ) = f(x) + f'(x) [\\delta x] + o(\\lVert \\delta x\\rVert). Now, just as we did for the simple case of Hessian matrices in Sec. 12.1 above, we can use f″f'' to form a quadratic approximation of f(x)f(x). In particular, one can show that f(x+δx)=f(x)+f′(x)[δx]+12f″(x)[δx,δx]+o(‖δx‖2).f(x+ \\delta x) = f(x) + f'(x) [\\delta x] + \\frac{1}{2} f''(x) [\\delta x, \\delta x] +  o (\\lVert \\delta x\\rVert^2). Note that the 12\\frac{1}{2} factor is just as in the Taylor series. To derive this, simply plug the quadratic approximation into f″(x)[dx,dx′]=f(x+dx+dx′)+f(x)−f(x+dx)−f(x+dx′).f''(x) [dx, dx'] = f(x + dx + dx') + f(x) - f(x + dx) - f(x + dx'). and check that the right-hand side reproduces f″(x)f''(x). (Note how dxdx and dx′dx' appear symmetrically in this formula, which reflects the symmetry of f″f''.)"
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#hessians-and-optimization",
    "href": "matrix-calculus/en-US/index.html#hessians-and-optimization",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Hessians and optimization",
    "text": "Hessians and optimization\nMany important applications of second derivatives, Hessians, and quadratic approximations arise in optimization: minimization (or maximization) of functions f(x)f(x).18\n\nNewton-like methods\nWhen searching for a local minimum (or maximum) of a complicated function f(x)f(x), a common procedure is to approximate f(x+δx)f(x+\\delta x) by a simpler “model” function for small δx\\delta x, and then to optimize this model to obtain a potential optimization step. For example, approximating f(x+δx)≈f(x)+f′(x)[δx]f(x+\\delta x) \\approx f(x)+f'(x)[\\delta x] (an affine model, colloquially called “linear”) leads to gradient descent and related algorithms. A better approximation for f(x+δx)f(x + \\delta x) will often lead to faster-converging algorithms, and so a natural idea is to exploit the second derivative f″f'' to make a quadratic model, as above, and accelerate optimization.\nFor unconstrained optimization, minimizing f(x)f(x) corresponds to finding a root of the derivative f′=0f' = 0 (i.e., ∇f=0\\nabla f = 0), and a quadratic approximation for ff yields a first-order (affine) approximation f′(x+δx)≈f′(x)+f″(x)[δx]f'(x + \\delta x) \\approx f'(x) + f''(x)[\\delta x] for the derivative f′f'. In ℝn\\mathbb{R}^n, this is δ(∇f)≈Hδx\\delta(\\nabla f) \\approx H \\delta x. So, minimizing a quadratic model is effectively a Newton step δx≈−H−1∇f\\delta x \\approx -H^{-1} \\nabla f to find a root of ∇f\\nabla f via first-order approximation. Thus, optimization via quadratic approximations is often viewed as a form of Newton algorithm. As discussed below, it is also common to employ approximate Hessians in optimization, resulting in “quasi-Newton” algorithms.\nMore complicated versions of this idea arise in optimization with constraints, e.g. minimizing an objective function f(x)f(x) subject to one or more nonlinear inequality constraints ck(x)≤0c_k(x) \\le 0. In such cases, there are a variety of methods that take both first and second derivatives into account, such as “sequential quadratic programming”19 (SQP) algorithms that solve a sequence of “QP” approximations involving quadratic objectives with affine constraints (see e.g. the book Numerical Optimization by Nocedal and Wright, 2006).\nThere are many technical details, beyond the scope of this course, that must be resolved in order to translate such high-level ideas into practical algorithms. For example, a quadratic model is only valid for small enough δx\\delta x, so there must be some mechanism to limit the step size. One possibility is “backtracking line search”: take a Newton step x+δxx+\\delta x and, if needed, progressively “backtrack” to x+δx/10,x+δx/100,…x+\\delta x/10, x+\\delta x/100, \\ldots until a sufficiently decreased value of the objective is found. Another commonplace idea is a “trust region”: optimize the model with the constraint that δx\\delta x is sufficiently small, e.g. ‖δx‖≤s\\Vert \\delta x \\Vert \\le s (a spherical trust region), along with some rules to adaptively enlarge or shrink the trust-region size (ss) depending on how well the model predicts δf\\delta f. There are many variants of Newton/SQP-like algorithms depending on the choices made for these and other details.\n\n\nComputing Hessians\nIn general, finding f″f'' or the Hessian is often computationally expensive in higher dimensions. If $f(x): \\R^n\\to\\R$, then the Hessian, HH, is an n×nn\\times n matrix, which can be huge if nn is large—even storing HH may be prohibitive, much less computing it. When using automatic differentiation (AD), Hessians are often computed by a combination of forward and reverse modes (Sec. 8.4.1), but AD does not circumvent the fundamental scaling difficulty for large nn.\nInstead of computing HH explicitly, however, one can instead approximate the Hessian in various ways; in the context of optimization, approximate Hessians are found in “quasi-Newton” methods such as the famous “BFGS” algorithm and its variants. One can also derive efficient methods to compute Hessian–vector products HvHv without computing HH explicitly, e.g. for use in Newton–Krylov methods. (Such a product HvHv is equivalent to a directional derivative of f′f', which is efficiently computed by “forward-over-reverse” AD as in Sec. 8.4.1.)\n\n\nMinima, maxima, and saddle points\nGeneralizing the rules you may recall from single- and multi-variable calculus, we can use the second derivative to determine whether an extremum is a minimum, maximum, or saddle point. Firstly, an extremum of a scalar function ff is a point x0x_0 such that f′(x0)=0f'(x_0) = 0. That is, f′(x0)[δx]=0f'(x_0) [\\delta x] = 0 for any δx\\delta x. Equivalently, ∇f|x0=f′(x0)T=0.\\nabla f \\bigr|_{x_0} = f'(x_0)^T = 0.\nUsing our quadratic approximation around x0x_0, we then have that f(x0+δx)=f(x0)+f′(x0)[δx]⏟=0+12f″(x0)[δx,δx]+o(‖δx‖2).f(x_0 + \\delta x) = f(x_0) + \\underbrace{f'(x_0) [\\delta x]}_{=0} + \\frac{1}{2} f''(x_0) [\\delta x, \\delta x] + o(\\lVert \\delta x\\rVert^2). The definition of a local minimum x0x_0 is that f(x0+δx)&gt;f(x0)f(x_0 + \\delta x ) &gt; f(x_0) for any δx≠0\\delta x \\neq 0 with ‖δx‖\\lVert \\delta x\\rVert sufficiently small. To achieve this at a point where f′=0f' = 0, it is enough to have f″f'' be a positive-definite quadratic form: f″(x0)[δx,δx]&gt;0 for all δx≠0⇔𝐩𝐨𝐬𝐢𝐭𝐢𝐯𝐞-𝐝𝐞𝐟𝐢𝐧𝐢𝐭𝐞 f″(x0).f''(x_0) [\\delta x, \\delta x] &gt;0 \\text{ for all } \\delta x \\ne 0 \\iff \\textbf{positive-definite } f''(x_0) \\, .\nFor example, for inputs $x\\in \\R^n$, so that f″f'' is a real-symmetric n×nn \\times n Hessian matrix, f″(x0)=H(x0)=H(x0)Tf''(x_0) = H(x_0) = H(x_0)^T, this corresponds to the usual criteria for a positive-definite matrix: f″(x0)[δx,δx]=δxTH(x0)δx&gt;0 for all δx≠0⇔H(x0) positive-definite ⇔all eigenvalues of H(x0)&gt;0.f''(x_0) [\\delta x, \\delta x] = \\delta x^T H(x_0) \\delta x &gt;0 \\text{ for all } \\delta x \\ne 0 \\iff H(x_0) \\text{ positive-definite } \\iff \\text{all eigenvalues of } H(x_0) &gt; 0.\nIn first-year calculus, one often focuses in particular on the 2-dimensional case, where HH is a 2×22\\times 2 matrix. In the 2×22\\times 2 case, there is a simple way to check the signs of the two eigenvalues of HH, in order to check whether an extremum is a minimum or maximum: the eigenvalues are both positive if and only if det(H)&gt;0\\det (H) &gt;0 and $\\tr(H)  &gt;0$, since det(H)=λ1λ2\\det (H) = \\lambda_1 \\lambda_2 and $\\tr(H) = \\lambda_1 + \\lambda_2$. In higher dimensions, however, one needs more complicated techniques to compute eigenvalues and/or check positive-definiteness, e.g. as discussed in MIT courses 18.06 (Linear Algebra) and/or 18.335 (Introduction to Numerical Methods). (In practice, one typically checks positive-definiteness by performing a form of Gaussian elimination, called a Cholesky factorization, and checking that the diagonal “pivot” elements are &gt;0&gt; 0, rather than by computing eigenvalues which are much more expensive.)\nSimilarly, a point x0x_0 where ∇f=0\\nabla f = 0 is a local maximum if f″f'' is negative-definite, or equivalently if the eigenvalues of the Hessian are all negative. Additionally, x0x_0 is a saddle point if f″f'' is indefinite, i.e. the eigenvalues include both positive and negative values. However, cases where some eigenvalues are zero are more complicated to analyze; e.g. if the eigenvalues are all ≥0\\ge 0 but some are =0=0, then whether the point is a minimum depends upon higher derivatives."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#further-reading-1",
    "href": "matrix-calculus/en-US/index.html#further-reading-1",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Further Reading",
    "text": "Further Reading\nAll of this formalism about “bilinear forms” and so forth may seem like a foray into abstraction for the sake of abstraction. Can’t we always reduce things to ordinary matrices by choosing a basis (“vectorizing” our inputs and outputs)? However, we often don’t want to do this for the same reason that we often prefer to express first derivatives as linear operators rather than as explicit Jacobian matrices. Writing linear or bilinear operators as explicit matrices, e.g. vec(AdA+dAA)=(I⊗A+AT⊗I)vec(dA)\\operatorname{vec}(A\\,dA + dA\\,A) = (I\\otimes A + A^T \\otimes I)\\operatorname{vec}(dA) as in Sec. 3, often disguises the underlying structure of the operator and introduces a lot of algebraic complexity for no purpose, as well as being potentially computationally costly (e.g. exchanging small matrices AA for large ones I⊗AI \\otimes A).\nAs we discussed in this chapter, an important generalization of quadratic operations to arbitrary vector spaces come in the form of bilinear maps and bilinear forms, and there are many textbooks and other sources discussing these ideas and variations thereof. For example, we saw that the second derivative can be seen as a symmetric bilinear form. This is closely related to a quadratic form Q[x]Q[x], which what we get by plugging the same vector twice into a symmetric bilinear form B[x,y]=B[y,x]B[x,y]=B[y,x], i.e. Q[x]=B[x,x]Q[x] = B[x,x]. (At first glance, it may seem like QQ carries “less information” than BB, but in fact this is not the case. It is easy to see that one can recover BB from QQ via B[x,y]=(Q[x+y]−Q[x−y])/4B[x,y] = (Q[x+y] - Q[x-y])/4, called a “polarization identity.”) For example, the f″(x)[δx,δx]/2f''(x) [\\delta x, \\delta x]/2 term that appears in quadratic approximations of f(x+δx)f(x+ \\delta x) is a quadratic form. The most familiar multivariate version of f″(x)f''(x) is the Hessian matrix when xx is a column vector and f(x)f(x) is a scalar, and Khan Academy has an elementary introduction to quadratic approximation.\nPositive-definite Hessian matrices, or more generally definite quadratic forms f″f'', appear at extrema (f′=0f' =0) of scalar-valued functions f(x)f(x) that are local minima. There are a lot more formal treatments of the same idea, and conversely Khan Academy has the simple 2-variable version where you can check the sign of the 2×22\\times 2 eignevalues just by looking at the determinant and a single entry (or the trace). There’s a nice stackexchange discussion on why an ill-conditioned Hessian tends to make steepest descent converge slowly. Some Toronto course notes on the topic may also be useful.\nLastly, see for example these Stanford notes on sequential quadratic optimization using trust regions (Section 2.2), as well as the 18.335 notes on BFGS quasi-Newton methods. The fact that a quadratic optimization problem in a sphere has strong duality, and hence is efficiently solvable, is discussed in Section 5.2.4 of the Convex Optimization book. There has been a lot of work on automatic Hessian computation, but for large-scale problems you may only be able to compute Hessian–vector products efficiently in general, which are equivalent to a directional derivative of the gradient and can be used (for example) for Newton–Krylov methods.\nThe Hessian matrix is also known as the “curvature matrix\" especially in optimization. If we have a scalar function f(x)f(x) of nn variables, its”graph” is the set of points (x,f(x))(x,f(x)) in Rn+1R^{n+1}; we call the last dimension the “vertical\" dimension. At a”critical point” xx (where ∇f=0\\nabla f = 0), then vTHvv^T H v is the ordinary curvature sometimes taught in first-year calculus, of the curve obtained by intersecting the graph with the plane in the direction vv from xx and the vertical (the “normal section”). The determinant of HH, sometimes known as the Hessian determinant, yields the Gaussian curvature.\nA closely related idea is the derivative of the unit normal. For a graph as in the preceding paragraph we may assume that f(x)=xTHx/2f(x)=x^THx/2 to second order. It is easy to see that at any point xx the tangents have the form (dx,f′(x)[dx])=(dx,xTHdx)(dx, f'(x)[dx])=(dx,x^THdx) and the normal is then (Hx,1)(Hx,1). Near x=0x=0 this a unit normal to second order, and its derivative is (Hdx,0)(Hdx,0). Projecting onto the horizontal, we see that the Hessian is the derivative of the unit normal. This is called the “shape operator\" in differential geometry."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#differentiating-on-the-unit-sphere",
    "href": "matrix-calculus/en-US/index.html#differentiating-on-the-unit-sphere",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Differentiating on the Unit Sphere",
    "text": "Differentiating on the Unit Sphere\nGeometrically, we know that velocity vectors (equivalently, tangents) on the sphere are orthogonal to the radii. Out differentials say this algebraically, since given x∈𝕊nx\\in \\mathbb{S}^n we have xTx=1x^T x = 1, this implies that 2xTdx=d(xTx)=d(1)=0.2x^T dx = d(x^T x) = d(1) = 0. In other words, at the point xx on the sphere (a radius, if you will), dxdx, the linearization of the constraint of moving along the sphere satisfies dx⊥xdx \\perp x. This is our first example where we have seen the infinitesimal perturbation dxdx being constrained. See Figure 13.\n\n\n\nDifferentials on a sphere (xTx=1x^T x = 1): the differential dxdx is constrained to be perpendicular to xx.\n\n\n\nSpecial Case: A Circle\nLet us simply consider the unit circle in the plane where x=(cosθ,sinθ)x = (\\cos \\theta, \\sin \\theta) for some θ∈[0,2π)\\theta \\in [0,2\\pi). Then, xTdx=(cosθ,sinθ)⋅(−sinθ,cosθ)dθ=0.x^T dx = (\\cos \\theta, \\sin \\theta) \\cdot (-\\sin \\theta, \\cos \\theta) d\\theta = 0. Here, we can think of xx as “extrinsic” coordinates, in that it is a vector in $\\R^2$. On the other hand, θ\\theta is an “intrinsic” coordinate, as every point on the circle is specified by one θ\\theta.\n\n\nOn the Sphere\nYou may remember that the rank-1 matrix xxTx x^T, for any unit vector xTx=1x^T x = 1, is a projection matrix (meaning that it is equal to its square and it is symmetric) which projects vectors onto their components in the direction of xx. Correspondingly, I−xxTI - x x^T is also a projection matrix, but onto the directions perpendicular to xx: geometrically, the matrix removes components in the xx direction. In particular, if xTdx=0x^T dx= 0, then (I−xxT)dx=dx.(I - x x^T) dx = dx. It follows that if xTdx=0x^T dx = 0 and AA is a symmetric matrix, we have d(12xTAx)=(Ax)Tdx=xTA(dx)=xTA(I−xxT)dx=((I−xxT)Ax)Tdx.\\begin{aligned}\n    d\\left(\\frac{1}{2} x^T A x\\right) &= (A x)^T dx \\\\\n    &= x^T A (dx) \\\\\n    &= x^T A ( I - x x^T) dx  \\\\\n    &= ((I - x x^T) A x )^T dx.\n\\end{aligned} In other words, (I−xxT)Ax(I - x x^T)A x is the gradient of 12xTAx\\frac{1}{2} x^T A x on the sphere.\nSo what did we just do? To obtain the gradient on the sphere, we needed (i) a linearization of the function that is correct on tangents, and (ii) a direction that is tangent (i.e. satisfies the linearized constraint). Using this, we obtain the gradient of a general scalar function on the sphere:\n\nGiven $f: \\mathbb{S}^n \\to \\R$, we have df=g(x)Tdx=((I−xxT)g(x))Tdx.df = g(x)^T dx = (( I - x x^T) g(x))^T dx.\n\nThe proof of this is precisely the same as we did before for f(x)=12xTAxf(x) = \\frac{1}{2} x^T A x."
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#differentiating-on-orthogonal-matrices",
    "href": "matrix-calculus/en-US/index.html#differentiating-on-orthogonal-matrices",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Differentiating on Orthogonal Matrices",
    "text": "Differentiating on Orthogonal Matrices\nLet QQ be an orthogonal matrix. Then, computationally (as is done in the Julia notebook), one can see that QTdQQ^T dQ is an anti-symmetric matrix (sometimes called skew-symmetric).\n\nA matrix MM is anti-symmetric if M=−MTM = - M^T. Note that all anti-symmetric matrices thus have zeroes on their diagonals.\n\nIn fact, we can prove that QTdQQ^T dQ is anti-symmetric.\n\nGiven QQ is an orthogonal matrix, we have that QTdQQ^T dQ is anti-symmetric.\n\n\nProof. Proof. The constraint of being orthogonal implies that QTQ=IQ^T Q = I. Differentiating this equation, we obtain QTdQ+dQTQ=0⟹QTdQ=−(QTdQ)T.Q^T dQ + dQ^T \\, Q = 0 \\implies Q^T dQ = - (Q^T dQ)^T. This is precisely the definition of being anti-symmetric. ◻\n\nBefore we move on, we may ask what the dimension of the “surface” of orthogonal matrices is in $\\R^{n^2}$.\nWhen n=2n = 2, all orthogonal matrices are rotations and reflections, and rotations have the form Q=(cosθsinθ−sinθcosθ).Q = \\begin{pmatrix}\n    \\cos \\theta & \\sin \\theta \\\\ \n    - \\sin \\theta & \\cos \\theta\n\\end{pmatrix}. Hence, when n=2n=2 we have one parameter.\nWhen n=3n = 3, airplane pilots know about “roll, pitch, and yaw”, which are the three parameters for the orthogonal matrices when n=3.n =3. In general, in $\\R^{n^2}$, the orthogonal group has dimension n(n−1)/2n(n-1)/2.\nThere are a few ways to see this.\n\nFirstly, orthogonality QTQ=IQ^T Q = I imposes n(n+1)/2n(n+1)/2 constraints, leaving n(n−1)/2n(n-1)/2 free parameters.\nWhen we do QRQR decomposition, the RR “eats” up n(n+1)/2n(n+1)/2 of the parameters, again leaving n(n−1)/2n(n-1)/2 for QQ.\nLastly, If we think about the symmetric eigenvalue problem where S=QΛQTS = Q \\Lambda Q^T, SS has n(n+1)/2n(n+1) /2 parameters and Λ\\Lambda has nn, so QQ has n(n−1)/2n(n-1)/2.\n\n\nDifferentiating the Symmetric Eigendecomposition\nLet SS be a symmetric matrix, Λ\\Lambda be diagonal containing eigenvalues of SS, and QQ be orthogonal with column vectors as eigenvectors of SS such that S=QΛQTS = Q \\Lambda Q^T. [For simplicity, let’s assume that the eigenvalues are “simple” (multiplicity 1); repeated eigenvalues turn out to greatly complicate the analysis of perturbations because of the ambiguity in their eigenvector basis.] Then, we have dS=dQΛQT+QdΛQT+QΛdQT,dS = dQ \\, \\Lambda Q^T + Q \\, d\\Lambda \\, Q^T + Q \\Lambda dQ^T, which may be written as QTdSQ=QTdQΛ−ΛQTdQ+dΛ.Q^T dS \\, Q = Q^T dQ \\Lambda - \\Lambda Q^T dQ + d\\Lambda.\nAs an exercise, one may check that the left and right hand sides of the above are both symmetric. This may be easier if one looks at the diagonal entries on their own, as there (QTdSQ)ii=qiTdSqi(Q^T dS \\, Q)_{ii} = q_i^T dS \\, q_i. Since qiq_i is the iith eigenvector, this implies qiTdSqi=dλi.q_i^T dS\\, q_i = d\\lambda_i. (In physics, this is sometimes called the “Hellman–Feynman” theorem, or non-degenerate first-order eigenvalue-perturbation theory.)\nSometimes we think of a curve of matrices S(t)S(t) depending on a parameter such as time. If we ask for dλidt\\frac{d\\lambda_i }{dt}, this implies it is thus equal to qiTdS(t)dtqi.q_i^T \\frac{ dS(t)}{dt} q_i. So how can we get the gradient ∇λi\\nabla \\lambda_i for one of the eigenvalues? Well, firstly, note that $$\\tr (q_i q_i^T)^T dS) = d\\lambda_i \\implies \\nabla \\lambda_i = q_i q_i^T.$$\nWhat about the eigenvectors? Those come from off diagonal elements, where for i≠j,i \\neq j, (QTdSQ)ij=(QTdQdt)ij(λj−λi).(Q^T dS \\, Q)_{ij} = \\left(Q^T \\frac{dQ}{\ndt}\\right)_{ij} (\\lambda_j - \\lambda_i). Therefore, we can form the elements of QTdQdtQ^T \\frac{dQ}{ dt}, and left multiply by QQ to obtain dQdt\\frac{dQ}{dt} (as QQ is orthogonal).\nIt is interesting to get the second derivative of eigenvalues when moving along a line in symmetric matrix space. For simplicity, suppose Λ\\Lambda is diagonal and S(t)=Λ+tE.S(t) = \\Lambda + t E. Therefore, differentiating dΛdt=diag(QTdS(t)dtQ),\\frac{d\\Lambda}{dt} = \\operatorname{diag}\\left(Q^T \\frac{dS(t)}{dt} Q\\right), we get d2Λdt2=diag(QTd2S(t)dt2Q)+2diag(QTdS(t)dtdQdt).\\frac{d^2 \\Lambda}{dt^2} = \\operatorname{diag}\\left(Q^T \\frac{d^2 S(t)}{dt^2} Q\\right) + 2 \\operatorname{diag}\\left(Q^T \\frac{dS(t)}{dt} \\frac{dQ}{dt}\\right). Evaluating this at Q=IQ = I and recognizing the first term is zero as we are on a line, we have that d2Λdt2=2diag(E⋅dQdt),\\frac{d^2 \\Lambda}{dt^2} = 2 \\operatorname{diag}\\left(E \\cdot  \\frac{dQ}{dt}\\right), or d2Λdt2=2∑k≠iEik2/(λi−λk).\\frac{d^2 \\Lambda}{dt^2} = 2 \\sum_{k \\neq i} E_{ik}^2/(\\lambda_i - \\lambda_k). Using this, we can write out the eigenvalues as a Taylor series: λi(ϵ)=λi+ϵEii+ϵ2∑k≠iEik2/(λi−λk)+….\\lambda_i (\\epsilon) = \\lambda_i + \\epsilon E_{ii} + \\epsilon^2 \\sum_{k \\neq i} E_{ik}^2/(\\lambda_i - \\lambda_k) + \\dots. (In physics, this is known as second-order eigenvalue perturbation theory.)"
  },
  {
    "objectID": "matrix-calculus/en-US/index.html#footnotes",
    "href": "matrix-calculus/en-US/index.html#footnotes",
    "title": "Matrix Calculus\n(for Machine Learning and Beyond)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBriefly, a function g(δx)g(\\delta x) is o(δx)o(\\delta x) if limδx→0‖g(δx)‖‖δx‖=0\\lim_{\\delta x \\to 0} \\frac{\\Vert g(\\delta x) \\Vert}{\\Vert \\delta x \\Vert} = 0. We will return to this subject in Section 5.2.↩︎\nInformally, one can think of the vector space of infinitesimals dxdx as living in the same space as xx (understood as a small change in a vector, but still a vector nonetheless). Formally, one can define a distinct “vector space of infinitesimals” in various ways, e.g. as a cotangent space in differential geometry, though we won’t go into more detail here.↩︎\nIn some Julia AD software, this is done with by defining a “ChainRule”, and in Python autograd/JAX it is done by defining a custom “vJp” (row-vector—Jacobian product) and/or “Jvp” (Jacobian–vector product).↩︎\nThe concept of a “row vector” can be formalized as something called a “covector,” a “dual vector,” or an element of a “dual space,” not to be confused with the dual numbers used in automatic differentiation (Sec. 8).↩︎\nSome authors distinguish the “dot product” from an “inner product” for complex vector spaces, saying that a dot product has no complex conjugation x⋅y=y⋅xx \\cdot y = y \\cdot x (in which case x⋅xx \\cdot x need not be real and does not equal ‖x‖2\\Vert x \\Vert^2), whereas the inner product must be conjugate-symmetric, via ⟨x,y⟩=x‾⋅y\\langle x, y \\rangle = \\bar{x} \\cdot y. Another source of confusion for complex vector spaces is that some fields of mathematics define ⟨x,y⟩=x⋅y‾\\langle x, y \\rangle = x \\cdot \\bar{y}, i.e. they conjugate the right argument instead of the left (so that it is linear in the left argument and conjugate-linear in the right argument). Aren’t you glad we’re sticking with real numbers?↩︎\nCompleteness means that any Cauchy sequence of points in the vector space—any sequence of points that gets closer and closer together—has a limit lying within the vector space. This criterion usually holds in practice for vector spaces over real or complex scalars, but can get trickier when talking about vector spaces of functions, since e.g. the limit of a sequence of continuous functions can be a discontinuous function.↩︎\nProving the triangle inequality for an arbitrary inner product is not so obvious; one uses a result called the Cauchy–Schwarz inequality.↩︎\nIn fact, extraordinarily difficult: “NP-complete” (Naumann, 2006).↩︎\nThe Autodiff Cookbook, part of the JAX documentation, discusses this algorithm in a section on Hessian–vector products. It notes that one could also interchange the ∂/∂α\\partial/\\partial \\alpha and ∇x\\nabla_x derivatives and employ reverse-over-forward mode, but suggests that this is less efficient in practice: “because forward-mode has less overhead than reverse-mode, and since the outer differentiation operator here has to differentiate a larger computation than the inner one, keeping forward-mode on the outside works best.” It also presents another alternative: using the identity (∇f)′v=∇(vT∇f)(\\nabla f)'v = \\nabla (v^T \\nabla f), one can apply reverse-over-reverse mode to take the gradient of vT∇fv^T \\nabla f, but this has even more computational overhead.↩︎\nOf course, the independent variable need not be time, it just needs to be a real scalar. But in a generic context it is convenient to imagine ODE solutions as evolving in time.↩︎\nFor example, the second-order ODE d2vdt2+dvdt=h(v,t)\\frac{d^{2}v}{dt^{2}}+\\frac{dv}{dt}=h(v,t) could be re-written in first-order form by defining u=(u1u2)=(vdv/dt)u=\\left(\\begin{array}{c}\nu_{1}\\\\\nu_{2}\n\\end{array}\\right)=\\left(\\begin{array}{c}\nv\\\\\ndv/dt\n\\end{array}\\right), in which case du/dt=f(u,t)du/dt=f(u,t) where f=(u2h(u1,t)−u2)f=\\left(\\begin{array}{c}\nu_{2}\\\\\nh(u_{1},t)-u_{2}\n\\end{array}\\right).↩︎\nFor a modern and full-featured example, see the DifferentialEquations.jl suite of ODE solvers in the Julia language.↩︎\nThis “left-to-right” picture can be made very explicit if we imagine discretizing the ODE into a recurrence, e.g. via Euler’s method for an arbitrarily small Δt\\Delta t, as described in the MIT course notes Adjoint methods and sensitivity analysis for recurrence relations by S. G. Johnson (2011).↩︎\nBeing fully mathematically rigorous with vector spaces of functions requires a lot of tedious care in specifying a well-behaved set of functions, inserting annoying caveats about functions that differ only at isolated points, and so forth. In this lecture, we will mostly ignore such technicalities—we will implicitly assume that our functions are integrable, differentiable, etcetera, as needed. The subject of functional analysis exists to treat such matters with more care.↩︎\nTechnically, it only needs to be small “almost everywhere” since jumps that occur only at isolated points don’t affect the integral.↩︎\nFor more discussion of these concepts, see (e.g.) the review article “Monte Carlo gradient estimation in machine learning” (2020) by Mohamed et al. (https://arxiv.org/abs/1906.10652).↩︎\nMost computer hardware cannot generate numbers that are actually random, only numbers that seem random, called “pseudo-random” numbers. The design of these random-seeming numeric sequences is a subtle subject, steeped in number theory, with a long history of mistakes. A famous ironic quotation in this field is (Robert Coveyou, 1970): “Random number generation is too important to be left to chance.”↩︎\nMuch of machine learning uses only variations on gradient descent, without incorporating Hessian information except implicitly via “momentum” terms. Partly this can be explained by the fact that optimization problems in ML are typically solved only to low accuracy, often have nonsmooth/stochastic aspects, rarely involve nonlinear constraints, and are often very high-dimensional. This is only a small corner of the wider universe of computational optimization!↩︎\nThe term “programming” in optimization theory does not refer to software engineering, but is rather an anachronistic term for optimization problems. For example, “linear programming” (LP) refers to optimizing affine objectives and affine constraints, while “quadratic programming” (QP) refers to optimizing convex quadratic objectives with affine constraints. {% endraw %}↩︎"
  },
  {
    "objectID": "matrix-calculus/index.html",
    "href": "matrix-calculus/index.html",
    "title": "Matrix Calculus",
    "section": "",
    "text": "Matrix Calculus\narXiv: https://arxiv.org/abs/2501.14787\nTranslations: - English - 简体中文"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Главная",
    "section": "",
    "text": "Это мой сайт, написанный в Markdown.\ny=wx+by=wx+b\nA=[a11a12a13a21a22a23a31a32a33]\nA = \\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{bmatrix}\n\n{\n    \"a\": \"b\"\n    // comment\n}\n\n\n\n\n\n\n\n\nТрансформеры используют FFN после механизма самовнимания."
  },
  {
    "objectID": "index.html#введение",
    "href": "index.html#введение",
    "title": "Главная",
    "section": "",
    "text": "Трансформеры используют FFN после механизма самовнимания."
  },
  {
    "objectID": "iindex.html",
    "href": "iindex.html",
    "title": "Главная",
    "section": "",
    "text": "Это мой сайт, написанный в Markdown.\ny=wx+by=wx+b\nA=[a11a12a13a21a22a23a31a32a33]\nA = \\begin{bmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{bmatrix}\n\n{\n    \"a\": \"b\"\n    // comment\n}\n\n\n\n\n\n\n\n\nТрансформеры используют FFN после механизма самовнимания."
  },
  {
    "objectID": "iindex.html#введение",
    "href": "iindex.html#введение",
    "title": "Главная",
    "section": "",
    "text": "Трансформеры используют FFN после механизма самовнимания."
  },
  {
    "objectID": "matrix-calculus/arXiv-2501.14787v1/figures/matrixcalc-ODE-fig.html",
    "href": "matrix-calculus/arXiv-2501.14787v1/figures/matrixcalc-ODE-fig.html",
    "title": "ivanstepanovftw",
    "section": "",
    "text": "using PyPlot\nplt.\"rcParams\"[\"font.family\"] = \"Times New Roman\"\n\n\"Times New Roman\"\n\n\n\nt = range(0, 2, length=1000)\n\n0.0:0.002002002002002002:2.0\n\n\n\nplot(t, exp.(-t), \"r-\", linewidth=2)\nplot(t, exp.(-(1 + 0.1)*t), \"b--\", linewidth=2)\nxlabel(L\"“time” $t$\", math_fontfamily=\"dejavuserif\")\nylabel(L\"solution $u(p, t)$\", math_fontfamily=\"dejavuserif\")\ntitle(L\"solutions to $\\frac{\\partial u}{\\partial t} = -p u$ with $u(p,0) = 1$\", math_fontfamily=\"dejavuserif\", y=0.9, fontsize=16)\ntext(0.5, exp(-0.5)-0.09, L\"u(p, t)\", math_fontfamily=\"dejavuserif\", fontsize=16, color=\"red\", rotation=-39)\ntext(0.75, exp(-0.75)-0.3, L\"u(p + \\delta p, t)\", math_fontfamily=\"dejavuserif\", fontsize=16, color=\"blue\", rotation=-31)\n\narrow(1.5, exp(-(1 + 0.1)*1.5) - 0.1, 0, 0.1, width=0.01, length_includes_head=true, color=\"black\")\narrow(1.5, exp(-1 * 1.5) + 0.1, 0, -0.1, width=0.01, length_includes_head=true, color=\"black\")\ntext(1.55, exp(-1 * 1.5) + 0.035, L\"\\delta u\", math_fontfamily=\"dejavuserif\", fontsize=16)\n\nylim(0, 1)\nxlim(0,2)\nxticks([0, 0.5, 1.0, 1.5, 2.0])\nsavefig(\"ode-fig.pdf\")"
  }
]